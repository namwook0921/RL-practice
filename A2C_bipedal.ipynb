{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOm+FXCwsOTTWCrDzFP97UK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYXvVHQ_JOHJ","executionInfo":{"status":"ok","timestamp":1746410004874,"user_tz":-540,"elapsed":27069,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"960a35ce-77c8-4cb5-f1bc-5cb015af25b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kV6-Zech0MzG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747576451751,"user_tz":-540,"elapsed":15484,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"885b1ee3-d244-4d13-d160-5d0e25f27ed1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n"]}],"source":["!pip install gymnasium\n","!pip install pygame"]},{"cell_type":"code","source":["!pip install swig\n","!pip install gymnasium[box2d]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wo85t2-PCybp","outputId":"95bd931e-bbfc-41c8-8529-3c255d65628a","executionInfo":{"status":"ok","timestamp":1747576509522,"user_tz":-540,"elapsed":57768,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379373 sha256=ea7782c5dc17aba2232d7bf5b29d0fb75ee34eb70638c902697a8e3a5070f3cd\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","gym.pprint_registry()"],"metadata":{"id":"zNFrxef6CVeE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747576509621,"user_tz":-540,"elapsed":96,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"a685e6e0-7e81-413e-fcb2-d81462b82a18"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["===== classic_control =====\n","Acrobot-v1             CartPole-v0            CartPole-v1\n","MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n","===== phys2d =====\n","phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n","===== box2d =====\n","BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v3\n","LunarLander-v3         LunarLanderContinuous-v3\n","===== toy_text =====\n","Blackjack-v1           CliffWalking-v0        FrozenLake-v1\n","FrozenLake8x8-v1       Taxi-v3\n","===== tabular =====\n","tabular/Blackjack-v0   tabular/CliffWalking-v0\n","===== mujoco =====\n","Ant-v2                 Ant-v3                 Ant-v4\n","Ant-v5                 HalfCheetah-v2         HalfCheetah-v3\n","HalfCheetah-v4         HalfCheetah-v5         Hopper-v2\n","Hopper-v3              Hopper-v4              Hopper-v5\n","Humanoid-v2            Humanoid-v3            Humanoid-v4\n","Humanoid-v5            HumanoidStandup-v2     HumanoidStandup-v4\n","HumanoidStandup-v5     InvertedDoublePendulum-v2 InvertedDoublePendulum-v4\n","InvertedDoublePendulum-v5 InvertedPendulum-v2    InvertedPendulum-v4\n","InvertedPendulum-v5    Pusher-v2              Pusher-v4\n","Pusher-v5              Reacher-v2             Reacher-v4\n","Reacher-v5             Swimmer-v2             Swimmer-v3\n","Swimmer-v4             Swimmer-v5             Walker2d-v2\n","Walker2d-v3            Walker2d-v4            Walker2d-v5\n","===== None =====\n","GymV21Environment-v0   GymV26Environment-v0\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","\n","class GaussianActorCritic(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dims=(64, 64)):\n","        super().__init__()\n","        # shared layers\n","        self.share = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","\n","        # actor layers. actor_mean estimates the mean of the Gaussian distribution, and actor_log_std estimates the log_std\n","        self.actor_mean = nn.Linear(hidden_dims[1], output_dim)\n","\n","        # log_std is safe from having negative values, therefore more stable than estimating std. Since action is bounded from -1 to 1, initialize it as e^-2\n","        self.actor_log_std = nn.Parameter(torch.ones(output_dim) * -2)\n","\n","        # critic layer estimates value function\n","        self.critic = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.share(x)\n","\n","        # actor's mean layer estimates mean.\n","        mean = self.actor_mean(x)\n","\n","        # actor's log_std layer estimates log_std. Then, convert it to std\n","        log_std = self.actor_log_std\n","        std = torch.exp(log_std)\n","\n","        # critic layer estimates value\n","        value = self.critic(x)\n","\n","        return mean, std, value\n"],"metadata":{"id":"If6ymze-CrUd","executionInfo":{"status":"ok","timestamp":1747578529304,"user_tz":-540,"elapsed":4,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","\n","class A2CAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5, num_envs = 8, vectorization_mode = \"sync\"):\n","        # using vectorized environments to boost training\n","        # sync is more stable, async is faster\n","        self.env = gym.make_vec(env_id, num_envs = num_envs, vectorization_mode=vectorization_mode)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        # DEBUG NOTE must use single_action_space, not action_space since action_space.shape has dimension (num_env, num_actions)\n","        # self.policy_net = GaussianActorCritic(self.env.single_observation_space.shape[0], self.env.action_space.shape[0]).to(self.device)\n","        self.policy_net = GaussianActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.shape[0]).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        mean, std, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Normal(mean, std)\n","        action = action_dist.sample()\n","\n","        # bipedal walker's action space is bounded to [-1, 1]. Apply tanh function to keep the action in range\n","        action = torch.tanh(action)\n","        return action\n","\n","    # computing the gamma decaying rewards in Monte carlo\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","\n","        # Bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","\n","    # mostly equal to A2C_cartpole\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","        step_sum = 0\n","        random_seed = 1\n","        torch.manual_seed(random_seed)\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs = [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            done_steps = np.zeros(self.num_envs)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","\n","                # differs from A2C_cartpole when sampling action\n","                mean, std, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Normal(mean, std)\n","                action = action_dist.sample()\n","\n","                # not exactly the log probability, but log(probability density) since it is a continuous space\n","                # sum it in the dimension of num_actions (multiplying probability)\n","                log_prob = action_dist.log_prob(action).sum(dim=-1)\n","\n","                # need to move the tensor to the cpu to convert it to numpy\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","                done_mask = np.logical_or(done_mask, done)\n","                reward = np.where(done_mask, 0.0, reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                # if (steps % self.num_steps == 0) or np.any(done):\n","                if np.any(done):\n","                    with torch.no_grad():\n","                        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, _, next_value = self.policy_net(next_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                    # returns = self.compute_n_step_returns(rewards, next_value)  # shape: (n_steps, num_envs)\n","                    returns = self.compute_return(rewards)\n","                    returns = returns.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    values = torch.stack(values).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    log_probs = torch.stack(log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    advantages = returns - values\n","                    # matching the dimensions fo log_probs\n","\n","                    # print(steps, \"values\", values.shape, \"returns\", returns.shape, \"log probs\", log_probs.shape)\n","\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","\n","                    print(steps, \"\\nLOSS\", actor_loss, critic_loss, \"R/V\", returns, values)\n","\n","                    loss = actor_loss + 0.4 * critic_loss\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","            if episode % 20 == 0:\n","               print('Episode {}\\tlengths: {}\\treward: {}]\\tfull length: {}'.format(episode, done_steps, episode_reward, steps))\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","        self.env.close()\n","        return np.array(episode_rewards)\n"],"metadata":{"id":"8RhcUczlmRO_","executionInfo":{"status":"ok","timestamp":1747579952742,"user_tz":-540,"elapsed":52,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env_id = 'LunarLanderContinuous-v3'\n","gamma = 0.99\n","num_episodes = 1\n","max_steps = 1000\n","num_steps = 8\n","lr = 7e-3\n","\n","a2c_model =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = num_steps)\n","\n","rewards = a2c_model.train()"],"metadata":{"id":"R1-YSXWCrFe8","executionInfo":{"status":"ok","timestamp":1747579953724,"user_tz":-540,"elapsed":439,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9265915-035c-4f45-e3e5-4a15bef34f17"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["71 \n","LOSS tensor(24.7913, device='cuda:0', grad_fn=<NegBackward0>) tensor(894.9126, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[-7.3137e+00, -7.2056e+00, -9.5065e+00, -1.1881e+01, -1.1437e+01,\n","         -1.3863e+01, -1.3170e+01, -1.3660e+01, -1.3650e+01, -1.3412e+01,\n","         -1.5899e+01, -1.8395e+01, -2.0777e+01, -2.0804e+01, -2.2035e+01,\n","         -2.3475e+01, -2.2657e+01, -2.1051e+01, -1.8917e+01, -1.8464e+01,\n","         -1.6236e+01, -1.3996e+01, -1.5230e+01, -1.3091e+01, -1.2309e+01,\n","         -1.0105e+01, -1.0944e+01, -8.8366e+00, -9.1341e+00, -8.6338e+00,\n","         -6.6275e+00, -7.1739e+00, -6.1091e+00, -6.8650e+00, -4.8969e+00,\n","         -2.9626e+00, -3.7250e+00, -1.8606e+00, -3.2789e-02,  2.7975e-01,\n","         -7.4783e-01,  9.5235e-01, -1.1153e+00, -2.6298e+00, -3.6539e+00,\n","         -3.1986e+00, -1.6547e+00, -3.3880e-02,  1.7801e+00,  1.7302e+00,\n","          1.8226e+00,  3.5953e+00,  3.1707e+00,  3.3494e+00,  3.1456e+00,\n","          1.5281e+00,  3.2459e+00,  2.6427e+00,  4.3161e+00,  5.9493e+00,\n","          4.6274e+00,  4.2018e+00,  2.8976e+00,  1.5339e+00,  1.6370e+00,\n","          1.1413e+00,  1.1568e+00,  2.6576e+00,  1.3346e+00,  3.8922e-01,\n","          4.0045e-01],\n","        [-6.2212e+01, -6.2209e+01, -6.2207e+01, -6.0643e+01, -6.0057e+01,\n","         -5.8497e+01, -5.8468e+01, -5.8398e+01, -5.7030e+01, -5.6927e+01,\n","         -5.6697e+01, -5.6516e+01, -5.6451e+01, -5.6192e+01, -5.5794e+01,\n","         -5.5892e+01, -5.4470e+01, -5.4096e+01, -5.3677e+01, -5.3215e+01,\n","         -5.1819e+01, -5.1165e+01, -5.0066e+01, -4.9551e+01, -4.8998e+01,\n","         -4.8393e+01, -4.7747e+01, -4.7093e+01, -4.6410e+01, -4.6026e+01,\n","         -4.5303e+01, -4.4553e+01, -4.4331e+01, -4.3736e+01, -4.2142e+01,\n","         -4.1357e+01, -4.0752e+01, -3.9062e+01, -3.8536e+01, -3.8145e+01,\n","         -3.7275e+01, -3.6384e+01, -3.5475e+01, -3.4750e+01, -3.3791e+01,\n","         -3.2818e+01, -3.1831e+01, -3.0831e+01, -2.9821e+01, -2.8800e+01,\n","         -2.8142e+01, -2.6895e+01, -2.5839e+01, -2.4774e+01, -2.3346e+01,\n","         -2.1522e+01, -1.9767e+01, -1.9110e+01, -1.7505e+01, -1.6453e+01,\n","         -1.5665e+01, -1.4550e+01, -1.3128e+01, -1.1616e+01, -1.0437e+01,\n","         -9.2379e+00, -7.9793e+00, -6.0072e+00, -4.6322e+00, -3.2581e+00,\n","         -1.4514e+00],\n","        [ 2.9070e+00,  2.0507e+00,  1.4855e+00,  4.0839e-01,  1.3650e+00,\n","          2.2342e+00,  8.3434e-01,  2.5208e-01,  1.4263e+00,  5.5800e-01,\n","         -2.8402e-01,  7.7650e-01,  6.9043e-01, -1.1805e+00, -1.1462e+00,\n","         -6.0482e-02,  6.3908e-01,  1.6714e+00,  3.2068e-01,  1.3514e+00,\n","          1.4507e+00,  2.4409e+00,  2.1262e+00,  1.6711e+00,  1.1731e+00,\n","          6.3224e-01,  1.6053e+00,  4.6085e-01, -8.9092e-01,  1.1908e-01,\n","         -3.4381e-01, -9.3341e-02, -6.3752e-01,  3.3806e-01,  1.3000e+00,\n","         -3.8235e-01,  5.9040e-01, -6.5099e-01,  3.2759e-01,  1.2938e+00,\n","          1.1214e+00,  2.0549e+00,  1.0283e+00,  4.7968e-01, -1.0595e+00,\n","         -1.3201e+00, -1.9010e+00, -2.4507e+00, -1.4415e+00, -1.2905e+00,\n","         -1.1775e+00, -1.7970e-01, -1.0380e+00, -1.9410e+00, -8.3939e-01,\n","         -1.1987e-01,  1.0326e+00,  1.8672e+00,  3.0812e+00,  4.3552e+00,\n","          5.6990e+00,  5.2520e+00,  5.5351e+00,  7.1722e+00,  7.3840e+00,\n","          9.3003e+00,  1.1089e+01,  1.3270e+01,  1.5619e+01,  6.7497e+00,\n","          0.0000e+00],\n","        [-3.4565e+01, -3.4112e+01, -3.3493e+01, -3.2801e+01, -3.2042e+01,\n","         -3.1220e+01, -3.0344e+01, -3.1475e+01, -3.1685e+01, -3.0786e+01,\n","         -2.9837e+01, -2.8844e+01, -2.7813e+01, -2.7063e+01, -2.6002e+01,\n","         -2.5853e+01, -2.4796e+01, -2.3720e+01, -2.2631e+01, -2.1531e+01,\n","         -2.0425e+01, -1.9318e+01, -1.8212e+01, -1.8032e+01, -1.9391e+01,\n","         -1.8209e+01, -1.8668e+01, -1.7538e+01, -1.8372e+01, -1.7265e+01,\n","         -1.6198e+01, -1.6343e+01, -1.7313e+01, -1.7875e+01, -1.8705e+01,\n","         -1.8073e+01, -1.7055e+01, -1.7432e+01, -1.8061e+01, -1.8383e+01,\n","         -1.8451e+01, -1.7381e+01, -1.7454e+01, -1.7237e+01, -1.7045e+01,\n","         -1.7061e+01, -1.7331e+01, -1.6373e+01, -1.6003e+01, -1.6418e+01,\n","         -1.6592e+01, -1.5570e+01, -1.4549e+01, -1.3529e+01, -1.2508e+01,\n","         -1.1486e+01, -1.1433e+01, -1.0428e+01, -1.0574e+01, -9.5517e+00,\n","         -9.4349e+00, -9.1154e+00, -8.5158e+00, -7.3848e+00, -7.2859e+00,\n","         -6.0478e+00, -6.3096e+00, -4.6462e+00, -3.1892e+00, -2.8396e+00,\n","         -1.2991e+00],\n","        [-5.8108e+01, -5.7158e+01, -5.6841e+01, -5.6804e+01, -5.7814e+01,\n","         -5.7212e+01, -5.6725e+01, -5.6700e+01, -5.5584e+01, -5.5643e+01,\n","         -5.4931e+01, -5.3615e+01, -5.4482e+01, -5.2883e+01, -5.1181e+01,\n","         -5.0158e+01, -5.0929e+01, -5.1668e+01, -5.0031e+01, -5.0712e+01,\n","         -4.9422e+01, -4.8944e+01, -4.8105e+01, -4.8636e+01, -4.7473e+01,\n","         -4.7961e+01, -4.8318e+01, -4.8589e+01, -4.8060e+01, -4.7721e+01,\n","         -4.6078e+01, -4.6118e+01, -4.5003e+01, -4.4440e+01, -4.3455e+01,\n","         -4.3296e+01, -4.3039e+01, -4.1216e+01, -4.0857e+01, -4.0404e+01,\n","         -3.8910e+01, -3.8370e+01, -3.7743e+01, -3.5784e+01, -3.4507e+01,\n","         -3.3809e+01, -3.2044e+01, -3.1517e+01, -3.0738e+01, -2.9354e+01,\n","         -2.7447e+01, -2.6199e+01, -2.5133e+01, -2.4255e+01, -2.3319e+01,\n","         -2.1510e+01, -2.0524e+01, -1.8317e+01, -1.6673e+01, -1.5680e+01,\n","         -1.4646e+01, -1.3347e+01, -1.1839e+01, -9.8147e+00, -8.8630e+00,\n","         -7.7560e+00, -6.6191e+00, -5.4552e+00, -4.2671e+00, -2.8349e+00,\n","         -1.0099e+00],\n","        [-7.1435e+01, -7.1411e+01, -7.0796e+01, -7.0101e+01, -6.9938e+01,\n","         -6.9741e+01, -6.9632e+01, -6.8920e+01, -6.7609e+01, -6.7214e+01,\n","         -6.6934e+01, -6.7057e+01, -6.6695e+01, -6.6298e+01, -6.5866e+01,\n","         -6.5402e+01, -6.4906e+01, -6.4379e+01, -6.3939e+01, -6.3287e+01,\n","         -6.2711e+01, -6.2107e+01, -6.0922e+01, -6.0308e+01, -5.9381e+01,\n","         -5.8719e+01, -5.8620e+01, -5.7915e+01, -5.7190e+01, -5.6447e+01,\n","         -5.5687e+01, -5.4912e+01, -5.4125e+01, -5.3409e+01, -5.3140e+01,\n","         -5.1589e+01, -5.0782e+01, -4.9965e+01, -4.9139e+01, -4.7977e+01,\n","         -4.7359e+01, -4.6982e+01, -4.6100e+01, -4.5207e+01, -4.4521e+01,\n","         -4.3590e+01, -4.2933e+01, -4.1094e+01, -4.0142e+01, -3.8996e+01,\n","         -3.7422e+01, -3.6176e+01, -3.4555e+01, -3.3870e+01, -3.2034e+01,\n","         -3.0400e+01, -2.8695e+01, -2.7564e+01, -2.6406e+01, -2.4701e+01,\n","         -2.2184e+01, -2.0948e+01, -1.9675e+01, -1.8363e+01, -1.7009e+01,\n","         -1.4475e+01, -1.3041e+01, -1.0650e+01, -8.5593e+00, -5.7829e+00,\n","         -3.3397e+00],\n","        [-3.8618e+01, -3.8717e+01, -3.7955e+01, -3.6824e+01, -3.6700e+01,\n","         -3.6633e+01, -3.5589e+01, -3.5838e+01, -3.5687e+01, -3.4346e+01,\n","         -3.4122e+01, -3.3813e+01, -3.3506e+01, -3.3117e+01, -3.2848e+01,\n","         -3.1664e+01, -3.1242e+01, -3.0746e+01, -3.0294e+01, -2.8793e+01,\n","         -2.8957e+01, -2.7837e+01, -2.7251e+01, -2.7166e+01, -2.6701e+01,\n","         -2.5626e+01, -2.5005e+01, -2.4728e+01, -2.4236e+01, -2.3714e+01,\n","         -2.3136e+01, -2.2505e+01, -2.1827e+01, -2.1105e+01, -2.0342e+01,\n","         -1.9544e+01, -1.9389e+01, -1.9536e+01, -1.8985e+01, -1.8111e+01,\n","         -1.9021e+01, -1.8144e+01, -1.7788e+01, -1.7562e+01, -1.6774e+01,\n","         -1.5885e+01, -1.4973e+01, -1.5229e+01, -1.4253e+01, -1.3258e+01,\n","         -1.2248e+01, -1.1613e+01, -1.0604e+01, -9.5858e+00, -9.8851e+00,\n","         -1.0210e+01, -9.2544e+00, -9.7178e+00, -9.6915e+00, -9.5563e+00,\n","         -8.3870e+00, -7.2078e+00, -6.5729e+00, -5.3619e+00, -5.1863e+00,\n","         -5.5150e+00, -4.7490e+00, -4.5828e+00, -3.5157e+00, -2.2430e+00,\n","         -7.4154e-01],\n","        [-1.5145e+01, -1.4038e+01, -1.2796e+01, -1.1537e+01, -1.1079e+01,\n","         -1.2489e+01, -1.1232e+01, -1.2224e+01, -1.0899e+01, -9.5712e+00,\n","         -8.2447e+00, -8.9555e+00, -7.6269e+00, -8.4377e+00, -7.1086e+00,\n","         -5.7908e+00, -6.3922e+00, -7.4925e+00, -8.7671e+00, -9.0067e+00,\n","         -7.8153e+00, -7.9208e+00, -6.8058e+00, -5.7048e+00, -6.3442e+00,\n","         -6.2718e+00, -5.1571e+00, -4.0613e+00, -3.4615e+00, -3.9380e+00,\n","         -5.0000e+00, -6.1675e+00, -7.1218e+00, -6.1244e+00, -7.1237e+00,\n","         -7.0380e+00, -7.1721e+00, -7.5902e+00, -7.5349e+00, -7.3044e+00,\n","         -6.4091e+00, -5.5311e+00, -7.2641e+00, -6.3720e+00, -5.4960e+00,\n","         -5.4312e+00, -5.4557e+00, -5.8464e+00, -7.6356e+00, -7.1729e+00,\n","         -6.3219e+00, -7.2250e+00, -6.3538e+00, -5.4809e+00, -6.6310e+00,\n","         -5.7271e+00, -4.8085e+00, -6.2409e+00, -6.8912e+00, -6.9018e+00,\n","         -5.8292e+00, -5.8498e+00, -4.6489e+00, -4.7444e+00, -4.7875e+00,\n","         -4.3884e+00, -4.2903e+00, -4.9411e+00, -4.0870e+00, -3.2354e+00,\n","         -2.3728e+00]], device='cuda:0') tensor([[-0.0397, -0.0406, -0.0436, -0.0464, -0.0461, -0.0488, -0.0480, -0.0495,\n","         -0.0504, -0.0503, -0.0527, -0.0550, -0.0573, -0.0566, -0.0576, -0.0598,\n","         -0.0610, -0.0632, -0.0651, -0.0655, -0.0674, -0.0693, -0.0687, -0.0706,\n","         -0.0713, -0.0731, -0.0725, -0.0740, -0.0742, -0.0747, -0.0761, -0.0753,\n","         -0.0764, -0.0760, -0.0774, -0.0788, -0.0787, -0.0803, -0.0817, -0.0826,\n","         -0.0824, -0.0835, -0.0829, -0.0823, -0.0824, -0.0829, -0.0838, -0.0847,\n","         -0.0856, -0.0857, -0.0858, -0.0867, -0.0863, -0.0866, -0.0869, -0.0859,\n","         -0.0872, -0.0870, -0.0883, -0.0896, -0.0890, -0.0891, -0.0883, -0.0873,\n","         -0.0873, -0.0873, -0.0875, -0.0886, -0.0877, -0.0871, -0.0875],\n","        [-0.0259, -0.0256, -0.0290, -0.0295, -0.0314, -0.0328, -0.0357, -0.0388,\n","         -0.0407, -0.0438, -0.0451, -0.0481, -0.0479, -0.0507, -0.0523, -0.0524,\n","         -0.0531, -0.0556, -0.0582, -0.0603, -0.0616, -0.0616, -0.0627, -0.0645,\n","         -0.0662, -0.0676, -0.0696, -0.0710, -0.0723, -0.0715, -0.0728, -0.0738,\n","         -0.0737, -0.0730, -0.0729, -0.0737, -0.0736, -0.0733, -0.0729, -0.0724,\n","         -0.0730, -0.0736, -0.0740, -0.0737, -0.0741, -0.0748, -0.0754, -0.0755,\n","         -0.0759, -0.0760, -0.0749, -0.0745, -0.0747, -0.0748, -0.0738, -0.0728,\n","         -0.0725, -0.0712, -0.0704, -0.0712, -0.0708, -0.0716, -0.0701, -0.0697,\n","         -0.0705, -0.0690, -0.0698, -0.0687, -0.0679, -0.0688, -0.0684],\n","        [-0.1047, -0.1042, -0.1033, -0.1033, -0.1033, -0.1035, -0.1032, -0.1024,\n","         -0.1035, -0.1021, -0.1012, -0.1023, -0.1023, -0.1023, -0.1028, -0.1043,\n","         -0.1045, -0.1057, -0.1054, -0.1065, -0.1068, -0.1082, -0.1085, -0.1083,\n","         -0.1088, -0.1087, -0.1103, -0.1095, -0.1094, -0.1108, -0.1107, -0.1105,\n","         -0.1106, -0.1119, -0.1131, -0.1121, -0.1133, -0.1127, -0.1136, -0.1146,\n","         -0.1145, -0.1159, -0.1159, -0.1164, -0.1166, -0.1170, -0.1171, -0.1178,\n","         -0.1193, -0.1199, -0.1206, -0.1221, -0.1224, -0.1220, -0.1234, -0.1240,\n","         -0.1253, -0.1254, -0.1267, -0.1279, -0.1291, -0.1289, -0.1286, -0.1296,\n","         -0.1297, -0.1310, -0.1316, -0.1330, -0.1343, -0.1320, -0.1091],\n","        [-0.0516, -0.0537, -0.0554, -0.0571, -0.0589, -0.0608, -0.0625, -0.0612,\n","         -0.0607, -0.0624, -0.0640, -0.0656, -0.0672, -0.0681, -0.0698, -0.0699,\n","         -0.0716, -0.0734, -0.0748, -0.0761, -0.0769, -0.0775, -0.0777, -0.0773,\n","         -0.0762, -0.0763, -0.0756, -0.0757, -0.0748, -0.0752, -0.0751, -0.0745,\n","         -0.0736, -0.0728, -0.0720, -0.0718, -0.0723, -0.0716, -0.0709, -0.0705,\n","         -0.0699, -0.0704, -0.0697, -0.0695, -0.0690, -0.0683, -0.0678, -0.0687,\n","         -0.0686, -0.0683, -0.0679, -0.0688, -0.0697, -0.0706, -0.0716, -0.0725,\n","         -0.0716, -0.0724, -0.0713, -0.0722, -0.0712, -0.0713, -0.0713, -0.0724,\n","         -0.0721, -0.0722, -0.0699, -0.0691, -0.0682, -0.0672, -0.0675],\n","        [-0.0087, -0.0087, -0.0088, -0.0096, -0.0126, -0.0136, -0.0151, -0.0156,\n","         -0.0176, -0.0182, -0.0190, -0.0186, -0.0218, -0.0210, -0.0206, -0.0221,\n","         -0.0253, -0.0284, -0.0293, -0.0324, -0.0346, -0.0359, -0.0356, -0.0387,\n","         -0.0397, -0.0428, -0.0457, -0.0481, -0.0483, -0.0488, -0.0504, -0.0527,\n","         -0.0534, -0.0528, -0.0522, -0.0547, -0.0572, -0.0582, -0.0606, -0.0628,\n","         -0.0630, -0.0653, -0.0676, -0.0685, -0.0694, -0.0718, -0.0721, -0.0720,\n","         -0.0716, -0.0721, -0.0736, -0.0730, -0.0738, -0.0765, -0.0787, -0.0797,\n","         -0.0817, -0.0820, -0.0820, -0.0835, -0.0847, -0.0841, -0.0842, -0.0851,\n","         -0.0839, -0.0852, -0.0864, -0.0875, -0.0886, -0.0883, -0.0885],\n","        [-0.0331, -0.0360, -0.0364, -0.0369, -0.0397, -0.0426, -0.0432, -0.0446,\n","         -0.0459, -0.0464, -0.0488, -0.0494, -0.0518, -0.0542, -0.0566, -0.0587,\n","         -0.0605, -0.0623, -0.0623, -0.0619, -0.0634, -0.0650, -0.0651, -0.0666,\n","         -0.0674, -0.0684, -0.0683, -0.0691, -0.0699, -0.0707, -0.0714, -0.0720,\n","         -0.0723, -0.0719, -0.0713, -0.0711, -0.0716, -0.0720, -0.0721, -0.0717,\n","         -0.0710, -0.0701, -0.0703, -0.0705, -0.0695, -0.0697, -0.0690, -0.0680,\n","         -0.0681, -0.0677, -0.0663, -0.0660, -0.0659, -0.0647, -0.0644, -0.0634,\n","         -0.0627, -0.0637, -0.0645, -0.0645, -0.0625, -0.0628, -0.0630, -0.0631,\n","         -0.0631, -0.0615, -0.0615, -0.0602, -0.0585, -0.0583, -0.0580],\n","        [-0.0707, -0.0719, -0.0713, -0.0711, -0.0716, -0.0730, -0.0720, -0.0720,\n","         -0.0733, -0.0731, -0.0743, -0.0738, -0.0750, -0.0762, -0.0758, -0.0749,\n","         -0.0760, -0.0773, -0.0770, -0.0766, -0.0763, -0.0755, -0.0742, -0.0736,\n","         -0.0751, -0.0753, -0.0747, -0.0753, -0.0748, -0.0763, -0.0776, -0.0792,\n","         -0.0809, -0.0825, -0.0842, -0.0856, -0.0858, -0.0861, -0.0861, -0.0878,\n","         -0.0870, -0.0875, -0.0864, -0.0859, -0.0863, -0.0879, -0.0897, -0.0901,\n","         -0.0920, -0.0940, -0.0959, -0.0960, -0.0980, -0.1001, -0.1002, -0.0997,\n","         -0.1007, -0.1011, -0.1018, -0.1016, -0.1031, -0.1043, -0.1048, -0.1058,\n","         -0.1056, -0.1052, -0.1054, -0.1050, -0.1052, -0.1061, -0.1062],\n","        [-0.0935, -0.0944, -0.0954, -0.0960, -0.0954, -0.0953, -0.0965, -0.0969,\n","         -0.0981, -0.0996, -0.1011, -0.1012, -0.1027, -0.1028, -0.1043, -0.1062,\n","         -0.1065, -0.1048, -0.1042, -0.1037, -0.1054, -0.1046, -0.1061, -0.1074,\n","         -0.1079, -0.1086, -0.1098, -0.1112, -0.1114, -0.1115, -0.1110, -0.1104,\n","         -0.1103, -0.1113, -0.1108, -0.1110, -0.1111, -0.1113, -0.1115, -0.1115,\n","         -0.1125, -0.1134, -0.1128, -0.1141, -0.1155, -0.1161, -0.1166, -0.1172,\n","         -0.1174, -0.1185, -0.1201, -0.1205, -0.1221, -0.1236, -0.1239, -0.1254,\n","         -0.1270, -0.1271, -0.1273, -0.1277, -0.1289, -0.1293, -0.1305, -0.1306,\n","         -0.1309, -0.1306, -0.1301, -0.1292, -0.1294, -0.1292, -0.1292]],\n","       device='cuda:0', grad_fn=<TransposeBackward0>)\n","88 \n","LOSS tensor(7.4470, device='cuda:0', grad_fn=<NegBackward0>) tensor(595.6723, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[  6.2859,   5.1613,   6.5761,   6.4689,   4.1836,   3.2530,   4.5339,\n","           3.9733,   5.1420,   4.3670,   5.5106,   6.6056,   3.4762,   1.5582,\n","           2.6275,   3.6476,   2.3490],\n","        [-35.8351, -34.2044, -32.4434, -30.9221, -29.5512, -27.9917, -26.4664,\n","         -24.6763, -22.7023, -20.5432, -16.9588, -14.9426, -12.3495, -10.1138,\n","          -7.7905,  -5.3753,  -2.6402],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000],\n","        [-30.7303, -29.4751, -27.9855, -27.8247, -26.0625, -24.1589, -22.1014,\n","         -19.8778, -18.5717, -15.9957, -13.2267, -10.2559,  -7.0758,  -4.6607,\n","          -1.0581,   2.7619,   5.8465],\n","        [-21.8674, -20.9670, -19.9192, -18.8546, -17.7754, -16.6837, -15.2634,\n","         -14.1617, -12.9697, -12.1101, -10.6858,  -9.9431,  -7.4166,  -6.5726,\n","          -5.1658,  -3.8853,  -1.2599],\n","        [-43.7562, -42.3949, -40.9551, -38.7814, -36.7906, -35.0112, -33.1321,\n","         -31.1473, -29.0508, -26.8364, -23.8046, -21.1002, -17.9263, -14.5400,\n","         -11.2833,  -8.1046,  -3.4755],\n","        [-16.1113, -15.7658, -14.6052, -12.9632, -12.8483, -12.8018, -11.8052,\n","         -10.5018,  -8.9955,  -7.6656,  -7.0843,  -6.3213,  -4.8439,  -4.1038,\n","          -2.7673,  -1.6010,  -0.7699],\n","        [118.9113, 121.8841, 114.6828,  28.8003,  26.3646,  22.4903,  20.8795,\n","          21.5465,   9.8086,   3.8879,  -5.5524, -10.6314,  -5.7235,  -2.5773,\n","          -4.3155,  -3.2105,   0.0000]], device='cuda:0') tensor([[-0.2111, -0.2100, -0.2109, -0.2100, -0.2085, -0.2073, -0.2081, -0.2071,\n","         -0.2081, -0.2081, -0.2094, -0.2110, -0.2090, -0.2077, -0.2093, -0.2109,\n","         -0.2099],\n","        [-0.2527, -0.2524, -0.2533, -0.2522, -0.2509, -0.2486, -0.2495, -0.2478,\n","         -0.2457, -0.2431, -0.2411, -0.2422, -0.2403, -0.2415, -0.2427, -0.2438,\n","         -0.2436],\n","        [-0.0046, -0.1852, -0.1883, -0.1893, -0.1890, -0.1893, -0.1914, -0.1906,\n","         -0.1920, -0.1924, -0.1954, -0.1985, -0.2017, -0.2048, -0.2080, -0.2109,\n","         -0.2114],\n","        [-0.2131, -0.2121, -0.2126, -0.2102, -0.2108, -0.2112, -0.2117, -0.2119,\n","         -0.2092, -0.2092, -0.2093, -0.2092, -0.2092, -0.2071, -0.2072, -0.2073,\n","         -0.2054],\n","        [-0.2510, -0.2506, -0.2517, -0.2527, -0.2536, -0.2545, -0.2544, -0.2555,\n","         -0.2553, -0.2549, -0.2551, -0.2539, -0.2542, -0.2535, -0.2522, -0.2539,\n","         -0.2547],\n","        [-0.2507, -0.2513, -0.2522, -0.2511, -0.2498, -0.2505, -0.2511, -0.2516,\n","         -0.2520, -0.2523, -0.2508, -0.2500, -0.2492, -0.2476, -0.2467, -0.2472,\n","         -0.2470],\n","        [-0.2051, -0.2041, -0.2049, -0.2043, -0.2033, -0.2021, -0.2019, -0.2028,\n","         -0.2023, -0.2034, -0.2030, -0.2021, -0.2031, -0.2013, -0.2012, -0.2004,\n","         -0.1996],\n","        [-0.2094, -0.2085, -0.1887, -0.1706, -0.1466, -0.1266, -0.1120, -0.1092,\n","         -0.0543, -0.0468, -0.0427, -0.0450, -0.0098, -0.0094, -0.0527, -0.0654,\n","         -0.0787]], device='cuda:0', grad_fn=<TransposeBackward0>)\n","90 \n","LOSS tensor(1.0348, device='cuda:0', grad_fn=<NegBackward0>) tensor(7.3932, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[ 2.4797,  0.9314],\n","        [-5.4918, -2.8011],\n","        [ 0.0000,  0.0000],\n","        [ 0.5167,  0.0000],\n","        [-2.6321, -1.2839],\n","        [-7.2697, -3.7134],\n","        [-3.3700, -1.6625],\n","        [ 0.0000,  0.0000]], device='cuda:0') tensor([[-0.2706, -0.2694],\n","        [-0.3733, -0.3754],\n","        [-0.3049, -0.3076],\n","        [-0.3339, -0.3437],\n","        [-0.3736, -0.3738],\n","        [-0.3904, -0.3918],\n","        [-0.2447, -0.2432],\n","        [-0.1292, -0.2969]], device='cuda:0', grad_fn=<TransposeBackward0>)\n","95 \n","LOSS tensor(3.4594, device='cuda:0', grad_fn=<NegBackward0>) tensor(31.2568, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[  2.3915,   0.7980,  -0.0545,  -0.7284,   0.2218],\n","        [-15.4933, -12.7333,  -9.8544,  -6.8515,  -3.2933],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [ -7.3490,  -5.7151,  -4.4639,  -3.1191,  -1.7996],\n","        [-15.8387, -12.1248,  -8.2489,  -4.2081,   0.0000],\n","        [ -8.8285,  -7.6296,  -6.3428,  -4.2377,  -2.2595],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]], device='cuda:0') tensor([[-0.3188, -0.3164, -0.3155, -0.3150, -0.3163],\n","        [-0.4801, -0.4823, -0.4845, -0.4862, -0.4871],\n","        [-0.3888, -0.3918, -0.3916, -0.3907, -0.3929],\n","        [-0.5473, -0.3397, -0.3411, -0.3429, -0.3433],\n","        [-0.4726, -0.4728, -0.4745, -0.4737, -0.4752],\n","        [-0.5021, -0.5034, -0.5047, -0.5058, -0.5068],\n","        [-0.2785, -0.2762, -0.2755, -0.2762, -0.2751],\n","        [-0.3697, -0.3725, -0.3752, -0.3778, -0.3777]], device='cuda:0',\n","       grad_fn=<TransposeBackward0>)\n","102 \n","LOSS tensor(4.7495, device='cuda:0', grad_fn=<NegBackward0>) tensor(55.4890, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[  3.8908,   1.7730,  -0.5028,  -3.4118,  -2.4835,  -1.6052,  -0.7773],\n","        [-26.0641, -21.2428, -17.4912, -13.8695, -10.1983,  -6.3812,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [-14.6724, -11.9213,  -9.9367,  -7.9758,  -5.8841,  -4.5793,  -1.7498],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [-17.3610, -15.1905, -13.4115, -11.0483,  -7.7608,  -5.2507,  -2.6456],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n","       device='cuda:0') tensor([[-0.3593, -0.3566, -0.3528, -0.3487, -0.3502, -0.3517, -0.3531],\n","        [-0.5901, -0.5934, -0.5944, -0.5951, -0.5972, -0.5991, -0.6036],\n","        [-0.4692, -0.4697, -0.4699, -0.4712, -0.4726, -0.4736, -0.4745],\n","        [-0.4056, -0.4055, -0.4085, -0.4080, -0.4100, -0.4091, -0.4119],\n","        [-0.5677, -0.5689, -0.5702, -0.5705, -0.5719, -0.5737, -0.5758],\n","        [-0.6160, -0.4249, -0.4233, -0.4234, -0.4236, -0.4221, -0.4239],\n","        [-0.3106, -0.3112, -0.3103, -0.3108, -0.3091, -0.3096, -0.3099],\n","        [-0.4480, -0.4471, -0.4478, -0.4481, -0.4475, -0.4464, -0.4486]],\n","       device='cuda:0', grad_fn=<TransposeBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  2.35it/s]"]},{"output_type":"stream","name":"stdout","text":["119 \n","LOSS tensor(7.8479, device='cuda:0', grad_fn=<NegBackward0>) tensor(271.8023, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[ 13.5675,  12.9365,  10.6420,  11.5526,  10.9891,   8.2531,   5.3555,\n","           6.2450,   7.0896,   6.0635,   6.8373,   6.1888,   5.6782,   3.8648,\n","           1.2203,   0.7525,  -0.9287],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000],\n","        [-56.5082, -55.5832, -54.2372, -52.0556, -50.9956, -48.1263, -43.8254,\n","         -40.3106, -37.0245, -33.7459, -29.7031, -25.1549, -20.5363, -16.0280,\n","         -11.0791,  -8.1028,  -5.0373],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000],\n","        [-44.8807, -42.4484, -40.3889, -36.7085, -34.1330, -31.6946, -28.0592,\n","         -24.4924, -21.2218, -16.0889, -11.3792,  -7.8960,  -3.5248,  -7.7909,\n","          -2.7714, -13.8332,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n","           0.0000,   0.0000,   0.0000]], device='cuda:0') tensor([[-0.4116, -0.4110, -0.4071, -0.4090, -0.4076, -0.4031, -0.3975, -0.3994,\n","         -0.4017, -0.4002, -0.4024, -0.4017, -0.4011, -0.3982, -0.3930, -0.3919,\n","         -0.3896],\n","        [-0.7415, -0.4695, -0.4717, -0.4780, -0.4791, -0.4802, -0.4835, -0.4852,\n","         -0.4906, -0.4967, -0.4993, -0.5021, -0.5050, -0.5068, -0.5131, -0.5155,\n","         -0.5205],\n","        [-0.5612, -0.5631, -0.5647, -0.5682, -0.5691, -0.5706, -0.5722, -0.5719,\n","         -0.5752, -0.5784, -0.5819, -0.5833, -0.5862, -0.5881, -0.5874, -0.5881,\n","         -0.5889],\n","        [-0.4840, -0.4870, -0.4885, -0.4892, -0.4899, -0.4902, -0.4904, -0.4895,\n","         -0.4893, -0.4888, -0.4887, -0.4896, -0.4880, -0.4876, -0.4904, -0.4910,\n","         -0.4936],\n","        [-0.6885, -0.6891, -0.6896, -0.6906, -0.6932, -0.6951, -0.7002, -0.7042,\n","         -0.7087, -0.7133, -0.7172, -0.7224, -0.7266, -0.7315, -0.7369, -0.7402,\n","         -0.7440],\n","        [-0.4990, -0.4973, -0.4991, -0.4964, -0.4968, -0.4953, -0.4936, -0.4955,\n","         -0.4972, -0.4943, -0.4932, -0.4913, -0.4882, -0.4856, -0.4829, -0.4817,\n","         -0.4810],\n","        [-0.3528, -0.3502, -0.3476, -0.3452, -0.3431, -0.3415, -0.3396, -0.3362,\n","         -0.3367, -0.3323, -0.3297, -0.3283, -0.3268, -0.3127, -0.3124, -0.4987,\n","         -0.6191],\n","        [-0.5302, -0.5328, -0.5343, -0.5373, -0.5376, -0.5394, -0.5392, -0.5417,\n","         -0.5401, -0.5425, -0.5448, -0.5447, -0.5438, -0.5427, -0.5439, -0.5434,\n","         -0.5458]], device='cuda:0', grad_fn=<TransposeBackward0>)\n","123 \n","LOSS tensor(0.8467, device='cuda:0', grad_fn=<NegBackward0>) tensor(46.2574, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[ 15.1765,  13.7039,   4.4468,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000],\n","        [-22.5200, -18.7171, -14.4149,  -6.9411],\n","        [  0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000]], device='cuda:0') tensor([[-0.4519, -0.4480, -0.4569, -0.4360],\n","        [-0.6169, -0.6192, -0.6258, -0.6323],\n","        [-0.6888, -0.6913, -0.6929, -0.6967],\n","        [-0.5712, -0.5742, -0.5771, -0.5779],\n","        [-0.9017, -0.9078, -0.9144, -0.9245],\n","        [-0.5583, -0.5549, -0.5558, -0.5578],\n","        [-0.2949, -0.4927, -0.4973, -0.5019],\n","        [-0.6374, -0.6383, -0.6382, -0.6391]], device='cuda:0',\n","       grad_fn=<TransposeBackward0>)\n","128 \n","LOSS tensor(1.4122, device='cuda:0', grad_fn=<NegBackward0>) tensor(20.6949, device='cuda:0', grad_fn=<MseLossBackward0>) R/V tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [-21.7475, -16.9607, -11.5507,  -5.7601,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]], device='cuda:0') tensor([[-0.6907, -0.5961, -0.5966, -0.5962, -0.5980],\n","        [-0.7190, -0.7259, -0.7327, -0.7380, -0.7428],\n","        [-0.7873, -0.7883, -0.7925, -0.7967, -0.8006],\n","        [-0.6421, -0.6435, -0.6464, -0.6460, -0.6442],\n","        [-1.0747, -1.0822, -1.0906, -1.0993, -1.1076],\n","        [-0.6211, -0.6225, -0.6208, -0.6176, -0.6174],\n","        [-0.5645, -0.5636, -0.5646, -0.5656, -0.5656],\n","        [-0.7237, -0.7274, -0.7259, -0.7236, -0.7235]], device='cuda:0',\n","       grad_fn=<TransposeBackward0>)\n","Episode 0\tlengths: [123. 102.  71.  90. 128.  95. 119.  88.]\treward: [  34.09709729 -175.79761743    4.03952868  -79.87526562 -239.94866831\n"," -175.7049572  -148.85698857  102.31257276]]\tfull length: 128\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"id":"TCfaIz4P9LuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","env = gym.make('LunarLanderContinuous-v3')\n","test_episodes = 10\n","test_rewards = []\n","\n","for _ in range(test_episodes):\n","    state, _ = env.reset()\n","    done = False\n","    ep_reward = 0\n","\n","    while not done:\n","        with torch.no_grad():\n","            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(a2c_model.device)\n","            mean, std, _ = a2c_model.policy_net(state_tensor)\n","            action = mean\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy().squeeze(0))\n","        done = terminated or truncated\n","        ep_reward += reward\n","        state = next_state\n","\n","    test_rewards.append(ep_reward)\n","\n","\n","print(f\"Average reward over {test_episodes} test episodes: {np.mean(test_rewards):.2f}\")\n","\n","plt.plot(test_rewards)\n","plt.title(\"A2C Test Episode Rewards\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Reward\")\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"OEaRNu7_IzPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","\n","\n","env = gym.make('LunarLanderContinuous-v3')\n","test_episodes = 10\n","random_rewards = []\n","\n","for ep in range(test_episodes):\n","    state, _ = env.reset()\n","    done = False\n","    ep_reward = 0\n","\n","    while not done:\n","        action = env.action_space.sample()  # random continuous action\n","        state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        ep_reward += reward\n","\n","    random_rewards.append(ep_reward)\n","    print(f\"Episode {ep+1} reward: {ep_reward:.2f}\")\n","\n","env.close()\n","\n","avg_reward = np.mean(random_rewards)\n","print(f\"\\n✅ Average reward over {test_episodes} random episodes: {avg_reward:.2f}\")\n"],"metadata":{"id":"9OgOgj5DKVzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6QaeQNNRKtfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from tqdm import tqdm\n","# import torch.profiler\n","\n","# n_envs = 1\n","# envs = [gym.make('LunarLanderContinuous-v3') for _ in range(n_envs)]\n","\n","# n_states = envs[0].observation_space.shape[0]\n","# n_actions = envs[0].action_space.shape[0]\n","\n","# actor_network = GaussianActor(n_states, n_actions)\n","# critic_network = Critic(n_states)\n","\n","# state_normalizer = Normalizer(shape=(envs[0].observation_space.shape[0],))\n","# optimizer = optim.Adam(list(actor_network.parameters()) + list(critic_network.parameters()), lr=initial_lr)\n","# mse_loss = nn.MSELoss()\n","\n","# max_step = 1600\n","# states, actions, log_probs, rewards, values, dones, entropies = [], [], [], [], [], [], []\n","\n","\n","# for ep in tqdm(range(episodes)):\n","#     states, rewards, dones, values, actions = [], [], [], [], []\n","#     ep_reward = 0\n","#     latest_debug_info = {}\n","\n","#     for env in envs:\n","#         state, _ = env.reset()\n","#         done = False\n","#         steps = 0\n","\n","#         while not done:\n","#             state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","#             mean, std = actor_network(state_tensor)\n","#             dist = torch.distributions.Normal(mean, std)\n","#             action = dist.sample()\n","#             log_prob = dist.log_prob(action).sum(dim=-1)\n","#             entropy = dist.entropy().sum(dim=-1)\n","#             value = critic_network(state_tensor)\n","#             steps += 1\n","\n","#             next_state, reward, terminated, truncated, _ = env.step(action.detach().numpy().squeeze(0))\n","#             done = terminated or truncated or steps > max_step\n","\n","#             states.append(state_tensor)\n","#             actions.append(action)\n","#             log_probs.append(log_prob)\n","#             values.append(value)\n","#             rewards.append(reward)\n","#             dones.append(done)\n","#             entropies.append(entropy)\n","#             ep_reward += reward\n","\n","#             if len(rewards) >= num_steps or done:\n","\n","#                 next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n","#                 if dones[-1]:\n","#                     bootstrap_value = 0\n","#                 else:\n","#                     bootstrap_value = critic_network(next_state_tensor).item()\n","\n","#                 returns = []\n","#                 R = bootstrap_value\n","#                 for i in reversed(range(len(rewards))):\n","#                     R = rewards[-1 - i] + gamma * R\n","#                     returns.insert(0, R)\n","#                 returns = torch.tensor(returns, dtype=torch.float32)\n","\n","#                 values_tensor = torch.stack(values[-num_steps:]).squeeze(-1)\n","#                 log_probs_tensor = torch.stack(log_probs[-num_steps:]).squeeze(-1)\n","#                 entropies_tensor = torch.stack(entropies[-num_steps:]).squeeze(-1)\n","#                 returns_tensor = returns.clone().detach()\n","#                 advantages = returns - values_tensor\n","#                 clipped_advantages = torch.clamp(advantages, min=-10.0, max=10.0)\n","\n","#                 policy_loss = -(log_probs_tensor * clipped_advantages.detach()).mean()\n","#                 value_loss = mse_loss(values_tensor, returns_tensor)\n","#                 entropy_loss = -entropies_tensor.mean()\n","\n","#                 # total_loss = policy_loss + 0.5 * value_loss + 0.001 * entropy_loss\n","#                 total_loss = policy_loss + 0.4 * value_loss    #following rl-baselines3-zoo\n","\n","#                 optimizer.zero_grad()\n","#                 total_loss.backward()\n","#                 grad_norm = torch.nn.utils.clip_grad_norm_(\n","#                     list(actor_network.parameters()) + list(critic_network.parameters()),\n","#                     max_norm=0.5\n","#                 )\n","#                 optimizer.step()\n","\n","\n","#                 latest_debug_info = {\n","#                     \"values\": values_tensor,\n","#                     \"returns\": returns,\n","#                     \"log_probs\": log_probs_tensor,\n","#                     \"entropies\": entropies_tensor,\n","#                     \"policy_loss\": policy_loss,\n","#                     \"value_loss\": value_loss,\n","#                     \"grad_norm\": grad_norm\n","#                 }\n","\n","\n","#                 states, actions, log_probs, rewards, values, dones, entropies = [], [], [], [], [], [], []\n","\n","#             state = next_state\n","\n","\n","#     if ep % 10 == 0 and latest_debug_info:\n","#         a2c_debug_log(ep, steps, ep_reward, **latest_debug_info)"],"metadata":{"id":"deb3jHJxP-ta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def a2c_debug_log(ep, steps, ep_reward, values, returns, log_probs, entropies, policy_loss, value_loss, grad_norm):\n","    advantages = returns - values\n","\n","    print(f\"\\n[Episode {ep}] Debug Summary\")\n","    print(f\"# of Steps:         {steps:.2f}\")\n","    print(f\"Total Reward:       {ep_reward:.2f}\")\n","    print(f\"Mean V(s):          {values.mean().item():.4f}\")\n","    print(f\"Advantage Mean:     {advantages.mean().item():.4f}\")\n","    print(f\"Advantage Std:      {advantages.std().item():.4f}\")\n","    print(f\"Entropy (avg):      {entropies.mean().item():.4f}\")\n","    print(f\"Policy Loss:        {policy_loss.item():.4f}\")\n","    print(f\"Value Loss:         {value_loss.item():.4f}\")\n","    print(f\"Gradient Norm:      {grad_norm:.4f}\")\n","    print(f\"Log Prob Mean:      {log_probs.mean().item():.4f}\")\n","    print(\"-\" * 50)\n"],"metadata":{"id":"meg69gHmODz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","# class Normalizer:\n","#     def __init__(self, shape, epsilon=1e-8):\n","#         self.shape = shape\n","#         self.mean = np.zeros(shape)\n","#         self.var = np.ones(shape)\n","#         self.count = epsilon\n","\n","#     def update(self, x):\n","#         batch_mean = np.mean(x, axis=0)\n","#         batch_var = np.var(x, axis=0)\n","\n","#         batch_size = x.shape[0]\n","#         self.count += batch_size\n","#         self.mean += (batch_mean - self.mean) * batch_size / self.count\n","#         self.var += (batch_var - self.var) * batch_size / self.count\n","\n","#     def normalize(self, x):\n","#         return (x - self.mean) / np.sqrt(self.var + 1e-8)\n","\n","\n","def compute_gae(rewards, values, next_values, dones, gamma, gae_lambda):\n","    advantages = []\n","    advantage = 0\n","    for i in reversed(range(len(rewards))):\n","        delta = rewards[i] + gamma * next_values[i] * dones[i] - values[i]\n","        advantage = delta + gamma * gae_lambda * dones[i] * advantage\n","        advantages.insert(0, advantage)\n","    return advantages\n","\n"],"metadata":{"id":"2YnfV8N5RsA3"},"execution_count":null,"outputs":[]}]}