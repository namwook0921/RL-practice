{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6599,"status":"ok","timestamp":1747490829641,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"pmwHTljcl8sU","outputId":"d8f141d8-8508-4486-db7f-fe6d04f021f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n"]}],"source":["!pip install numpy==1.24.4"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77812,"status":"ok","timestamp":1747490984330,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"5RBaM_aPkwUd","outputId":"3e86dfaf-3988-41a3-932d-16baec8e608e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy\u003e=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.24.4)\n","Requirement already satisfied: cloudpickle\u003e=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions\u003e=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications\u003e=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.7.1)\n","Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy\u003e=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.24.4)\n","Requirement already satisfied: cloudpickle\u003e=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions\u003e=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications\u003e=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame\u003e=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","\u001b[33m  DEPRECATION: Building 'box2d-py' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'box2d-py'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n","\u001b[0m  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379445 sha256=1f9c60936936e9c3e6ad69ae08ad27afe267497eb2577faa70122be489a08d2d\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools pip --upgrade\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1747492401014,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"gr_aN4gGkY0s"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","\n","class ActorCritic(nn.Module):\n","    def __init__(self):\n","        super(ActorCritic, self).__init__()\n","        self.affine = nn.Linear(8, 128)\n","\n","        self.action_layer = nn.Linear(128, 4)\n","        self.value_layer = nn.Linear(128, 1)\n","\n","        self.logprobs = []\n","        self.state_values = []\n","        self.rewards = []\n","\n","    def forward(self, state):\n","        state = torch.from_numpy(state).float()\n","        state = F.relu(self.affine(state))\n","\n","        state_value = self.value_layer(state)\n","\n","        action_probs = F.softmax(self.action_layer(state))\n","        action_distribution = Categorical(action_probs)\n","        action = action_distribution.sample()\n","\n","        self.logprobs.append(action_distribution.log_prob(action))\n","        self.state_values.append(state_value)\n","\n","        return action.item()\n","\n","    def calculateLoss(self, gamma=0.99):\n","\n","        # calculating discounted rewards:\n","        rewards = []\n","        dis_reward = 0\n","        for reward in self.rewards[::-1]:\n","            dis_reward = reward + gamma * dis_reward\n","            rewards.insert(0, dis_reward)\n","\n","        # normalizing the rewards:\n","        rewards = torch.tensor(rewards)\n","        rewards = (rewards - rewards.mean()) / (rewards.std())\n","\n","        loss = 0\n","        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n","            advantage = reward  - value.item()\n","            action_loss = -logprob * advantage\n","            value_loss = F.smooth_l1_loss(value, reward)\n","            loss += (action_loss + value_loss)\n","        return loss\n","\n","    def clearMemory(self):\n","        del self.logprobs[:]\n","        del self.state_values[:]\n","        del self.rewards[:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"zChqN3vCkcRu"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.02 (0.9, 0.999)\n","Episode 0\tlength: 66\treward: -3.1183604976256825\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-9-540418c456d7\u003e:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  action_probs = F.softmax(self.action_layer(state))\n","\u003cipython-input-9-540418c456d7\u003e:50: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  value_loss = F.smooth_l1_loss(value, reward)\n"]},{"name":"stdout","output_type":"stream","text":["Episode 20\tlength: 59\treward: -326.64259672798323\n","Episode 40\tlength: 96\treward: -358.12689133770755\n","Episode 60\tlength: 127\treward: -223.85821546824187\n","Episode 80\tlength: 98\treward: -175.84287969337993\n","Episode 100\tlength: 76\treward: -126.47985032201078\n","Episode 120\tlength: 119\treward: -117.58751803705684\n","Episode 140\tlength: 100\treward: -158.13668947017345\n","Episode 160\tlength: 99\treward: -183.29623358281157\n","Episode 180\tlength: 116\treward: -107.12636175346165\n","Episode 200\tlength: 158\treward: -171.42489777473867\n","Episode 220\tlength: 115\treward: -120.69973696310183\n","Episode 240\tlength: 94\treward: -74.21016911106044\n","Episode 260\tlength: 121\treward: -47.000315659780995\n","Episode 280\tlength: 91\treward: -63.82739069753311\n","Episode 300\tlength: 236\treward: -32.1469801595273\n","Episode 320\tlength: 106\treward: -74.75074885411712\n","Episode 340\tlength: 117\treward: 8.268474940183026\n","Episode 360\tlength: 164\treward: -108.68407995660554\n","Episode 380\tlength: 196\treward: -167.12174770513968\n","Episode 400\tlength: 101\treward: -43.06009903830154\n","Episode 420\tlength: 558\treward: -137.51075906634134\n","Episode 440\tlength: 145\treward: -196.94158929778322\n","Episode 460\tlength: 914\treward: -257.4799692021583\n","Episode 480\tlength: 212\treward: -160.35026614500651\n","Episode 500\tlength: 187\treward: -65.67716314110403\n","Episode 520\tlength: 403\treward: -68.27525767907186\n","Episode 540\tlength: 165\treward: 44.443177518066726\n","Episode 560\tlength: 999\treward: 22.692895317255218\n","Episode 580\tlength: 999\treward: 1.3357627836974453\n","Episode 600\tlength: 112\treward: 15.619347462157474\n","Episode 620\tlength: 809\treward: 69.34038011604603\n","Episode 640\tlength: 236\treward: 40.530524577200836\n","Episode 660\tlength: 249\treward: 143.31422864676804\n","Episode 680\tlength: 298\treward: 85.3988440437412\n","Episode 700\tlength: 262\treward: 116.79778682080044\n","Episode 720\tlength: 156\treward: 87.58302763927949\n","Episode 740\tlength: 197\treward: 169.2422385398411\n","Episode 760\tlength: 299\treward: 91.46893696424607\n","Episode 780\tlength: 209\treward: 188.54858072761365\n","Episode 800\tlength: 357\treward: 160.84828270683192\n","Episode 820\tlength: 216\treward: 192.49159497549766\n","Episode 840\tlength: 191\treward: 151.38270620712478\n","Episode 860\tlength: 220\treward: 123.17550716491886\n","Episode 880\tlength: 303\treward: 90.63460624962464\n","Episode 900\tlength: 211\treward: 136.22428228522614\n","Episode 920\tlength: 358\treward: 118.19134428906864\n","Episode 940\tlength: 250\treward: 173.64994521851864\n","Episode 960\tlength: 182\treward: 125.53511589309214\n","Episode 980\tlength: 153\treward: 135.65218332133207\n","Episode 1000\tlength: 262\treward: 148.92340827599784\n","Episode 1020\tlength: 330\treward: 122.54748779559728\n","Episode 1040\tlength: 129\treward: 73.06370451491622\n","Episode 1060\tlength: 352\treward: 201.8811355566172\n","Episode 1080\tlength: 233\treward: 177.3098492208819\n","Episode 1100\tlength: 272\treward: 220.88753559186944\n","Episode 1120\tlength: 137\treward: 182.28306268953355\n","Episode 1140\tlength: 321\treward: 205.45925634766027\n","Episode 1160\tlength: 167\treward: 153.2240177327481\n","Episode 1180\tlength: 92\treward: 177.42180391335455\n","Episode 1200\tlength: 199\treward: 173.09446889020336\n","Episode 1220\tlength: 428\treward: 204.57304040260422\n","Episode 1240\tlength: 147\treward: 180.10418049304772\n","Episode 1260\tlength: 203\treward: 200.4201521375658\n","Episode 1280\tlength: 348\treward: 185.3663169785978\n","Episode 1300\tlength: 136\treward: 150.55498789782098\n","Episode 1320\tlength: 369\treward: 175.42029240972548\n","Episode 1340\tlength: 221\treward: 263.30128398495947\n","Episode 1360\tlength: 167\treward: 142.24674303938428\n","Episode 1380\tlength: 189\treward: 204.79956504134594\n","Episode 1400\tlength: 142\treward: 124.55949667475886\n","Episode 1420\tlength: 239\treward: 184.98572222640885\n","Episode 1440\tlength: 148\treward: 236.60983902281515\n","Episode 1460\tlength: 386\treward: 130.24505426715788\n","Episode 1480\tlength: 200\treward: 125.4810624549885\n","Episode 1500\tlength: 102\treward: 75.02666028781547\n","Episode 1520\tlength: 136\treward: 108.61243109746587\n","Episode 1540\tlength: 198\treward: 72.26629085246745\n","Episode 1560\tlength: 181\treward: 161.2019068031662\n","Episode 1580\tlength: 125\treward: 143.9127347946428\n","Episode 1600\tlength: 197\treward: 185.4673257142069\n","Episode 1620\tlength: 159\treward: 191.83074963332393\n","Episode 1640\tlength: 234\treward: 186.799781608072\n","Episode 1660\tlength: 264\treward: 251.90763085063054\n","Episode 1680\tlength: 254\treward: 210.20591130779084\n","Episode 1700\tlength: 172\treward: 179.04727891118458\n","Episode 1720\tlength: 284\treward: 118.2125757875099\n","Episode 1740\tlength: 424\treward: 175.97582032772016\n","Episode 1760\tlength: 282\treward: 168.39883683640542\n","Episode 1780\tlength: 162\treward: 176.96065843907803\n","Episode 1800\tlength: 999\treward: 142.98711834721468\n","Episode 1820\tlength: 235\treward: 106.90564756715233\n","Episode 1840\tlength: 156\treward: 30.421902015588405\n","Episode 1860\tlength: 999\treward: 72.48970981110872\n","Episode 1880\tlength: 753\treward: 109.29598206438007\n","Episode 1900\tlength: 999\treward: 166.14387857018806\n","Episode 1920\tlength: 385\treward: 163.52671845792364\n","Episode 1940\tlength: 386\treward: 105.54277901696965\n","Episode 1960\tlength: 263\treward: 169.76147867933875\n","Episode 1980\tlength: 519\treward: 152.0283064066288\n"]}],"source":["import torch\n","import torch.optim as optim\n","import gym\n","\n","def train():\n","    # Defaults parameters:\n","    #    gamma = 0.99\n","    #    lr = 0.02\n","    #    betas = (0.9, 0.999)\n","    #    random_seed = 543\n","\n","    render = False\n","    gamma = 0.99\n","    lr = 0.02\n","    betas = (0.9, 0.999)\n","    random_seed = 543\n","\n","    torch.manual_seed(random_seed)\n","\n","    env = gym.make('LunarLander-v2')\n","    env.seed(random_seed)\n","\n","    policy = ActorCritic()\n","    optimizer = optim.Adam(policy.parameters(), lr=lr, betas=betas)\n","    print(lr,betas)\n","\n","    running_reward = 0\n","    for i_episode in range(0, 2000):\n","        state = env.reset()\n","        for t in range(10000):\n","            action = policy(state)\n","            state, reward, done, _ = env.step(action)\n","            policy.rewards.append(reward)\n","            running_reward += reward\n","            if render and i_episode \u003e 1000:\n","                env.render()\n","            if done:\n","                break\n","\n","        # Updating the policy :\n","        optimizer.zero_grad()\n","        loss = policy.calculateLoss(gamma)\n","        loss.backward()\n","        optimizer.step()\n","        policy.clearMemory()\n","\n","        # saving the model if episodes \u003e 999 OR avg reward \u003e 200\n","        #if i_episode \u003e 999:\n","        #    torch.save(policy.state_dict(), './preTrained/LunarLander_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n","\n","        # if running_reward \u003e 4000:\n","        #     torch.save(policy.state_dict(), './preTrained/LunarLander_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n","        #     print(\"########## Solved! ##########\")\n","        #     test(name='LunarLander_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n","        #     break\n","\n","        if i_episode % 20 == 0:\n","            running_reward = running_reward/20\n","            print('Episode {}\\tlength: {}\\treward: {}'.format(i_episode, t, running_reward))\n","            running_reward = 0\n","\n","if __name__ == '__main__':\n","    train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NSC8lMLzkegB"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './preTrained/LunarLander_TWO.pth'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-15d5b00532ec\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 33\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-11-15d5b00532ec\u003e\u001b[0m in \u001b[0;36mtest\u001b[0;34m(n_episodes, name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m     \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./preTrained/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './preTrained/LunarLander_TWO.pth'"]}],"source":["import torch\n","import gym\n","from PIL import Image\n","\n","def test(n_episodes=5, name='LunarLander_TWO.pth'):\n","    env = gym.make('LunarLander-v2')\n","    policy = ActorCritic()\n","\n","    policy.load_state_dict(torch.load('./preTrained/{}'.format(name)))\n","\n","    render = True\n","    save_gif = False\n","\n","    for i_episode in range(1, n_episodes+1):\n","        state = env.reset()\n","        running_reward = 0\n","        for t in range(10000):\n","            action = policy(state)\n","            state, reward, done, _ = env.step(action)\n","            running_reward += reward\n","            if render:\n","                 env.render()\n","                 if save_gif:\n","                     img = env.render(mode = 'rgb_array')\n","                     img = Image.fromarray(img)\n","                     img.save('./gif/{}.jpg'.format(t))\n","            if done:\n","                break\n","        print('Episode {}\\tReward: {}'.format(i_episode, running_reward))\n","    env.close()\n","\n","if __name__ == '__main__':\n","    test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8VjPhXyk9Qq"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM4c3BVSH15ECljOtPizLKe","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}