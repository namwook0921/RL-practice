{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPO/tf+qUI6sZy3ywc3jmHt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYXvVHQ_JOHJ","executionInfo":{"status":"ok","timestamp":1746410004874,"user_tz":-540,"elapsed":27069,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"960a35ce-77c8-4cb5-f1bc-5cb015af25b3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kV6-Zech0MzG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746410009566,"user_tz":-540,"elapsed":4686,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"dc283b2e-ecfb-4639-910a-6cfbf27d6c87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n"]}],"source":["!pip install gymnasium\n","!pip install pygame"]},{"cell_type":"code","source":["!pip install swig\n","!pip install gymnasium[box2d]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wo85t2-PCybp","outputId":"62dbea68-a135-4681-8898-02e21d0a4afa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","gym.pprint_registry()"],"metadata":{"id":"zNFrxef6CVeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","class Normalizer:\n","    def __init__(self, shape, epsilon=1e-8):\n","        self.shape = shape\n","        self.mean = np.zeros(shape)\n","        self.var = np.ones(shape)\n","        self.count = epsilon\n","\n","    def update(self, x):\n","        batch_mean = np.mean(x, axis=0)\n","        batch_var = np.var(x, axis=0)\n","\n","        batch_size = x.shape[0]\n","        self.count += batch_size\n","        self.mean += (batch_mean - self.mean) * batch_size / self.count\n","        self.var += (batch_var - self.var) * batch_size / self.count\n","\n","    def normalize(self, x):\n","        return (x - self.mean) / np.sqrt(self.var + 1e-8)\n","\n","\n","def compute_gae(rewards, values, next_values, dones, gamma, gae_lambda):\n","    advantages = []\n","    advantage = 0\n","    for i in reversed(range(len(rewards))):\n","        delta = rewards[i] + gamma * next_values[i] * dones[i] - values[i]\n","        advantage = delta + gamma * gae_lambda * dones[i] * advantage\n","        advantages.insert(0, advantage)\n","    return advantages\n","\n"],"metadata":{"id":"2YnfV8N5RsA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alpha = 0.1\n","gamma = 0.99\n","episodes = 2500\n","num_steps = 8\n","initial_lr = 1e-3\n","gae_lambda = 0.9\n","# n_timesteps = int(5e6)"],"metadata":{"id":"wITtIlDACG-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GaussianActor(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dims=(128, 128)):\n","        super().__init__()\n","        self.share = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","\n","        self.mean_head = nn.Linear(hidden_dims[1], output_dim)\n","\n","        self.log_std = nn.Parameter(torch.zeros(output_dim))\n","\n","    def forward(self, x):\n","        x = self.share(x)\n","        mean = torch.tanh(self.mean_head(x))\n","        std = torch.exp(self.log_std.expand_as(mean))\n","        return mean, std\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, input_dim, hidden_dims=(128, 128)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.LayerNorm(hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.LayerNorm(hidden_dims[1]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[1], 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x).squeeze(-1)\n","\n"],"metadata":{"id":"If6ymze-CrUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def a2c_debug_log(ep, steps, ep_reward, values, returns, log_probs, entropies, policy_loss, value_loss, grad_norm):\n","    advantages = returns - values\n","\n","    print(f\"\\n[Episode {ep}] Debug Summary\")\n","    print(f\"# of Steps:         {steps:.2f}\")\n","    print(f\"Total Reward:       {ep_reward:.2f}\")\n","    print(f\"Mean V(s):          {values.mean().item():.4f}\")\n","    print(f\"Advantage Mean:     {advantages.mean().item():.4f}\")\n","    print(f\"Advantage Std:      {advantages.std().item():.4f}\")\n","    print(f\"Entropy (avg):      {entropies.mean().item():.4f}\")\n","    print(f\"Policy Loss:        {policy_loss.item():.4f}\")\n","    print(f\"Value Loss:         {value_loss.item():.4f}\")\n","    print(f\"Gradient Norm:      {grad_norm:.4f}\")\n","    print(f\"Log Prob Mean:      {log_probs.mean().item():.4f}\")\n","    print(\"-\" * 50)\n"],"metadata":{"id":"meg69gHmODz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import torch.profiler\n","\n","n_envs = 1\n","envs = [gym.make('LunarLanderContinuous-v3') for _ in range(n_envs)]\n","\n","n_states = envs[0].observation_space.shape[0]\n","n_actions = envs[0].action_space.shape[0]\n","\n","actor_network = GaussianActor(n_states, n_actions)\n","critic_network = Critic(n_states)\n","\n","state_normalizer = Normalizer(shape=(envs[0].observation_space.shape[0],))\n","optimizer = optim.Adam(list(actor_network.parameters()) + list(critic_network.parameters()), lr=initial_lr)\n","mse_loss = nn.MSELoss()\n","\n","max_step = 1600\n","states, actions, log_probs, rewards, values, dones, entropies = [], [], [], [], [], [], []\n","\n","\n","for ep in tqdm(range(episodes)):\n","    states, rewards, dones, values, actions = [], [], [], [], []\n","    ep_reward = 0\n","    latest_debug_info = {}\n","\n","    for env in envs:\n","        state, _ = env.reset()\n","        done = False\n","        steps = 0\n","\n","        while not done:\n","            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","            mean, std = actor_network(state_tensor)\n","            dist = torch.distributions.Normal(mean, std)\n","            action = dist.sample()\n","            log_prob = dist.log_prob(action).sum(dim=-1)\n","            entropy = dist.entropy().sum(dim=-1)\n","            value = critic_network(state_tensor)\n","            steps += 1\n","\n","            next_state, reward, terminated, truncated, _ = env.step(action.detach().numpy().squeeze(0))\n","            done = terminated or truncated or steps > max_step\n","\n","            states.append(state_tensor)\n","            actions.append(action)\n","            log_probs.append(log_prob)\n","            values.append(value)\n","            rewards.append(reward)\n","            dones.append(done)\n","            entropies.append(entropy)\n","            ep_reward += reward\n","\n","            if len(rewards) >= num_steps or done:\n","\n","                next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n","                if dones[-1]:\n","                    bootstrap_value = 0\n","                else:\n","                    bootstrap_value = critic_network(next_state_tensor).item()\n","\n","                returns = []\n","                R = bootstrap_value\n","                for i in reversed(range(len(rewards))):\n","                    R = rewards[-1 - i] + gamma * R\n","                    returns.insert(0, R)\n","                returns = torch.tensor(returns, dtype=torch.float32)\n","\n","                values_tensor = torch.stack(values[-num_steps:]).squeeze(-1)\n","                log_probs_tensor = torch.stack(log_probs[-num_steps:]).squeeze(-1)\n","                entropies_tensor = torch.stack(entropies[-num_steps:]).squeeze(-1)\n","                returns_tensor = returns.clone().detach()\n","                advantages = returns - values_tensor\n","                clipped_advantages = torch.clamp(advantages, min=-10.0, max=10.0)\n","\n","                policy_loss = -(log_probs_tensor * clipped_advantages.detach()).mean()\n","                value_loss = mse_loss(values_tensor, returns_tensor)\n","                entropy_loss = -entropies_tensor.mean()\n","\n","                # total_loss = policy_loss + 0.5 * value_loss + 0.001 * entropy_loss\n","                total_loss = policy_loss + 0.4 * value_loss    #following rl-baselines3-zoo\n","\n","                optimizer.zero_grad()\n","                total_loss.backward()\n","                grad_norm = torch.nn.utils.clip_grad_norm_(\n","                    list(actor_network.parameters()) + list(critic_network.parameters()),\n","                    max_norm=0.5\n","                )\n","                optimizer.step()\n","\n","\n","                latest_debug_info = {\n","                    \"values\": values_tensor,\n","                    \"returns\": returns,\n","                    \"log_probs\": log_probs_tensor,\n","                    \"entropies\": entropies_tensor,\n","                    \"policy_loss\": policy_loss,\n","                    \"value_loss\": value_loss,\n","                    \"grad_norm\": grad_norm\n","                }\n","\n","\n","                states, actions, log_probs, rewards, values, dones, entropies = [], [], [], [], [], [], []\n","\n","            state = next_state\n","\n","\n","    if ep % 10 == 0 and latest_debug_info:\n","        a2c_debug_log(ep, steps, ep_reward, **latest_debug_info)"],"metadata":{"id":"deb3jHJxP-ta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","test_episodes = 10\n","test_rewards = []\n","\n","for _ in range(test_episodes):\n","    state, _ = env.reset()\n","    done = False\n","    ep_reward = 0\n","\n","    while not done:\n","        with torch.no_grad():\n","            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","            mean, std = actor_network(state_tensor)\n","            action = mean\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action.detach().numpy().squeeze(0))\n","        done = terminated or truncated\n","        ep_reward += reward\n","        state = next_state\n","\n","    test_rewards.append(ep_reward)\n","\n","\n","print(f\"Average reward over {test_episodes} test episodes: {np.mean(test_rewards):.2f}\")\n","\n","plt.plot(test_rewards)\n","plt.title(\"A2C Test Episode Rewards\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Reward\")\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"OEaRNu7_IzPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","from gymnasium.wrappers import RecordVideo\n","import torch\n","\n","# Create environment with rendering enabled\n","env = gym.make(\"LunarLanderContinuous-v3\", render_mode=\"rgb_array\")\n","\n","# Set up video recording\n","env = RecordVideo(env, video_folder=\"/content/videos\", name_prefix=\"lunar-lander-test\", episode_trigger=lambda x: True)\n","\n","# Reset environment\n","state, _ = env.reset()\n","done = False\n","\n","while not done:\n","    with torch.no_grad():\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","        mean, _ = actor_network(state_tensor)\n","        action = mean.squeeze(0).numpy()\n","\n","    # Step environment\n","    state, _, terminated, truncated, _ = env.step(action)\n","    done = terminated or truncated\n","\n","env.close()\n","print(\"Recording complete. Check /content/videos.\")\n"],"metadata":{"id":"fuplCUA7Juxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","\n","env = gym.make(\"BipedalWalker-v3\")\n","test_episodes = 10\n","random_rewards = []\n","\n","for ep in range(test_episodes):\n","    state, _ = env.reset()\n","    done = False\n","    ep_reward = 0\n","\n","    while not done:\n","        action = env.action_space.sample()  # random continuous action\n","        state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        ep_reward += reward\n","\n","    random_rewards.append(ep_reward)\n","    print(f\"Episode {ep+1} reward: {ep_reward:.2f}\")\n","\n","env.close()\n","\n","avg_reward = np.mean(random_rewards)\n","print(f\"\\n✅ Average reward over {test_episodes} random episodes: {avg_reward:.2f}\")\n"],"metadata":{"id":"9OgOgj5DKVzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6QaeQNNRKtfF"},"execution_count":null,"outputs":[]}]}