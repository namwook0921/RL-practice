{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BoXsZSP1F20e"},"outputs":[],"source":["### Comparing single environment vs. synchronized environments vs. asynchronized environments\n","\n","# | Mode              | Training Time | Avg. Reward |\n","# |------------------|---------------|-------------|\n","# | Single Env       | 00:40         | 130.9       |\n","# | Sync 8 Envs      | 00:16         | 194.8       |\n","# | Async 8 Envs     | 00:35         | 307.5       |\n","\n","# Training performance async >> sync > single, training time sync >> async > single. Expected async to train faster and sync to show better reward, but turned out opposite.\n","# Training time dominatnly depended on how many steps the model ran while training, faster training in terms of reward led to longer steps to train, and longer training time.\n","# My hypothesis is while async's environments that processed earlier gave information to slower training environments in their early episodes, and that led to better earlier policies and therefore longer steps in further episodes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18011,"status":"ok","timestamp":1747056737127,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"nXMnywNrJT21","outputId":"20421eee-eeac-471f-bca9-664a3fa1c731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51282,"status":"ok","timestamp":1747573908482,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"9TE5jqXjRGj5","outputId":"3bf55828-615e-436b-c517-ffc07782e4c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n","Collecting pygame\n","  Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.6.1\n","Collecting wheel\n","  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n","Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wheel\n","Successfully installed wheel-0.45.1\n","Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379371 sha256=ddac63cb584dc1a3cdc847a82a5263dc094147e8789db0015eb0e2cedac292c0\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JifS_sPARTGj"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCritic(nn.Module):\n","\n","    # initially started with (64, 64) hidden dimension, but emprically found out (32, 32) works better. (64, 64) might be too much power for simple game like cartpole\n","    def __init__(self, input_dim, output_dim, hidden_dims=(32, 32)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLbCiMcsOt5a"},"outputs":[],"source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5, num_envs = 8, vectorization_mode = \"sync\"):\n","        # using vectorized environments to boost training\n","        # sync is more stable, async is faster\n","        self.env = gym.make_vec(env_id, num_envs = num_envs, vectorization_mode=vectorization_mode)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value  # shape: (num_envs,)\n","        returns = []\n","        for r in reversed(rewards):  # each r: (num_envs,)\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)  # shape: (n_steps, num_envs)\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","        step_sum = 0\n","        random_seed = 543\n","        torch.manual_seed(random_seed)\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs = [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_mask = np.logical_or(done_mask, done)\n","                reward = np.where(done_mask, 0.0, reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or np.any(done):\n","                    with torch.no_grad():\n","                        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_value = self.policy_net(next_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                    returns = self.compute_n_step_returns(rewards, next_value)  # shape: (n_steps, num_envs)\n","                    returns = returns.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    values = torch.stack(values).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    log_probs = torch.stack(log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    advantages = returns - values\n","\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","                    # penalize using entropy to encourage exploration\n","                    entropy = action_dist.entropy().mean()\n","\n","                    loss = actor_loss + 0.4 * critic_loss- 0.01 * entropy\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","            step_sum += steps\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"]},{"cell_type":"code","source":["class A2CAgent_single:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","    def train(self):\n","        episode_steps = []\n","        episode_rewards = []\n","        step_sum = 0\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            values = []\n","            rewards = []\n","            log_probs = []\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n","                done = terminated or truncated\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(reward)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or done:\n","                    _, next_value = self.policy_net(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n","                    next_value = next_value.squeeze()\n","                    # BUG ALERT\n","                    # MUST MULTIPLY (1 - done) to next_value to mask the bootstrapped next_value when the game is over. CRITICAL BUG THAT TOOK HOURS TO FIND\n","                    returns = self.compute_n_step_returns(rewards, next_value * (1 - done))\n","                    values = torch.stack(values)\n","                    log_probs = torch.stack(log_probs)\n","                    advantages = returns - values\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","\n","                    # penalize using entropy to encourage exploration\n","                    entropy = action_dist.entropy().mean()\n","                    loss = actor_loss + 0.4 * critic_loss- 0.01 * entropy\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_steps.append(steps)\n","            episode_rewards.append(episode_reward)\n","            step_sum += steps\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"],"metadata":{"id":"1yZl_zJG2ztG","executionInfo":{"status":"ok","timestamp":1747577150956,"user_tz":-540,"elapsed":24,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env = gym.make(\"CartPole-v1\")\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 1000 episodes explored\n","a2c_model_single_env =  A2CAgent_single(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards, steps = a2c_model_single_env.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAMO0Kpj2Es2","executionInfo":{"status":"ok","timestamp":1747577206820,"user_tz":-540,"elapsed":55737,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"d73a7f3b-7c52-49d0-8b31-3c56b3b1da09"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:55<00:00, 17.96it/s]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":162019,"status":"error","timestamp":1747574230373,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"szBR38HBF7_Q","outputId":"621341dc-a3ae-42d8-d8d1-79697a8ac3ed"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 340/1000 [02:41<05:14,  2.10it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-adc6b3e65cf4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ma2c_model_multiple_env\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mA2CAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrewards_mul_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_mul_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma2c_model_multiple_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-93a2300fdb52>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["env_id = \"CartPole-v1\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 8000 episodes explored\n","a2c_model_multiple_env =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards_mul_env, steps_mul_env = a2c_model_multiple_env.train()"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Assuming you already have:\n","# rewards, steps = agent.train()\n","\n","episodes = range(1, len(rewards) + 1)\n","\n","fig, ax1 = plt.subplots(figsize=(12, 5))\n","\n","# --- Plot rewards (left y-axis)\n","color = 'tab:blue'\n","ax1.set_xlabel('Episode')\n","ax1.set_ylabel('Reward', color=color)\n","ax1.plot(episodes, rewards, color=color, label='Reward')\n","ax1.tick_params(axis='y', labelcolor=color)\n","\n","# # --- Plot steps (right y-axis)\n","ax2 = ax1.twinx()\n","color = 'tab:red'\n","ax2.set_ylabel('Episode Length (Steps)', color=color)\n","ax2.plot(episodes, steps, color=color, linestyle='--', label='Steps')\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","# --- Add titles and grid\n","plt.title('A2C Training Progress: Rewards and Episode Lengths')\n","fig.tight_layout()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"1i7CmLohVDQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Assuming you already have:\n","# rewards, steps = agent.train()\n","\n","episodes = range(1, len(rewards_mul_env) + 1)\n","\n","fig, ax1 = plt.subplots(figsize=(12, 5))\n","\n","# --- Plot rewards (left y-axis)\n","# color = 'tab:blue'\n","# ax1.set_xlabel('Episode')\n","# ax1.set_ylabel('Reward', color=color)\n","# ax1.plot(episodes, rewards_mul_env, color=color, label='Reward')\n","# ax1.tick_params(axis='y', labelcolor=color)\n","\n","# --- Plot steps (right y-axis)\n","ax2 = ax1.twinx()\n","color = 'tab:red'\n","ax2.set_ylabel('Episode Length (Steps)', color=color)\n","ax2.plot(episodes, steps_mul_env, color=color, linestyle='--', label='Steps')\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","# --- Add titles and grid\n","plt.title('A2C Training Progress: Rewards and Episode Lengths')\n","fig.tight_layout()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"GNAGXCAdVD3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(steps), sum(steps_mul_env)"],"metadata":{"id":"_QUoIwlPXxnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1747574422002,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"TPjtvt_7F7mW","outputId":"2e091a70-6141-457b-92da-bce2f81c35ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 112.0\n","Episode 2 Reward: 116.0\n","Episode 3 Reward: 157.0\n","Episode 4 Reward: 125.0\n","Episode 5 Reward: 140.0\n","Episode 6 Reward: 116.0\n","Episode 7 Reward: 178.0\n","Episode 8 Reward: 117.0\n","Episode 9 Reward: 115.0\n","Episode 10 Reward: 133.0\n","Average Reward over 10 episodes: 130.9\n"]}],"source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_single_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rkMAPuZTq8B"},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 125\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 1000 episoded explored\n","a2c_model_multiple_env_125 =  A2CAgent(env_id, num_episodes=125, max_steps=max_steps, lr=lr, num_steps = max_steps, vectorization_mode= \"sync\")\n","\n","rewards_125, steps_125 = a2c_model_multiple_env_125.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92W2A2XCM6yU","executionInfo":{"status":"ok","timestamp":1747574593087,"user_tz":-540,"elapsed":16256,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"75316bb5-f1af-4cde-f6d1-5c68bdb06bd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 125/125 [00:16<00:00,  7.69it/s]\n"]}]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 125\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 1000 episoded explored\n","a2c_model_multiple_env_125_async =  A2CAgent(env_id, num_episodes=125, max_steps=max_steps, lr=lr, num_steps = max_steps, vectorization_mode= \"async\")\n","\n","rewards_125_async, steps_125_async = a2c_model_multiple_env_125_async.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6OM5xxpj-Io","executionInfo":{"status":"ok","timestamp":1747574813667,"user_tz":-540,"elapsed":35396,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"d8d73cee-5a76-483c-8193-721c7ab8c254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 125/125 [00:35<00:00,  3.55it/s]\n"]}]},{"cell_type":"code","source":["sum(steps_125), sum(steps_125_async)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QbA94YzYKmF","executionInfo":{"status":"ok","timestamp":1747574830986,"user_tz":-540,"elapsed":6,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"4a1b9e93-e6da-4294-97ce-45d2f36a38e2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(np.int64(8683), np.int64(17795))"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["steps_125_async"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWVB5NZTkm13","executionInfo":{"status":"ok","timestamp":1747574763362,"user_tz":-540,"elapsed":17,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"d31f725f-e056-448d-8337-21cdf7415f5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 56,  28,  35,  28,  23,  32,  79,  30,  20,  60,  47,  35,  34,\n","        29,  30,  35,  35,  34,  26,  31,  32,  31,  37,  64,  34,  45,\n","        61,  31,  73,  42,  41,  63,  44,  31,  63,  18,  32,  26,  51,\n","        37,  36,  35,  22,  40,  41,  44,  43,  48,  30,  27,  36,  85,\n","        29,  58,  79,  56,  67,  40,  96,  44, 102,  57, 127,  85,  64,\n","       155,  61,  87,  87,  98, 109, 108,  87, 116, 140, 121, 171, 241,\n","       101, 196, 153, 266, 202, 194, 194, 183, 142, 192, 219, 174, 288,\n","       338, 202, 162, 330, 262, 390, 446, 192, 202, 286, 302, 149, 181,\n","       269, 297, 471, 413, 440, 478, 394, 433, 250, 317, 245, 130, 142,\n","       215, 196, 327, 284, 499, 346, 211, 208])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env_125.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLoIDtGZM8Vb","executionInfo":{"status":"ok","timestamp":1747574838374,"user_tz":-540,"elapsed":889,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"4c16a9c7-8e78-477c-fac5-46100ef5f9ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 196.0\n","Episode 2 Reward: 220.0\n","Episode 3 Reward: 192.0\n","Episode 4 Reward: 164.0\n","Episode 5 Reward: 204.0\n","Episode 6 Reward: 202.0\n","Episode 7 Reward: 162.0\n","Episode 8 Reward: 196.0\n","Episode 9 Reward: 197.0\n","Episode 10 Reward: 215.0\n","Average Reward over 10 episodes: 194.8\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env_125_async.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahZYfekzkOiq","executionInfo":{"status":"ok","timestamp":1747574839521,"user_tz":-540,"elapsed":1138,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"02d1dc2a-0e57-4234-cea9-5b501a6c011f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 274.0\n","Episode 2 Reward: 330.0\n","Episode 3 Reward: 361.0\n","Episode 4 Reward: 280.0\n","Episode 5 Reward: 418.0\n","Episode 6 Reward: 327.0\n","Episode 7 Reward: 338.0\n","Episode 8 Reward: 226.0\n","Episode 9 Reward: 267.0\n","Episode 10 Reward: 254.0\n","Average Reward over 10 episodes: 307.5\n"]}]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 10\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 80 episodes explored\n","a2c_model_multiple_env_10 =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env_10.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHgJhh8H6YjC","executionInfo":{"status":"ok","timestamp":1747233990693,"user_tz":-540,"elapsed":10432,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"d232b207-91b3-4579-a23d-fbcb4ab7b51a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:10<00:00,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["5000\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env_10.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRVv-c8o6eXJ","executionInfo":{"status":"ok","timestamp":1747233991168,"user_tz":-540,"elapsed":469,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"389a5e92-ec99-47fc-fa19-a6da194635a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 134.0\n","Episode 2 Reward: 132.0\n","Episode 3 Reward: 127.0\n","Episode 4 Reward: 122.0\n","Episode 5 Reward: 121.0\n","Episode 6 Reward: 107.0\n","Episode 7 Reward: 111.0\n","Episode 8 Reward: 101.0\n","Episode 9 Reward: 115.0\n","Episode 10 Reward: 123.0\n","Average Reward over 10 episodes: 119.3\n"]}]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[],"authorship_tag":"ABX9TyPBi+VrMp30XwC8+8WHhnKf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}