{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO32t9kB91jfR1n9H4nYB2m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vE4Q3j3jIkHQ","executionInfo":{"status":"ok","timestamp":1746409798424,"user_tz":-540,"elapsed":21057,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"5c0f569f-0373-4b12-8e0e-4a1ef7a47eb5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CU_HjAwbcYKa","executionInfo":{"status":"ok","timestamp":1746409800525,"user_tz":-540,"elapsed":2098,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","\n","env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n","n_states = env.observation_space.n\n","n_actions = env.action_space.n\n","\n","Q_sarsa = np.zeros((n_states, n_actions))\n","Q_qlearn = np.zeros((n_states, n_actions))\n","\n","alpha = 0.1\n","gamma = 0.99\n","epsilon = 0.1\n","episodes = 500\n","\n","# Choosing greedy action with prob 1 - epsilon, random with prob epsilon. Higher epsilon -> more exploration, slower convergence\n","def epsilon_greedy(Q, state):\n","    if np.random.rand() < epsilon:\n","        return env.action_space.sample()\n","    return np.argmax(Q[state])\n","\n","\n","for ep in range(episodes):\n","    state_sarsa, _ = env.reset()\n","    state_q, _ = env.reset()\n","\n","    # SARSA\n","    action_sarsa = epsilon_greedy(Q_sarsa, state_sarsa)\n","    done = False\n","    while not done:\n","        next_state, reward, terminated, truncated, _ = env.step(action_sarsa)\n","        done = terminated or truncated\n","        next_action = epsilon_greedy(Q_sarsa, next_state)\n","        # Updates Q matrix with term gamma * Q[next_state, next_action)] -> on-policy because it updates based on the next action which is the real trajectory\n","        Q_sarsa[state_sarsa, action_sarsa] += alpha * (reward + gamma * Q_sarsa[next_state, next_action] - Q_sarsa[state_sarsa, action_sarsa])\n","        state_sarsa, action_sarsa = next_state, next_action\n","\n","    # Q-learning\n","    state_q, _ = env.reset()\n","    done = False\n","    while not done:\n","        action = epsilon_greedy(Q_qlearn, state_q)\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        # Updates Q matrix with term gamma * np.max(Q_[next_state]) -> off-policy because it chooses greedily after next_state\n","        Q_qlearn[state_q, action] += alpha * (reward + gamma * np.max(Q_qlearn[next_state]) - Q_qlearn[state_q, action])\n","        state_q = next_state"]},{"cell_type":"code","source":[],"metadata":{"id":"JiS1aDQacmQB"},"execution_count":null,"outputs":[]}]}