{"cells":[{"cell_type":"code","source":["\"\"\"\n","Implementing PPO\n","First mistake I made -> ran the action using the policy, then calculated the ratio using old_policy. The correct way is to run the action using old_policy, then calculate the ratio using the new policy.\n","Second mistake -> didn't run K epochs while calculating the surrogate objective.\n","Then, it started behaving like a trained model, though did not achieve goal.\n","Another fix. Added minibatching according to the original paper.\n","Minibatching made the training significantly slower due to more frequent optimizer steps.\n","\"\"\""],"metadata":{"id":"UG4iiEKdsB2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21539,"status":"ok","timestamp":1748169063399,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aNbCFO1m96R1","outputId":"a8925ba7-428c-49c1-c267-c2cc47506f7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77508,"status":"ok","timestamp":1748264123932,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"0Va0BgsV98Lt","outputId":"15307c5f-7e87-4ec4-ee9a-6cc96e6b3c94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n","Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379371 sha256=f2a46bc6d62fbb5c7e4d8f71af7d1d4564c0c774b848e7437199f522a648ca31\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9367,"status":"ok","timestamp":1748264133316,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aVKs6T5ifpUd","outputId":"008e6fe5-8201-4fe9-a4ce-f70baa7a86fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: box2d-py 2.3.5\n","Uninstalling box2d-py-2.3.5:\n","  Successfully uninstalled box2d-py-2.3.5\n","Collecting box2d\n","  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n"]}],"source":["!pip uninstall -y box2d-py\n","!pip install box2d pygame swig\n","!pip install \"gymnasium[box2d]\" --no-deps"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1748269464461,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KJwVWxwiW0Q3"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCriticPPO(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim, hidden_dims=(64, 64)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1748270997729,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"GYE9dNX2-NMy"},"outputs":[],"source":["from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","import copy\n","\n","class PPOAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, epsilon=float('inf'), gamma=0.99, lambda_GAE=1, lr=1e-3, num_steps=0, num_envs=8, num_epochs=4, minibatch_size=4, vectorization_mode = \"sync\", seed=123):\n","        # using vectorized environments to boost training speed\n","        self.env = gym.make_vec(env_id, num_envs=num_envs, vectorization_mode=vectorization_mode)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.num_epochs = num_epochs\n","        self.max_steps = max_steps\n","        self.minibatch_size = minibatch_size\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","        self.lambda_GAE = lambda_GAE\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCriticPPO(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.old_policy_net = copy.deepcopy(self.policy_net)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        # added scheduler after observing divergence after getting close to solving\n","        self.scheduler = StepLR(self.optimizer, step_size=100, gamma=0.9)\n","        self.loss = nn.MSELoss()\n","        self.seed = seed\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.old_policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing GAE\n","    # def compute_returns(self, rewards, values, next_value):\n","    #     rewards = torch.stack(rewards)\n","    #     values = torch.cat([values, next_value.unsqueeze(0)], dim=0)\n","\n","    #     T, N = rewards.shape\n","    #     advantages = torch.zeros_like(rewards)\n","    #     gae = torch.zeros(N, device=rewards.device)\n","    #     for t in reversed(range(T)):\n","    #         # temporal difference error\n","    #         td = rewards[t] + self.gamma * values[t + 1] - values[t]\n","    #         # higher labmda -> more sampling, lower lambda -> more bootstrapping\n","    #         gae = td + self.gamma * self.lambda_GAE * gae\n","    #         advantages[t] = gae\n","\n","    #     # compute returns by adding value to advantage\n","    #     returns = advantages + values[:-1]\n","    #     returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","\n","    #     # normalize advantage across timesteps and environments\n","    #     advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","    #     return returns, advantages\n","\n","    # computing the gamma decaying rewards\n","    def compute_returns(self, rewards, values, next_value):\n","        \"\"\"\n","        Args:\n","            rewards: torch.Tensor of shape [T, N] where\n","                    T = rollout steps, N = num_envs\n","        Returns:\n","            returns: torch.Tensor of shape [T, N], normalized\n","        \"\"\"\n","        rewards = torch.stack(rewards)\n","\n","        T, N = rewards.shape\n","        returns = torch.zeros_like(rewards)\n","        R = torch.zeros(N, device=rewards.device)\n","        for t in reversed(range(T)):\n","            R = rewards[t] + self.gamma * R\n","            returns[t] = R\n","\n","        # Normalize returns across all timesteps and environments\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","        advantages = returns - values\n","\n","        return returns, advantages\n","\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset(seed=self.seed + episode)\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            old_states, old_actions, old_log_probs, old_rewards, old_values = [], [], [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            done_steps = np.zeros(self.num_envs)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                old_states.append(state_tensor)\n","\n","                with torch.no_grad():\n","                    old_action_probs, old_value = self.old_policy_net(state_tensor)\n","                    old_action_dist = torch.distributions.Categorical(old_action_probs)\n","                    old_action = old_action_dist.sample()\n","                    old_log_prob = old_action_dist.log_prob(old_action)\n","\n","                next_state, old_reward, terminated, truncated, _ = self.env.step(old_action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","                done_mask = np.logical_or(done_mask, done)\n","                # record when each environment is done\n","                old_reward = np.where(done_mask, 0.0, old_reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                old_values.append(old_value.squeeze())\n","                old_rewards.append(torch.tensor(old_reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                old_log_probs.append(old_log_prob)\n","                old_actions.append(old_action)\n","\n","                episode_reward += old_reward\n","                state = next_state\n","\n","\n","                # finish full trajectory, then update\n","                if self.num_steps == 0:\n","                    if np.any(done):\n","                        with torch.no_grad():\n","                            next_old_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                            _, next_old_value = self.policy_net(next_old_state_tensor)\n","                            done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                            next_old_value = next_old_value.squeeze() * (1 - done_tensor)\n","\n","                        old_states = torch.stack(old_states).to(device)\n","                        old_values = torch.stack(old_values).to(device)\n","                        old_log_probs = torch.stack(old_log_probs).to(device)\n","                        old_actions = torch.stack(old_actions).to(device)\n","                        old_returns, old_advantages = self.compute_returns(old_rewards, old_values, next_old_value)\n","\n","                        # flatten inputs\n","                        T, N = old_actions.shape[:2]\n","                        old_states = old_states.reshape(T * N, -1)\n","                        old_actions = old_actions.reshape(T * N)\n","                        old_log_probs = old_log_probs.reshape(T * N)\n","                        old_returns = old_returns.reshape(T * N)\n","                        old_advantages = old_advantages.reshape(T * N)\n","\n","                        batch_size = old_states.shape[0]\n","                        minibatch_size = self.minibatch_size\n","\n","                        # repeat in K epochs\n","                        for _ in range(self.num_epochs):\n","                            indices = torch.randperm(batch_size)\n","\n","                            # mini batching\n","                            for i in range(0, batch_size, minibatch_size):\n","                                # selecting inputs for this mini batch\n","                                mb_idx = indices[i : i + minibatch_size]\n","                                mb_states = old_states[mb_idx]\n","                                mb_actions = old_actions[mb_idx]\n","                                mb_log_probs_old = old_log_probs[mb_idx]\n","                                mb_returns = old_returns[mb_idx]\n","                                mb_advantages = old_advantages[mb_idx]\n","\n","                                # use current policy to find log_probs of the trajectory ran by old_policy\n","                                action_probs, values = self.policy_net(mb_states)\n","                                action_dist = torch.distributions.Categorical(action_probs)\n","                                mb_log_probs = action_dist.log_prob(mb_actions)\n","\n","                                prob_ratio = torch.exp(mb_log_probs - mb_log_probs)\n","                                values = values.squeeze(-1)\n","                                surrogate = torch.min(prob_ratio * mb_advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * mb_advantages)\n","                                actor_loss = -surrogate.mean()\n","                                # clipped_values = torch.clamp(values, old_values.detach() * (1 - self.epsilon), old_values.detach() * (1 + self.epsilon))\n","                                # critic_loss = min(self.loss(returns, values), self.loss(returns, clipped_values))\n","                                critic_loss = self.loss(mb_returns, values)\n","\n","                                loss = actor_loss + critic_loss\n","                                self.optimizer.zero_grad()\n","                                loss.backward()\n","                                self.optimizer.step()\n","\n","                        old_states, old_actions, old_log_probs, old_rewards, old_values = [], [], [], [], []\n","                        # self.scheduler.step()\n","\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                else:\n","                    if (steps % self.num_steps == 0) or np.any(done):\n","                        with torch.no_grad():\n","                            next_old_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                            _, next_old_value = self.policy_net(next_old_state_tensor)\n","                            done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                            next_old_value = next_old_value.squeeze() * (1 - done_tensor)\n","\n","                        old_states = torch.stack(old_states).to(device)\n","                        old_values = torch.stack(old_values).to(device)\n","                        old_log_probs = torch.stack(old_log_probs).to(device)\n","                        old_actions = torch.stack(old_actions).to(device)\n","                        old_returns, old_advantages = self.compute_returns(old_rewards, old_values, next_old_value)\n","\n","                        # flatten inputs\n","                        T, N = old_actions.shape[:2]\n","                        old_states = old_states.reshape(T * N, -1)\n","                        old_actions = old_actions.reshape(T * N)\n","                        old_log_probs = old_log_probs.reshape(T * N)\n","                        old_returns = old_returns.reshape(T * N)\n","                        old_advantages = old_advantages.reshape(T * N)\n","\n","                        batch_size = old_states.shape[0]\n","                        minibatch_size = self.minibatch_size\n","\n","                        # repeat in K epochs\n","                        for _ in range(self.num_epochs):\n","                            indices = torch.randperm(batch_size)\n","\n","                            # mini batching\n","                            for i in range(0, batch_size, minibatch_size):\n","                                # selecting inputs for this mini batch\n","                                mb_idx = indices[i : i + minibatch_size]\n","                                mb_states = old_states[mb_idx]\n","                                mb_actions = old_actions[mb_idx]\n","                                mb_log_probs_old = old_log_probs[mb_idx]\n","                                mb_returns = old_returns[mb_idx]\n","                                mb_advantages = old_advantages[mb_idx]\n","\n","                                # use current policy to find log_probs of the trajectory ran by old_policy\n","                                action_probs, values = self.policy_net(mb_states)\n","                                action_dist = torch.distributions.Categorical(action_probs)\n","                                mb_log_probs = action_dist.log_prob(mb_actions)\n","\n","                                prob_ratio = torch.exp(mb_log_probs - mb_log_probs_old)\n","                                values = values.squeeze(-1)\n","                                surrogate = torch.min(prob_ratio * mb_advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * mb_advantages)\n","                                actor_loss = -surrogate.mean()\n","                                # clipped_values = torch.clamp(values, old_values.detach() * (1 - self.epsilon), old_values.detach() * (1 + self.epsilon))\n","                                # critic_loss = min(self.loss(returns, values), self.loss(returns, clipped_values))\n","                                critic_loss = self.loss(mb_returns, values)\n","\n","                                loss = actor_loss + critic_loss\n","                                self.optimizer.zero_grad()\n","                                loss.backward()\n","                                self.optimizer.step()\n","\n","                        old_states, old_actions, old_log_probs, old_rewards, old_values = [], [], [], [], []\n","                        # self.scheduler.step()\n","\n","\n","            self.old_policy_net = copy.deepcopy(self.policy_net)\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","            if episode % 20 == 0:\n","               print('Episode {}\\tlengths: {}\\treward: {}]\\tfull length: {}'.format(episode, done_steps, episode_reward, steps))\n","            if episode % 10 == 0:\n","                print(f\"\\n[Episode {episode}]\")\n","                print(f\"Reward (mean): {np.mean(episode_reward):.2f}\")\n","                print(f\"Actor Loss: {actor_loss.item():.4f} | Critic Loss: {critic_loss.item():.4f}\")\n","                print(f\"Prob Ratio - mean: {prob_ratio.mean().item():.4f}, max: {prob_ratio.max().item():.4f}, min: {prob_ratio.min().item():.4f}\")\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xS4b8URHWybG","outputId":"facd4d74-1097-4429-f276-abae52daf796"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  1%|          | 1/100 [00:02<04:47,  2.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Episode 0\tlengths: [ 68.  75. 100.  79.  91. 100. 116.  65.]\treward: [  23.42290852  -21.0415077  -185.99135794 -122.54408225 -133.07595182\n","  -72.50355055 -164.4432558     1.19082493]]\tfull length: 116\n","\n","[Episode 0]\n","Reward (mean): -84.37\n","Actor Loss: -0.1178 | Critic Loss: 0.1815\n","Prob Ratio - mean: 1.0107, max: 1.0831, min: 0.8825\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[" 11%|█         | 11/100 [00:35<04:21,  2.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","[Episode 10]\n","Reward (mean): -161.28\n","Actor Loss: -0.4140 | Critic Loss: 1.8274\n","Prob Ratio - mean: 1.0174, max: 1.2128, min: 0.8223\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[" 21%|██        | 21/100 [01:10<04:25,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Episode 20\tlengths: [107. 100. 136. 133.  82.  94. 124. 118.]\treward: [-284.2452314   -25.55426189 -163.74740984   -3.49345575   -7.73744353\n"," -165.24696666 -210.96473393 -309.9329062 ]]\tfull length: 136\n","\n","[Episode 20]\n","Reward (mean): -146.37\n","Actor Loss: -0.1504 | Critic Loss: 0.1337\n","Prob Ratio - mean: 0.9907, max: 1.1293, min: 0.7797\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[" 31%|███       | 31/100 [01:53<05:19,  4.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","[Episode 30]\n","Reward (mean): -107.98\n","Actor Loss: 0.0385 | Critic Loss: 0.6195\n","Prob Ratio - mean: 0.9770, max: 1.1405, min: 0.8083\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[" 41%|████      | 41/100 [02:50<06:26,  6.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Episode 40\tlengths: [180. 108. 150. 148. 114.  98. 136. 143.]\treward: [ 15.61733602  76.55506404  37.81381911  -0.58695517  80.46928083\n","  84.41673134  58.24204205 128.58270396]]\tfull length: 180\n","\n","[Episode 40]\n","Reward (mean): 60.14\n","Actor Loss: -0.2583 | Critic Loss: 0.9100\n","Prob Ratio - mean: 1.0360, max: 1.3309, min: 0.8516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[" 51%|█████     | 51/100 [04:16<06:25,  7.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","[Episode 50]\n","Reward (mean): 66.05\n","Actor Loss: -0.1918 | Critic Loss: 0.7023\n","Prob Ratio - mean: 1.0338, max: 1.1305, min: 0.9037\n"]},{"output_type":"stream","name":"stderr","text":[" 59%|█████▉    | 59/100 [04:52<02:44,  4.00s/it]"]}],"source":["import gymnasium as gym\n","\n","env_id = 'LunarLander-v3'\n","num_episodes = 100\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=8, num_steps=32, minibatch_size=16)\n","\n","rewards, steps = ppo_model.train()\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14200,"status":"ok","timestamp":1748268961780,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KwcnQRLD9cTX","outputId":"895d4373-563b-4126-81c7-164700eb8286"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: -54.78416117561745\n","Episode 2 Reward: -119.94616451933459\n","Episode 3 Reward: -110.71677044528491\n","Episode 4 Reward: -91.66937614894313\n","Episode 5 Reward: -131.8112768704142\n","Episode 6 Reward: -51.91420243696872\n","Episode 7 Reward: -102.09082294946438\n","Episode 8 Reward: -94.30228997606928\n","Episode 9 Reward: -116.33677594644324\n","Episode 10 Reward: -80.7753335795721\n","Average Reward over 10 episodes: -95.4347174048112\n"]}],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = ppo_model.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","from gymnasium import spaces\n","import numpy as np\n","\n","class LinearValueEnv(gym.Env):\n","    def __init__(self, gamma=0.99, episode_length=100):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.episode_length = episode_length\n","        self.current_step = 0\n","\n","        # Observation: continuous scalar in [-1, 1]\n","        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n","\n","        # Action: continuous scalar (not used)\n","        self.action_space = spaces.Discrete(1)\n","\n","        self.state = None\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.state = np.random.uniform(-1.0, 1.0, size=(1,)).astype(np.float32)\n","        self.current_step = 0\n","        return self.state.copy(), {}\n","\n","    def step(self, action):\n","        # Reward is simply the state value\n","        reward = float(self.state[0])\n","        self.current_step += 1\n","\n","        terminated = self.current_step >= self.episode_length\n","        truncated = False\n","        return self.state.copy(), reward, terminated, truncated, {}\n","\n","    def render(self):\n","        print(f\"State: {self.state}\")\n","\n","    def close(self):\n","        pass\n","\n","\n"],"metadata":{"id":"gAXsopwasbMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gymnasium.envs.registration import register\n","\n","register(\n","    id=\"LinearValue-v0\",\n","    entry_point=\"__main__:LinearValueEnv\",  # if you're running in a script\n","    max_episode_steps=100\n",")\n"],"metadata":{"id":"UO-G1kxysz46"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQ0d1b4tKwwl"},"outputs":[],"source":[]},{"cell_type":"code","source":["env_id = \"LinearValue-v0\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model_value =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=2, num_steps=0)\n","\n","rewards, steps = ppo_model_value.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"z-BU9qP2s0si","executionInfo":{"status":"error","timestamp":1748184192621,"user_tz":-540,"elapsed":58338,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"0be61751-d655-4b26-d5bb-41f4679651e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/1000 [00:00<12:09,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 0\tlengths: [100. 100.]\treward: [-72.96960783  84.58938628]]\tfull length: 100\n","\n","[Episode 0]\n","Reward (mean): 5.81\n","Actor Loss: 0.3642 | Critic Loss: 0.9067\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  1%|          | 11/1000 [00:03<04:25,  3.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 10]\n","Reward (mean): -41.61\n","Actor Loss: 0.2812 | Critic Loss: 1.0306\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 21/1000 [00:06<04:13,  3.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 20\tlengths: [100. 100.]\treward: [-32.27667063  45.82619548]]\tfull length: 100\n","\n","[Episode 20]\n","Reward (mean): 6.77\n","Actor Loss: 0.3617 | Critic Loss: 1.0112\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 31/1000 [00:08<04:14,  3.81it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 30]\n","Reward (mean): 49.95\n","Actor Loss: 0.4339 | Critic Loss: 1.0572\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 41/1000 [00:11<04:05,  3.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 40\tlengths: [100. 100.]\treward: [-12.85368513  34.16484547]]\tfull length: 100\n","\n","[Episode 40]\n","Reward (mean): 10.66\n","Actor Loss: 0.3643 | Critic Loss: 1.0583\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 51/1000 [00:14<04:51,  3.26it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 50]\n","Reward (mean): -12.35\n","Actor Loss: 0.3224 | Critic Loss: 0.9231\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 61/1000 [00:17<04:10,  3.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 60\tlengths: [100. 100.]\treward: [ 41.3785345  -51.92305827]]\tfull length: 100\n","\n","[Episode 60]\n","Reward (mean): -5.27\n","Actor Loss: 0.3325 | Critic Loss: 0.9675\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 71/1000 [00:19<03:58,  3.90it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 70]\n","Reward (mean): -60.01\n","Actor Loss: 0.2350 | Critic Loss: 1.0500\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 81/1000 [00:22<03:56,  3.89it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 80\tlengths: [100. 100.]\treward: [18.60985883 19.40882935]]\tfull length: 100\n","\n","[Episode 80]\n","Reward (mean): 19.01\n","Actor Loss: 0.3707 | Critic Loss: 1.1324\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 91/1000 [00:25<04:43,  3.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 90]\n","Reward (mean): 54.82\n","Actor Loss: 0.4316 | Critic Loss: 1.1794\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 101/1000 [00:28<03:59,  3.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 100\tlengths: [100. 100.]\treward: [36.77222595 51.44622749]]\tfull length: 100\n","\n","[Episode 100]\n","Reward (mean): 44.11\n","Actor Loss: 0.4109 | Critic Loss: 1.1559\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 111/1000 [00:30<03:50,  3.86it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 110]\n","Reward (mean): 5.18\n","Actor Loss: 0.3404 | Critic Loss: 1.0804\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 121/1000 [00:33<03:42,  3.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 120\tlengths: [100. 100.]\treward: [-27.61505038  77.55969197]]\tfull length: 100\n","\n","[Episode 120]\n","Reward (mean): 24.97\n","Actor Loss: 0.3734 | Critic Loss: 0.9805\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 131/1000 [00:35<03:45,  3.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 130]\n","Reward (mean): -27.30\n","Actor Loss: 0.2786 | Critic Loss: 0.9290\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 141/1000 [00:39<04:24,  3.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 140\tlengths: [100. 100.]\treward: [-91.24863052  44.33011508]]\tfull length: 100\n","\n","[Episode 140]\n","Reward (mean): -23.46\n","Actor Loss: 0.2832 | Critic Loss: 0.8766\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 151/1000 [00:41<03:39,  3.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 150]\n","Reward (mean): -92.36\n","Actor Loss: 0.1577 | Critic Loss: 1.0184\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 161/1000 [00:44<03:34,  3.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 160\tlengths: [100. 100.]\treward: [ 3.80020684 92.52385944]]\tfull length: 100\n","\n","[Episode 160]\n","Reward (mean): 48.16\n","Actor Loss: 0.4072 | Critic Loss: 1.0393\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 171/1000 [00:46<03:33,  3.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 170]\n","Reward (mean): -14.98\n","Actor Loss: 0.2918 | Critic Loss: 0.9436\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 181/1000 [00:49<04:19,  3.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 180\tlengths: [100. 100.]\treward: [-11.27119991   1.15310721]]\tfull length: 100\n","\n","[Episode 180]\n","Reward (mean): -5.06\n","Actor Loss: 0.3075 | Critic Loss: 1.0708\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▉        | 191/1000 [00:52<03:34,  3.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 190]\n","Reward (mean): 41.69\n","Actor Loss: 0.3900 | Critic Loss: 1.0467\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 201/1000 [00:55<03:23,  3.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 200\tlengths: [100. 100.]\treward: [-81.71754402 -39.18708792]]\tfull length: 100\n","\n","[Episode 200]\n","Reward (mean): -60.45\n","Actor Loss: 0.2027 | Critic Loss: 0.9949\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██        | 211/1000 [00:57<03:23,  3.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 210]\n","Reward (mean): -59.89\n","Actor Loss: 0.2012 | Critic Loss: 1.0341\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██▏       | 213/1000 [00:58<03:35,  3.65it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-100-781daad46b2d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mppo_model_value\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_model_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-99-5553c9874b05>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                         \u001b[0;31m# self.scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPiBQb0leCtrJodNYbyOL3/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}