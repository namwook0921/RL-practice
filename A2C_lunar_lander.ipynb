{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LA6ipJjA9UFJ"},"outputs":[],"source":["# LunarLander debug note\n","# Initially, used the same code I used for cartpole.\n","# Would not train, so printed out the probs. When the model is first made, it is fairly distributed like this [[0.2660, 0.2257, 0.2731, 0.2352]]. However, after training for 1000 steps,\n","# it converges to a certain action like this - tensor([[9.9943e-01, 4.8898e-04, 7.9276e-05, 4.1656e-08]]\n","# Applied entropy for more exploration. Didn't work\n","# Found much larger absolute value of advantages, returns, compared to cartpole. Especially, when terminated, the return is -100 which is the dominant cause for the returns.\n","# Normalizing advantage solved the problem above. Now it doesn't fixate in a certain action.\n","# Then, action_probability turned into a somewhat uniform distribution. Therefore, printed out the losses.\n","# Actor loss:  tensor(-1.9073e-08, device='cuda:0', grad_fn=<NegBackward0>) Critic loss:  tensor(15.9172, device='cuda:0', grad_fn=<MseLossBackward0>)\n","# Entropy:  tensor(1.3784, device='cuda:0', grad_fn=<MeanBackward0>)\n","# Found out that Actor loss is extremely small. Therefore, actor layer was barely getting updated.\n","# Reason: log prob is pretty much uniform when model is initialized and the advantage is normalized with mean 0. We calculate actor_loss = (advantages * log_probs).mean()\n","# This converges to the mean of the normal distribution (since log_probs is uniform), and therefore to 0.\n","# Try with normalizing the n-step rewards instead of the advantage.\n","# Reference https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/blob/master/train.py\n","# The only difference between my model and the reference model was the reduction method of actor and critic loss. Reference model used sum, my model used mean.\n","# mean vs. sum was not the problem. The dimensions of log_probs, returns, state_values were all different in my code and was broadcasting very wierdly. That's why step-wise calculation of the reference code\n","# worked, but my code didn't. Should pay more attention to dimensions, and also debugging warnings because I was ignoring the dimension disparity.\n","# Trains well using mean. Maybe even better.\n","# Solved all the bugs and trained both sync and async models. Both took too much time and didn't train well.\n","# Compared to the reference code which ran in a single environment, I had my code running in 8 environments, and therefore decreased the learning rate from 0.02 -> 0.005.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":18029,"status":"error","timestamp":1746622996822,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aNbCFO1m96R1","outputId":"b4c2ba5b-4c48-4b4d-d2cb-ef95ab68535f"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    355\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Va0BgsV98Lt","executionInfo":{"status":"ok","timestamp":1747832160224,"user_tz":-540,"elapsed":75295,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"dd7daf9f-b9aa-4e63-f0c5-7a690d6b30df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n","Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379412 sha256=fb06db08284b83565f988532c29aa4bc5905aa0239f232e37dda892d10f2a008\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","source":["!pip uninstall -y box2d-py\n","!pip install box2d pygame swig\n","!pip install \"gymnasium[box2d]\" --no-deps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aVKs6T5ifpUd","executionInfo":{"status":"ok","timestamp":1747573456733,"user_tz":-540,"elapsed":3833,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"219417d0-e99b-4b8a-a121-ac770cc91ffb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping box2d-py as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting box2d\n","  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aZCrgHbE94dZ","executionInfo":{"status":"ok","timestamp":1747832166913,"user_tz":-540,"elapsed":6679,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["# New implementation referring to https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/blob/master/train.py\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","\n","class ActorCritic(nn.Module):\n","\n","    # Increased hidden dim to (128, 128) compared to cartpole as the input_dim is more complex\n","    def __init__(self, input_dim, output_dim, hidden_dims=(128, 128)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            # nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            # nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[0], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[0], 1)\n","\n","        self.logprobs = []\n","        self.state_values = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        state_value = self.critic_layer(x)\n","\n","        action_distribution = Categorical(action_probs)\n","        action = action_distribution.sample()\n","\n","        self.logprobs.append(action_distribution.log_prob(action))\n","        self.state_values.append(state_value.squeeze())\n","\n","        return action.item()\n","\n","    def compute_return(self, gamma):\n","        returns = []\n","        R = 0\n","        for r in reversed(self.rewards):\n","            R = r + gamma * R\n","            returns.insert(0, R)\n","\n","        returns = torch.tensor(returns)\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","        return returns\n","\n","    def calculate_loss(self, gamma):\n","        rewards = self.compute_return(gamma).detach()\n","\n","        # CRITICAL BUG - log_probs' shape was [90, 1], state_values' [90, 1, 1], rewards' [90] -- should read the warning messages more carefully from now on.\n","        # print(\"log_probs.shape\", self.logprobs.shape)\n","        # print(\"state_values.shape\", self.state_values.shape)\n","        # print(\"returns.shape\", returns.shape)\n","\n","        loss = 0\n","        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n","            value = value.to('cpu')\n","            advantage = reward - value.detach()\n","            action_loss = -logprob * advantage\n","            value_loss = F.smooth_l1_loss(value, reward)\n","            loss += (action_loss + value_loss)\n","        return loss\n","\n","\n","    def clearMemory(self):\n","        del self.logprobs[:]\n","        del self.state_values[:]\n","        del self.rewards[:]\n","\n"]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        random_seed = 543\n","        torch.manual_seed(random_seed)\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","                action = self.policy_net(state_tensor)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action)\n","                done = terminated or truncated\n","                self.policy_net.rewards.append(reward)\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                # if (steps % self.num_steps == 0) or done:\n","                if done:\n","                    self.optimizer.zero_grad()\n","                    loss = self.policy_net.calculate_loss(self.gamma)\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    self.policy_net.clearMemory()\n","\n","            if episode % 100 == 0:\n","                print(episode, \"reward: \", episode_reward, \"steps: \", steps)\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"],"metadata":{"id":"fkfHOIJ5DxWh","executionInfo":{"status":"ok","timestamp":1747832166919,"user_tz":-540,"elapsed":7,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-DxncwMM9Zn8","colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"status":"error","timestamp":1747832214653,"user_tz":-540,"elapsed":47461,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"3a3634db-4027-43f8-93ac-ab9b48d1c349"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 2/2000 [00:01<19:16,  1.73it/s]"]},{"output_type":"stream","name":"stdout","text":["0 reward:  -305.98365456353747 steps:  90\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 102/2000 [00:27<11:52,  2.67it/s]"]},{"output_type":"stream","name":"stdout","text":["100 reward:  -398.5175385073768 steps:  195\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▋         | 127/2000 [00:37<09:14,  3.38it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4cb7d9b71181>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2c_model_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma2c_model_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-33d7f9e03a4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import gymnasium as gym\n","\n","env = gym.make('LunarLander-v3')\n","env.reset(seed=543)\n","num_episodes = 2000\n","max_steps = 1000\n","lr = 0.02\n","\n","\n","a2c_model_ll =  A2CAgent(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 8)\n","\n","state, _ = env.reset()\n","state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(a2c_model_ll.device)\n","rewards, steps = a2c_model_ll.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxOavbFc2WGX","executionInfo":{"status":"aborted","timestamp":1747832214634,"user_tz":-540,"elapsed":47697,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(len(rewards)), rewards)\n","plt.show()"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCritic(nn.Module):\n","\n","    # reference model used only 1 layer. Will experiment both\n","    def __init__(self, input_dim, output_dim, hidden_dims=(128, 128)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            # nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            # nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n"],"metadata":{"id":"KJwVWxwiW0Q3","executionInfo":{"status":"ok","timestamp":1747832221919,"user_tz":-540,"elapsed":17,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":35,"metadata":{"id":"GYE9dNX2-NMy","executionInfo":{"status":"ok","timestamp":1747837754973,"user_tz":-540,"elapsed":17,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","\n","class A2CAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5, num_envs = 8, vectorization_mode = \"sync\"):\n","        # using vectorized environments to boost training speed\n","        def make_env(env_id, seed, idx):\n","            def thunk():\n","                env = gym.make(env_id)\n","                env.reset(seed=seed + idx)\n","                return env\n","            return thunk\n","        env_fns = [make_env(env_id, seed=543, idx=i) for i in range(num_envs)]\n","        self.env = gym.vector.SyncVectorEnv(env_fns)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        # added scheduler after observing divergence after getting close to solving\n","        self.scheduler = StepLR(self.optimizer, step_size=100, gamma=0.9)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_returns(self, rewards):\n","        \"\"\"\n","        Args:\n","            rewards: torch.Tensor of shape [T, N] where\n","                    T = rollout steps, N = num_envs\n","        Returns:\n","            returns: torch.Tensor of shape [T, N], normalized\n","        \"\"\"\n","        rewards = torch.stack(rewards)\n","\n","        T, N = rewards.shape\n","        returns = torch.zeros_like(rewards)\n","        R = torch.zeros(N, device=rewards.device)\n","        for t in reversed(range(T)):\n","            R = rewards[t] + self.gamma * R\n","            returns[t] = R\n","\n","        # Normalize returns across all timesteps and environments\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","\n","        return returns\n","\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        returns = torch.stack(returns)\n","        if returns.numel() > 1:\n","            return (returns - returns.mean()) / (returns.std() + 1e-8)\n","        else:\n","            return returns * 0\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs = [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            done_steps = np.zeros(self.num_envs)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","                done_mask = np.logical_or(done_mask, done)\n","                # record when each environment is done\n","                reward = np.where(done_mask, 0.0, reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                # if (steps % self.num_steps == 0) or np.any(done):\n","                if np.any(done):\n","                    with torch.no_grad():\n","                        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_value = self.policy_net(next_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                    returns = self.compute_returns(rewards)  # shape: (n_steps, num_envs)\n","                    returns = returns.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    values = torch.stack(values).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    log_probs = torch.stack(log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    advantages = returns - values\n","\n","                    # calculate sum instead of mean\n","                    actor_loss = - (log_probs * advantages.detach()).sum()\n","                    critic_loss = self.loss(returns, values)\n","\n","                    loss = actor_loss + critic_loss\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    # self.scheduler.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","            if episode % 20 == 0:\n","               print('Episode {}\\tlengths: {}\\treward: {}]\\tfull length: {}'.format(episode, done_steps, episode_reward, steps))\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env_id = 'LunarLander-v3'\n","num_episodes = 80\n","max_steps = 500\n","lr = 3e-3\n","\n","\n","a2c_model_ll =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 8)\n","\n","rewards, steps = a2c_model_ll.train()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xS4b8URHWybG","executionInfo":{"status":"ok","timestamp":1747838122812,"user_tz":-540,"elapsed":57618,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"a4226c3b-b509-40b8-8078-e6ea36731d0d"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["  1%|▏         | 1/80 [00:00<00:57,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 0\tlengths: [ 99.  98.  94.  70. 109.  63. 126.  69.]\treward: [-166.39995955  -51.20753231 -242.42402877  -89.14457934  -93.3466435\n","   37.57557693  -23.76910308   21.95506093]]\tfull length: 126\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▋       | 21/80 [00:11<00:50,  1.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 20\tlengths: [  0. 167. 135. 200. 160. 130. 201. 211.]\treward: [ 113.22333902 -155.05465395   70.33711321   92.95241842 -173.71621362\n","   49.55416256   32.52047522   88.13744365]]\tfull length: 500\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████▏    | 41/80 [00:22<00:26,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 40\tlengths: [156. 125. 109.  99.  95. 128. 124. 156.]\treward: [ 40.43781212 108.08222072  93.48589167 136.50936247  94.33565674\n"," 102.27260246 107.14904484 123.99814906]]\tfull length: 156\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▋  | 61/80 [00:38<00:12,  1.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 60\tlengths: [ 91. 122.  98.  97.  85. 122. 121. 122.]\treward: [104.27137147 113.65733053 146.81514964  68.16942896  27.57427676\n","  95.27770793  87.95648027 112.62542008]]\tfull length: 122\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 80/80 [00:57<00:00,  1.39it/s]\n"]}]},{"cell_type":"code","source":["# env_id = 'LunarLander-v3'\n","# num_episodes = 250\n","# max_steps = 500\n","# lr = 3e-3\n","\n","# a2c_model_ll_async =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 8, vectorization_mode=\"async\")\n","\n","# rewards_async, steps_async = a2c_model_ll_async.train()"],"metadata":{"id":"0rXhsqr0m7sh","executionInfo":{"status":"ok","timestamp":1747836906038,"user_tz":-540,"elapsed":3,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64405,"status":"ok","timestamp":1747838224705,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KwcnQRLD9cTX","outputId":"32b0dbe1-9ad8-47a5-a074-c642db89c1ac"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: -189.19590152462467\n","Episode 2 Reward: -125.67309362208087\n","Episode 3 Reward: -122.27225200262494\n","Episode 4 Reward: -240.09653210736192\n","Episode 5 Reward: -209.4713997490183\n","Episode 6 Reward: -218.93050849994881\n","Episode 7 Reward: -311.4440448859407\n","Episode 8 Reward: -198.4887331130318\n","Episode 9 Reward: -217.7466268388127\n","Episode 10 Reward: -199.33068248707443\n","Average Reward over 10 episodes: -203.26497748305192\n"]}],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_ll.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, name_prefix=\"async_demo\", episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_ll_async.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"id":"YsQWqwPEr6Vz","executionInfo":{"status":"aborted","timestamp":1747834633692,"user_tz":-540,"elapsed":2,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fr8M0ZjX0vhY"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyO9LSfbOSpEByi7PPXee1AA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}