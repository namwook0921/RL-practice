{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UG4iiEKdsB2l"},"outputs":[],"source":["\"\"\"\n","Implementing PPO\n","First mistake I made -> ran the action using the policy, then calculated the ratio using old_policy. The correct way is to run the action using old_policy, then calculate the ratio using the new policy.\n","Second mistake -> didn't run K epochs while calculating the surrogate objective.\n","Then, it started behaving like a trained model, though did not achieve goal.\n","Another fix. Added minibatching according to the original paper.\n","Minibatching made the training significantly slower due to more frequent optimizer steps.\n","Experimented both GAE and decaying returns. Both did not show good results. Will try debugging in Cartpole.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21539,"status":"ok","timestamp":1748169063399,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aNbCFO1m96R1","outputId":"a8925ba7-428c-49c1-c267-c2cc47506f7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Va0BgsV98Lt","outputId":"23e90b5b-0783-4730-c50c-a2c99a025b86","executionInfo":{"status":"ok","timestamp":1748525196453,"user_tz":-540,"elapsed":11181,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4137,"status":"ok","timestamp":1748525120538,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aVKs6T5ifpUd","outputId":"eb03ce50-1c98-489b-9bfd-e8e007757791"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: box2d-py 2.3.5\n","Uninstalling box2d-py-2.3.5:\n","  Successfully uninstalled box2d-py-2.3.5\n","Requirement already satisfied: box2d in /usr/local/lib/python3.11/dist-packages (2.3.10)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n"]}],"source":["!pip uninstall -y box2d-py\n","!pip install box2d pygame swig\n","!pip install \"gymnasium[box2d]\" --no-deps"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1600,"status":"ok","timestamp":1748525205502,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KJwVWxwiW0Q3"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCriticPPO(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim, hidden_dims=(64, 64)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1748529963425,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"GYE9dNX2-NMy"},"outputs":[],"source":["from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","import copy\n","import time\n","\n","class PPOAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, epsilon=float('inf'), gamma=0.99, lambda_GAE=0.95, lr=1e-3, num_steps=0, num_envs=8, num_epochs=4, minibatch_size=4, vectorization_mode = \"sync\", seed=123):\n","        # using vectorized environments to boost training speed\n","        self.env = gym.make_vec(env_id, num_envs=num_envs, vectorization_mode=vectorization_mode)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.num_epochs = num_epochs\n","        self.max_steps = max_steps\n","        self.minibatch_size = minibatch_size\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","        self.lambda_GAE = lambda_GAE\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCriticPPO(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.old_policy_net = copy.deepcopy(self.policy_net)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        # added scheduler after observing divergence after getting close to solving\n","        self.scheduler = StepLR(self.optimizer, step_size=100, gamma=0.9)\n","        self.loss = nn.MSELoss()\n","        self.seed = seed\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.old_policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing GAE\n","    def compute_returns(self, rewards, values, next_value):\n","        rewards = torch.stack(rewards)\n","        values = torch.cat([values, next_value.unsqueeze(0)], dim=0)\n","\n","        T, N = rewards.shape\n","        advantages = torch.zeros_like(rewards)\n","        gae = torch.zeros(N, device=rewards.device)\n","        for t in reversed(range(T)):\n","            # temporal difference error\n","            td = rewards[t] + self.gamma * values[t + 1] - values[t]\n","            # higher labmda -> more sampling, lower lambda -> more bootstrapping\n","            gae = td + self.gamma * self.lambda_GAE * gae\n","            advantages[t] = gae\n","\n","        # compute returns by adding value to advantage\n","        returns = advantages + values[:-1]\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","\n","        # normalize advantage across timesteps and environments\n","        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","        return returns, advantages\n","\n","    # computing the gamma decaying rewards\n","    # def compute_returns(self, rewards, values, next_value):\n","    #     \"\"\"\n","    #     Args:\n","    #         rewards: torch.Tensor of shape [T, N] where\n","    #                 T = rollout steps, N = num_envs\n","    #     Returns:\n","    #         returns: torch.Tensor of shape [T, N], normalized\n","    #     \"\"\"\n","    #     rewards = torch.stack(rewards)\n","\n","    #     T, N = rewards.shape\n","    #     returns = torch.zeros_like(rewards)\n","    #     R = torch.zeros(N, device=rewards.device)\n","    #     for t in reversed(range(T)):\n","    #         R = rewards[t] + self.gamma * R\n","    #         returns[t] = R\n","\n","    #     # Normalize returns across all timesteps and environments\n","    #     returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","    #     advantages = returns - values\n","\n","    #     return returns, advantages\n","\n","\n","def train(self):\n","    episode_rewards = []\n","    episode_steps = []\n","\n","    for episode in tqdm(range(self.num_episodes)):\n","        start_episode_time = time.time()  # ⏱ total episode start\n","        env_time = 0                      # ⏱ initialize environment interaction time\n","        train_time = 0                    # ⏱ initialize training time\n","\n","        state, _ = self.env.reset(seed=self.seed + episode)\n","        done = np.zeros(self.num_envs, dtype=bool)\n","        episode_reward = np.zeros(self.num_envs)\n","        old_states, old_actions, old_log_probs, old_rewards, old_values = [], [], [], [], []\n","        done_mask = np.zeros(self.num_envs, dtype=bool)\n","        done_steps = np.zeros(self.num_envs)\n","        steps = 0\n","\n","        while not np.all(done_mask) and steps < self.max_steps:\n","            steps += 1\n","            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","            old_states.append(state_tensor)\n","\n","            # ⏱ start measuring env time\n","            start_env_time = time.time()\n","            with torch.no_grad():\n","                old_action_probs, old_value = self.old_policy_net(state_tensor)\n","                old_action_dist = torch.distributions.Categorical(old_action_probs)\n","                old_action = old_action_dist.sample()\n","                old_log_prob = old_action_dist.log_prob(old_action)\n","\n","            next_state, old_reward, terminated, truncated, _ = self.env.step(old_action.cpu().numpy())\n","            env_time += time.time() - start_env_time  # ⏱ accumulate env time\n","\n","            done = np.logical_or(terminated, truncated)\n","            done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","            done_mask = np.logical_or(done_mask, done)\n","            # record when each environment is done\n","            old_reward = np.where(done_mask, 0.0, old_reward)\n","\n","            # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","            old_values.append(old_value.squeeze())\n","            old_rewards.append(torch.tensor(old_reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","            old_log_probs.append(old_log_prob)\n","            old_actions.append(old_action)\n","\n","            episode_reward += old_reward\n","            state = next_state\n","\n","            # finish full trajectory, then update\n","            if self.num_steps == 0:\n","                if np.any(done):\n","                    with torch.no_grad():\n","                        next_old_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_old_value = self.policy_net(next_old_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_old_value = next_old_value.squeeze() * (1 - done_tensor)\n","\n","                    old_states = torch.stack(old_states).to(self.device)\n","                    old_values = torch.stack(old_values).to(self.device)\n","                    old_log_probs = torch.stack(old_log_probs).to(self.device)\n","                    old_actions = torch.stack(old_actions).to(self.device)\n","                    old_returns, old_advantages = self.compute_returns(old_rewards, old_values, next_old_value)\n","\n","                    # flatten inputs\n","                    T, N = old_actions.shape[:2]\n","                    old_states = old_states.reshape(T * N, -1)\n","                    old_actions = old_actions.reshape(T * N)\n","                    old_log_probs = old_log_probs.reshape(T * N)\n","                    old_returns = old_returns.reshape(T * N)\n","                    old_advantages = old_advantages.reshape(T * N)\n","\n","                    batch_size = old_states.shape[0]\n","                    minibatch_size = self.minibatch_size\n","\n","                    # ⏱ measure train time\n","                    start_train_time = time.time()\n","\n","                    # repeat in K epochs\n","                    for _ in range(self.num_epochs):\n","                        indices = torch.randperm(batch_size)\n","\n","                        # mini batching\n","                        for i in range(0, batch_size, minibatch_size):\n","                            # selecting inputs for this mini batch\n","                            mb_idx = indices[i : i + minibatch_size]\n","                            mb_states = old_states[mb_idx]\n","                            mb_actions = old_actions[mb_idx]\n","                            mb_log_probs_old = old_log_probs[mb_idx]\n","                            mb_returns = old_returns[mb_idx]\n","                            mb_advantages = old_advantages[mb_idx]\n","\n","                            # use current policy to find log_probs of the trajectory ran by old_policy\n","                            action_probs, values = self.policy_net(mb_states)\n","                            action_dist = torch.distributions.Categorical(action_probs)\n","                            mb_log_probs = action_dist.log_prob(mb_actions)\n","\n","                            prob_ratio = torch.exp(mb_log_probs - mb_log_probs_old.detach())\n","                            values = values.squeeze(-1)\n","                            surrogate = torch.min(prob_ratio * mb_advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * mb_advantages)\n","                            actor_loss = -surrogate.mean()\n","                            critic_loss = self.loss(mb_returns, values)\n","\n","                            loss = actor_loss + 0.5 * critic_loss\n","                            self.optimizer.zero_grad()\n","                            loss.backward()\n","                            self.optimizer.step()\n","\n","                    train_time += time.time() - start_train_time  # ⏱ add train time\n","                    old_states, old_actions, old_log_probs, old_rewards, old_values = [], [], [], [], []\n","                    # self.scheduler.step()\n","\n","            # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","            else:\n","                if (steps % self.num_steps == 0) or np.any(done):\n","\n","                    with torch.no_grad():\n","                        next_old_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_old_value = self.policy_net(next_old_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_old_value = next_old_value.squeeze() * (1 - done_tensor)\n","\n","                    old_states = torch.stack(old_states).to(self.device)\n","                    old_values = torch.stack(old_values).to(self.device)\n","                    old_log_probs = torch.stack(old_log_probs).to(self.device)\n","                    old_actions = torch.stack(old_actions).to(self.device)\n","                    old_returns, old_advantages = self.compute_returns(old_rewards, old_values, next_old_value)\n","\n","                    # flatten inputs\n","                    T, N = old_actions.shape[:2]\n","                    old_states = old_states.reshape(T * N, -1)\n","                    old_actions = old_actions.reshape(T * N)\n","                    old_log_probs = old_log_probs.reshape(T * N)\n","                    old_returns = old_returns.reshape(T * N)\n","                    old_advantages = old_advantages.reshape(T * N)\n","\n","                    batch_size = old_states.shape[0]\n","                    minibatch_size = self.minibatch_size\n","\n","                    # ⏱ measure train time\n","                    start_train_time = time.time()\n","\n","                    # repeat in K epochs\n","                    for _ in range(self.num_epochs):\n","                        indices = torch.randperm(batch_size)\n","\n","                        # mini batching\n","                        for i in range(0, batch_size, minibatch_size):\n","                            # selecting inputs for this mini batch\n","                            mb_idx = indices[i : i + minibatch_size]\n","                            mb_states = old_states[mb_idx]\n","                            mb_actions = old_actions[mb_idx]\n","                            mb_log_probs_old = old_log_probs[mb_idx]\n","                            mb_returns = old_returns[mb_idx]\n","                            mb_advantages = old_advantages[mb_idx]\n","\n","                            # use current policy to find log_probs of the trajectory ran by old_policy\n","                            action_probs, values = self.policy_net(mb_states)\n","                            action_dist = torch.distributions.Categorical(action_probs)\n","                            mb_log_probs = action_dist.log_prob(mb_actions)\n","\n","                            prob_ratio = torch.exp(mb_log_probs - mb_log_probs_old.detach())\n","                            values = values.squeeze(-1)\n","                            surrogate = torch.min(prob_ratio * mb_advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * mb_advantages)\n","                            actor_loss = -surrogate.mean()\n","                            critic_loss = self.loss(mb_returns, values)\n","\n","                            loss = actor_loss + 0.5 * critic_loss\n","                            self.optimizer.zero_grad()\n","                            loss.backward()\n","                            self.optimizer.step()\n","\n","                    train_time += time.time() - start_train_time  # ⏱ add train time\n","                    old_states, old_actions, old_log_probs, old_rewards, old_values = [], [], [], [], []\n","                    # self.scheduler.step()\n","\n","        self.old_policy_net = copy.deepcopy(self.policy_net)\n","        episode_rewards.append(episode_reward)\n","        episode_steps.append(steps)\n","\n","        if episode % 10 == 0:\n","            print(f\"\\n[Episode {episode}]\")\n","            print(f\"Reward (mean): {np.mean(episode_reward):.2f}\")\n","            print(f\"Actor Loss: {actor_loss.item():.4f} | Critic Loss: {critic_loss.item():.4f}\")\n","            print(f\"Env Time: {env_time:.2f}s | Train Time: {train_time:.2f}s | Total: {time.time() - start_episode_time:.2f}s\")\n","\n","    self.env.close()\n","    return np.array(episode_rewards), np.array(episode_steps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xS4b8URHWybG","outputId":"c8c86987-bc4a-4c89-97be-f8123a948cda"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/300 [00:00<04:44,  1.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 0]\n","Reward (mean): -79.91\n","Actor Loss: -0.2480 | Critic Loss: 0.0765\n","Env Time: 0.18s | Train Time: 0.74s | Total: 0.95s\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▎         | 11/300 [00:12<05:37,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 10]\n","Reward (mean): -93.07\n","Actor Loss: -0.0391 | Critic Loss: 1.3283\n","Env Time: 0.21s | Train Time: 0.87s | Total: 1.11s\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 21/300 [00:33<09:01,  1.94s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 20]\n","Reward (mean): -65.31\n","Actor Loss: -0.0768 | Critic Loss: 0.0860\n","Env Time: 0.36s | Train Time: 1.50s | Total: 1.92s\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 31/300 [01:04<15:04,  3.36s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 30]\n","Reward (mean): 83.27\n","Actor Loss: 0.0847 | Critic Loss: 0.3503\n","Env Time: 0.74s | Train Time: 3.01s | Total: 3.84s\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▎        | 41/300 [01:45<17:24,  4.03s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 40]\n","Reward (mean): -45.42\n","Actor Loss: -0.1455 | Critic Loss: 0.4381\n","Env Time: 0.81s | Train Time: 3.12s | Total: 4.04s\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 51/300 [02:26<16:56,  4.08s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 50]\n","Reward (mean): -37.39\n","Actor Loss: 0.1478 | Critic Loss: 0.6866\n","Env Time: 0.81s | Train Time: 3.14s | Total: 4.05s\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 61/300 [03:07<16:10,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 60]\n","Reward (mean): -5.33\n","Actor Loss: 0.2784 | Critic Loss: 0.9136\n","Env Time: 0.81s | Train Time: 3.19s | Total: 4.11s\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▎       | 71/300 [03:47<15:29,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 70]\n","Reward (mean): -30.92\n","Actor Loss: -0.3160 | Critic Loss: 0.4654\n","Env Time: 0.81s | Train Time: 3.13s | Total: 4.05s\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 81/300 [04:28<14:48,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 80]\n","Reward (mean): -27.40\n","Actor Loss: 0.1365 | Critic Loss: 0.2028\n","Env Time: 0.82s | Train Time: 3.11s | Total: 4.03s\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 91/300 [05:08<13:56,  4.00s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 90]\n","Reward (mean): -30.17\n","Actor Loss: 0.2723 | Critic Loss: 0.7108\n","Env Time: 0.79s | Train Time: 3.06s | Total: 3.96s\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▎      | 101/300 [05:49<13:25,  4.05s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 100]\n","Reward (mean): -38.96\n","Actor Loss: 0.3184 | Critic Loss: 0.5561\n","Env Time: 0.81s | Train Time: 3.11s | Total: 4.03s\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 111/300 [06:29<12:39,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 110]\n","Reward (mean): -34.27\n","Actor Loss: 0.0169 | Critic Loss: 0.2468\n","Env Time: 0.80s | Train Time: 3.09s | Total: 3.99s\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 113/300 [06:37<12:37,  4.05s/it]"]}],"source":["import gymnasium as gym\n","\n","env_id = 'LunarLander-v3'\n","num_episodes = 300\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=8, num_steps=32, minibatch_size=16)\n","\n","rewards, steps = ppo_model.train()\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"ORJKdXXofjox"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28073,"status":"ok","timestamp":1748529518975,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KwcnQRLD9cTX","outputId":"61821e8c-5cfa-4a25-a847-fc517602eb7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: -209.6376773931259\n","Episode 2 Reward: -253.3317896907504\n","Episode 3 Reward: -127.91441076284075\n","Episode 4 Reward: -182.79585312779028\n","Episode 5 Reward: -119.73963978202433\n","Episode 6 Reward: -174.02446610394807\n","Episode 7 Reward: -147.8954565560111\n","Episode 8 Reward: -149.09936367264245\n","Episode 9 Reward: -141.18966495671833\n","Episode 10 Reward: -198.60302177229488\n","Average Reward over 10 episodes: -170.42313438181466\n"]}],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = ppo_model.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAXsopwasbMo","executionInfo":{"status":"aborted","timestamp":1748525061194,"user_tz":-540,"elapsed":6425,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["import gymnasium as gym\n","from gymnasium import spaces\n","import numpy as np\n","\n","class LinearValueEnv(gym.Env):\n","    def __init__(self, gamma=0.99, episode_length=100):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.episode_length = episode_length\n","        self.current_step = 0\n","\n","        # Observation: continuous scalar in [-1, 1]\n","        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n","\n","        # Action: continuous scalar (not used)\n","        self.action_space = spaces.Discrete(1)\n","\n","        self.state = None\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.state = np.random.uniform(-1.0, 1.0, size=(1,)).astype(np.float32)\n","        self.current_step = 0\n","        return self.state.copy(), {}\n","\n","    def step(self, action):\n","        # Reward is simply the state value\n","        reward = float(self.state[0])\n","        self.current_step += 1\n","\n","        terminated = self.current_step >= self.episode_length\n","        truncated = False\n","        return self.state.copy(), reward, terminated, truncated, {}\n","\n","    def render(self):\n","        print(f\"State: {self.state}\")\n","\n","    def close(self):\n","        pass\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UO-G1kxysz46","executionInfo":{"status":"aborted","timestamp":1748525061201,"user_tz":-540,"elapsed":6429,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["from gymnasium.envs.registration import register\n","\n","register(\n","    id=\"LinearValue-v0\",\n","    entry_point=\"__main__:LinearValueEnv\",  # if you're running in a script\n","    max_episode_steps=100\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQ0d1b4tKwwl","executionInfo":{"status":"aborted","timestamp":1748525061202,"user_tz":-540,"elapsed":6429,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6471,"status":"aborted","timestamp":1748525061246,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"z-BU9qP2s0si"},"outputs":[],"source":["env_id = \"LinearValue-v0\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model_value =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=2, num_steps=0)\n","\n","rewards, steps = ppo_model_value.train()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"pV9BZMLONrhD","executionInfo":{"status":"aborted","timestamp":1748525061251,"user_tz":-540,"elapsed":6476,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMRxmfABJ+khDfWRP6+RXn8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}