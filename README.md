# RL-practice

This repository documents my journey of studying reinforcement learning (RL) while serving in the military. Due to many limitations including available software and security issues, I use solely Google Collaboratory as the platform and Python as the language. PyTorch and Gymnasium are the libraries for the model and game environment, respectively. I welcome all forms of collaboration and contributions‚Äîfeel free to open a pull request, commit improvements, or share your insights!

---

## Goals

The primary goal of this project is to gain in-depth knowledge of RL by implementing various algorithms from scratch, debugging through trial and error, and adding skills to enhance the performances.  
The secondary goal is to provide a simple, yet intuitive path to study RL from scratch for any beginners like me, and to share & maintain the insights I have gained during the project.  
The last goal is to motivate myself by recording the progress of my study. I am mainly using the late-night hours from 22:00‚Äì24:00, and it is often tough to devote myself to studying after a fatiguing day of work. By recording my progress here and looking back at the progress I made, it will help me take steady steps toward mastering RL.

---

## Algorithms Covered

- Q-Learning (Tabular)
- SARSA
- DQN
- Double DQN
- Dueling DQN
- A2C (Monte Carlo and n-step variants)
- Generalized Advantage Estimation (GAE)
- Vectorized Environments
- Proximal Policy Optimization (PPO)
  
---

## References

For the fundamental theory, I mainly referenced Berkeley's CS 285 lectures by Professor Sergey Levine. For implementing the code, I gained help from multiple sources including GitHub repos, generative AI, Stack Overflow, etc.

---

## Timeline & Progress

| Date             | Project / Notebook                  | Description & Key Learnings |
|--------------|----------------------------------------|-----------------------------|
| **Apr 14, 2025** | `gymnasium_tutorial/`  ‚úÖ              | An introduction to the Gymnasium environment following the tutorial from https://gymnasium.farama.org/ |
| **Apr 14, 2025** | `sarsa_qlearn_comparison.ipynb`  ‚úÖ    | Implemented SARSA and Q-learning, the most fundamental on-policy and off-policy algorithms. |
| **Apr 15, 2025** | `DQN_cartpole.ipynb`  ‚úÖ               | Implemented DQN for CartPole, first time using MLPs for training. Had to refresh basic MLP knowledge/syntax. Learned how to use MLPs to predict certain functions and how to train them. In the DQN, it predicts the Q value from the current state and action and compares it with reward + gamma * max(Q(next_state)). |
| **Apr 15, 2025** | `DDQN_cartpole.ipynb`  ‚úÖ              | Added an MLP to the DQN to implement DDQN. DQN overestimates the general values of the Q function since it takes the max value of the Q vector. Therefore, DDQN adds another MLP Q' that estimates Q. The predicted Q value is estimated using Q'. The new target value is calculated as reward + gamma * Q(next_state, argmax(Q'(next_state))), which doesn't take the max value directly. This prevents overestimation of the Q value. Empirically found better results compared to DQN. |
| **Apr 15, 2025** | `Dueling_DQN_cartpole.ipynb`   ‚úÖ      | Implemented Dueling DQN which inherits DDQN's structure but adds an advantage stream. The Q vector is calculated as value + advantage - advantage.mean(), while the value and advantage are estimated through distinct streams. This separates the benefit of being in a good state from the benefit of taking a good action. In many real-life situations, it's the state that matters, not the action. In these cases, Dueling DQN's network can just flatten the advantage and learn the value, which is a function of the state only. |
| **Apr 16, 2025** | `A2C_bipedal.ipynb`  üõ†Ô∏è                | First try implementing Monte Carlo A2C. Tried it on BipedalWalker to experiment with a continuous environment. A2C is an algorithm that has two separate players, the Actor and the Critic. The Critic's goal is to estimate the value function, similar to Deep Q Networks, but estimating value instead. The Actor's goal is to optimize the policy function (a function that returns a probability distribution over actions in a given state). The basic A2C computes advantage by subtracting the estimated value from gamma-discounted reward. Then, the policy updates by increasing the probability of the selected actions if the advantage is positive, and vice versa otherwise. The Critic updates to minimize MSE(advantage, 0). Couldn't finish debugging the code. |
| **Apr 18, 2025** | `A2C_bipedal.ipynb` üõ†Ô∏è                 | Switched to n-step A2C (aka Vanilla A2C). n-step A2C is developed to update more frequently than Monte Carlo A2C, which has to wait for the whole episode to end. n-step A2C computes and stores the advantages, log probs, and values for n steps (or until the trajectory ends), and refreshes them after updating the Actor and Critic. The model wasn't training at all. Printed out multiple statistics such as values, returns, entropies for debugging. Found out the gradient was too big and applied gradient clipping. Also added normalization layers. Still couldn‚Äôt get it working. |
| **Apr 19, 2025** | `A2C_bipedal.ipynb` üõ†Ô∏è                 | Tried using RL Baselines3 Zoo's hyperparameters and implemented Generalized Advantage Estimation (GAE). Still wouldn‚Äôt work. |
| **Apr 20, 2025** | `A2C_cartpole.ipynb`, `RL_A2C.ipynb` üõ†Ô∏è | Came back to CartPole to try implementing n-step A2C in a simpler environment. Downloaded the following Jupyter Notebook and succeeded in training ([source](https://github.com/ayeenp/deep-rl-a2c-cartpole/blob/main/RL_A2C.ipynb)). Made sure that the structure of the MLP and hyperparameters were not the problem. |
| **Apr 24‚ÄìMay 1, 2025** | Vacation!! | Visited Tokyo to meet up with T@B friends, came back to Seoul and celebrated my brother's birthday. |
| **May 3, 2025** | `A2C_cartpole.ipynb`, `RL_A2C.ipynb`  ‚úÖ | Thought n-step was complicating the problem and started from the beginning by implementing Monte Carlo A2C. Finally got it to work. Then, tried n-step A2C again. Using a bigger n than max_step makes n-step A2C behave equally to Monte Carlo A2C. However, my model did not train correctly when using the n-step methods instead. Though, this helped my pinpoint that the issue is in the method compute_n_step_returns and fount the critical bug: forgetting to mask the bootstrapped value with (1 - done) caused the agent to learn from terminal states incorrectly. After fixing this, got both n-step and Monte Carlo A2C working. |
| **May 5, 2025** | `A2C_bipedal.ipynb` üõ†Ô∏è | Re-implemented n-step A2C in a continuous space using 'A2C_cartpole's code. Though it would work since the bug was fixed, but did not train well. Switched environment in to LunarLanderContinuous for easier train. Then, bounded action by using tanh function, and initialized std with a smaller value. Still didn't train well enough. |
| **May 6, 2025** | `A2C_lunar_lander.ipynb` üõ†Ô∏è | Made a new file to treat Lunar Lander in both discrete and continuous. For discrete Lunar Lander, I expected it to train well when using the same code for cartpole. However, it didn't, and printed out multiple parameters to find the problem. First, the gradients were too big since the calculated returns were all very large due to the crash (-100 reward). Therefore, implemented advantage normalization. However, this caused a problem of vanishing actor_loss since log_prob is pretty much uniform when initialized, and when taken dot product with the normlized advantage vector, the result should mathematically converge to 0. Lastly, tried normalizing n-step return instead of advantage. Showed it is training, but not efficiently. |
| **May 12, 2025** | `A2C_cartpole_vec_env.ipynb` üõ†Ô∏è | Practicing/analyzing usage of vectorized environments for faster training. Used A2C_cartpole file to execute. Found some interesting results. First, using multiple environments (8) took much more time than a single environment even when training same episodes on each environment. 00:30 to train 1000 episodes in single env, 12:31 to train 1000 episodes in 8 envs. Second, when same number of total episodes (num_envs * episodes) were explored, multiple environments showed much better reward. When 1000 episodes trained in single env, average reward was 119.2 and when 125 episodes were trained in 8 environments, average reward was 500. When only 10 episodes were trained each in 8 environments, average reward was 119.3.| 
| **May 14, 2025** | `A2C_cartpole_vec_env.ipynb` üõ†Ô∏è | A MONTH DONE! Found bug that every single episode (even the first training episodes) explored max_steps for vectorized environments which accounted for longer training time, better result. It was because np.all(done) always returned False since done was not permanently True after state being terminated. Resolved using done_mask vector. | 
| **May 15, 2025** | `A2C_cartpole_vec_env.ipynb` ‚úÖ | Successfully trained using multiple environments. Multiple environments had both significant time/reward advantage over single environment. Compared async/sync vectorization methods and async trained better, sync trained faster. This was unexpected, and more analyzed in the file's comments. | 
| **May 16, 2025** | `A2C_lunar_lander.ipynb` üõ†Ô∏è | Found this repo -- https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/blob/master/train.py. The algorithm was mostly alike except some futile differences, except the way loss was calculated. The refernce model took a stepwise sum of logprob * advantage, and my model took the mean of the whole batch. Researched difference between sum vs. mean reduction. Besides magnitude of gradient, not much varies. | 
| **May 17, 2025** | `A2C_lunar_lander.ipynb` üõ†Ô∏è | Found critical bug in my calculate_rewards method. The dimensions of the tensors logprobs, state_values, and rewards were all different --0 [n,1], [n, 1, 1], [n]. This led to a very weird way of broadcasting and messed up the loss completely. Learned two lessons - be careful with dimensions of tensors, don't ignore the warnings the cell outputs are telling you. | 
| **May 18, 2025** | `A2C_lunar_lander.ipynb` 'A2C_bipedal.ipynb' üõ†Ô∏è | Ran the lunar_lander code after fixing the critical bugs. Took hour and half training sync & async, then found out I was updating every n steps but calculating returns in monte carlo. Will try a gain tomorrow with MC first. Also, polished code for continuous environments in 'A2C_bipedal.ipynb. | 
| **May 21, 2025** | `A2C_lunar_lander.ipynb` 'A2C_bipedal.ipynb' üõ†Ô∏è | Succeeded in training discrete lunar lander. Was not converging in 500+ episodes, learning rate was too high (0.02) since the reference model was training in a single environment while I was using 8. Also, added decaying learning rate. For continuous lunar lander, same lr used for discrete was too large and studied the theoretical background for why. Tried smaller learning rates and gradient norm clipping but wasn't working perfectly. Will try different methods next time, and will implement n-step return for both. | 
| **May 22, 2025** | `A2C_lunar_lander.ipynb`  ‚úÖ | Implemented n-step learning for A2C lunar lander. Could see that it is training, but wasn't perfect. Compared to Monte-Carlo, it took 1.25x longer (assuming due to more frequent updates), but returned a better result. Will need some hyperparamter tuning, but A2C implementation is complete for now. Once done with other algorithms, will come back to tune hyperparameters then will compare each algorithms' performances in lunar lander(or other games). | 
| **May 24, 2025** | `PPO_lunar_lander.ipynb`  üõ†Ô∏è | The PPO algorithm shares a similar structure to the A2C algorithm but differs in how it calculates policy loss. When I trained A2C in lunar lander, it often showed dramatic updates and didn't converge well even in taking large steps. PPO prevents this by clipping the value of P_{old theta}(state)/P_{new theta}(state). Therefore, it is more stable than A2C and will lead to better convergence. Will implement tomorrow and compare performance. | 
| **May 25, 2025** | `PPO_lunar_lander.ipynb`  üõ†Ô∏è | Implemented PPO for continuous observation space and discrete action space. Calculated GAEs for advantage/returns. Had many issues while debugging. First, critic loss was much bigger than actor loss, causing policy to not update at all. Solved by clipping value estimate(technical reference: https://openreview.net/attachment?id=r1etN1rtPB&name=original_pdf). Still, wouldn't train well. | 
| **May 26, 2025** | `PPO_lunar_lander.ipynb`  üõ†Ô∏è | Debugged PPO. Referrenced the PPO paper by OpenAI for a more accurate implementation. There were a few components I was missing. First, I choose actions based on the current policy, but the trajectory was supposed to be generated on the old policy. Second, I wasn't repeating it in K epochs as it was suggested in the paper. Lastly, I implemented minibatching which will stabilize the learning process. However, these fixes led to much more frequent updates and therefore longer training time. Will do some hyperparamter tuning tomorrow so the training could be done within 2 hrs. | 
| **May 28-29, 2025** | `PPO_lunar_lander.ipynb`  üõ†Ô∏è | Stuck in PPO implementation. Seems to train somewhat for cartpole, but not training as expected for Lunar Lander. Maybe I should work on making the code more readable and concise during the weekend for easy debugging. Also, found out bottleneck is training, not running the environment, so maybe should decrease number of minibatch updates. Lastly, checked GAE was working slightly better than decaying rewards in cartpole. | 
| **June 1-3, 2025** | `PPO_lunar_lander.ipynb`  üõ†Ô∏è | Downloaded https://github.com/nikhilbarhate99/PPO-PyTorch model as a reference. Switched from updating every n steps (32) or whenever episode is done to updating every n steps (4000) across episodes. Then, increased number of epochs for each update (8 -> 40). Then, changed to single environment for simplicity. While transitioning into single environment, found another dimension bug. Also, had to edit masking terminal rewards when switching from episode based to time step based updates. Got the model functioning in early timesteps, but doesn't continue learning. | 
| **June 5-20, 2025** | On leave due to dispatch |
| **June 25-27, 2025** | `PPO_discrete.ipynb`, `PPO_continuous.ipynb`  ‚úÖ | Finished debugging PPO! Worked on simplifying the code architecture by dividing them into pieces. Splitted update() and choose_action() methods from the train method and simplified the buffers by storing it in the PPOAgent class. Also, made fixes in the asynchronous state updates. Then, moved onto PPO implementation in continuous environments. Needed to watch out for double backpropagating and the log probabilities as tanh() is applied to the raw actions sampled from the Gaussian Distribution (for lunar lander). Compared using advantages vs. Generalized Advantage Estimation(GAE) in both environments.
