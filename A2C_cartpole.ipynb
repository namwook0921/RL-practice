{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyOSwLcVjJ6zO71+w51S316j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"nXMnywNrJT21","executionInfo":{"status":"ok","timestamp":1746409984642,"user_tz":-540,"elapsed":18215,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"424f36f8-c925-4ed8-984a-1caec6e66929","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TE5jqXjRGj5","executionInfo":{"status":"ok","timestamp":1746409152147,"user_tz":-540,"elapsed":12675,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"95791406-fe82-4691-d7e5-0082fe6d8d29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n","Collecting pygame\n","  Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.6.1\n","Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for box2d-py \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for box2d-py (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n","\u001b[0mFailed to build box2d-py\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCritic(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dims=(32, 32)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n","\n","\n","# def compute_gae(rewards, values, next_values, dones, gamma, gae_lambda):\n","#     advantages = []\n","#     advantage = 0\n","#     for i in reversed(range(len(rewards))):\n","#         delta = rewards[i] + gamma * next_values[i] * dones[i] - values[i]\n","#         advantage = delta + gamma * gae_lambda * dones[i] * advantage\n","#         advantages.insert(0, advantage)\n","#     return advantages\n","\n"],"metadata":{"id":"JifS_sPARTGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # Choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # Computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # Computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # Bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","    def train(self):\n","        episode_rewards = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            values = []\n","            rewards = []\n","            log_probs = []\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n","                done = terminated or truncated\n","\n","                # Saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(reward)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # Every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or done:\n","                    _, next_value = self.policy_net(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n","                    next_value = next_value.squeeze()\n","                    # BUG ALERT\n","                    # MUST MULTIPLY (1 - done) to next_value to mask the bootstrapped next_value when the game is over. CRITICAL BUG THAT TOOK HOURS TO FIND\n","                    returns = self.compute_n_step_returns(rewards, next_value * (1 - done))\n","                    values = torch.stack(values)\n","                    log_probs = torch.stack(log_probs)\n","                    advantages = returns - values\n","                    # Calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # Updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","                    loss = actor_loss + 0.4 * critic_loss\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","\n","        self.env.close()\n","        return np.array(episode_rewards)\n"],"metadata":{"id":"iLbCiMcsOt5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env = gym.make('CartPole-v1')\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","#num_steps = max_steps make it Monte-Carlo\n","a2c_model =  A2CAgent(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szBR38HBF7_Q","executionInfo":{"status":"ok","timestamp":1746409183363,"user_tz":-540,"elapsed":25449,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"6582c009-574e-4ade-a51b-167df81bd062"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:21<00:00, 46.10it/s]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPjtvt_7F7mW","executionInfo":{"status":"ok","timestamp":1746409183877,"user_tz":-540,"elapsed":512,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"5dbed553-cf1a-4f93-ff21-8cececc33ca2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 80.0\n","Episode 2 Reward: 72.0\n","Episode 3 Reward: 114.0\n","Episode 4 Reward: 127.0\n","Episode 5 Reward: 84.0\n","Episode 6 Reward: 101.0\n","Episode 7 Reward: 77.0\n","Episode 8 Reward: 86.0\n","Episode 9 Reward: 71.0\n","Episode 10 Reward: 89.0\n","Average Reward over 10 episodes: 90.1\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env = gym.make('CartPole-v1')\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","#num_steps = 5. Updates more frequently than Monte Carlo which takes more training time\n","a2c_model =  A2CAgent(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 5)\n","\n","rewards = a2c_model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3LYEs_yRwDf","executionInfo":{"status":"ok","timestamp":1746409399976,"user_tz":-540,"elapsed":216095,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"5826dc42-e060-4f4e-c155-5ec0e7ec8048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [03:36<00:00,  4.63it/s]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rkMAPuZTq8B","executionInfo":{"status":"ok","timestamp":1746409401966,"user_tz":-540,"elapsed":1978,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"f57ebc8b-a154-482d-8487-68cd8fa5db70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 500.0\n","Episode 2 Reward: 500.0\n","Episode 3 Reward: 500.0\n","Episode 4 Reward: 500.0\n","Episode 5 Reward: 500.0\n","Episode 6 Reward: 500.0\n","Episode 7 Reward: 500.0\n","Episode 8 Reward: 500.0\n","Episode 9 Reward: 500.0\n","Episode 10 Reward: 500.0\n","Average Reward over 10 episodes: 500.0\n"]}]},{"cell_type":"code","source":["### Comparing Monte Carlo A2C vs. n-step A2C\n","# Monte Carlo takes significantly smaller time to train, given the same amount of episodes. This results due to the less frequent update compared to n-step A2C.\n","# While Monte Carlo updates once, 5-step A2C updates up to 100(max_step / n) times, causing this difference.\n","# While the Monte Carlo A2C showed an average return of 90 per episode, the n step A2C reached the max return 500 in only 1000 epsiodes trained.\n","# When each epsiode takes significantly long time to simulate, n-step A2C will have notable advantage over Monte Carlo A2C."],"metadata":{"id":"BoXsZSP1F20e"},"execution_count":null,"outputs":[]}]}