{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMIxU9oJA5tmpm4ENfAUOfl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYXvVHQ_JOHJ","executionInfo":{"status":"ok","timestamp":1746410004874,"user_tz":-540,"elapsed":27069,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"960a35ce-77c8-4cb5-f1bc-5cb015af25b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kV6-Zech0MzG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746504576072,"user_tz":-540,"elapsed":4596,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"6be75bf8-7bfe-4a45-ae4e-ee2263502b36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 4, in <module>\n","    from pip._internal.cli.main import main\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n","    from pip._internal.cli.autocompletion import autocomplete\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n","    from pip._internal.cli.main_parser import create_main_parser\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n","    from pip._internal.build_env import get_runnable_pip\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n","    from pip._internal.cli.spinners import open_spinner\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n","    from pip._internal.utils.logging import get_indentation\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n","    from pip._internal.utils.misc import ensure_dir\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/misc.py\", line 43, in <module>\n","    from pip._internal.locations import get_major_minor_version\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/locations/__init__.py\", line 9, in <module>\n","    from pip._internal.models.scheme import SCHEME_KEYS, Scheme\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/models/scheme.py\", line 13, in <module>\n","    @dataclass(frozen=True)\n","     ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/dataclasses.py\", line 1222, in wrap\n","    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/dataclasses.py\", line 1027, in _process_class\n","    _init_fn(all_init_fields,\n","  File \"/usr/lib/python3.11/dataclasses.py\", line 580, in _init_fn\n","    return _create_fn('__init__',\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/dataclasses.py\", line 433, in _create_fn\n","    exec(txt, globals, ns)\n","  File \"<string>\", line 0, in <module>\n","KeyboardInterrupt\n","^C\n"]}],"source":["!pip install gymnasium\n","!pip install pygame"]},{"cell_type":"code","source":["!pip install swig\n","!pip install gymnasium[box2d]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wo85t2-PCybp","outputId":"6247cc1b-1601-419e-ab57-679ec3e2e63b","executionInfo":{"status":"ok","timestamp":1746504636520,"user_tz":-540,"elapsed":60436,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379416 sha256=b1cecfd888fdae914043ba96d94e8cd6f5035cfcf6051cfdeda37440d2a909f8\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","gym.pprint_registry()"],"metadata":{"id":"zNFrxef6CVeE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746504636595,"user_tz":-540,"elapsed":73,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"8730348f-4290-4207-a039-8e6b86eceb33"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["===== classic_control =====\n","Acrobot-v1             CartPole-v0            CartPole-v1\n","MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n","===== phys2d =====\n","phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n","===== box2d =====\n","BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v3\n","LunarLander-v3         LunarLanderContinuous-v3\n","===== toy_text =====\n","Blackjack-v1           CliffWalking-v0        FrozenLake-v1\n","FrozenLake8x8-v1       Taxi-v3\n","===== tabular =====\n","tabular/Blackjack-v0   tabular/CliffWalking-v0\n","===== mujoco =====\n","Ant-v2                 Ant-v3                 Ant-v4\n","Ant-v5                 HalfCheetah-v2         HalfCheetah-v3\n","HalfCheetah-v4         HalfCheetah-v5         Hopper-v2\n","Hopper-v3              Hopper-v4              Hopper-v5\n","Humanoid-v2            Humanoid-v3            Humanoid-v4\n","Humanoid-v5            HumanoidStandup-v2     HumanoidStandup-v4\n","HumanoidStandup-v5     InvertedDoublePendulum-v2 InvertedDoublePendulum-v4\n","InvertedDoublePendulum-v5 InvertedPendulum-v2    InvertedPendulum-v4\n","InvertedPendulum-v5    Pusher-v2              Pusher-v4\n","Pusher-v5              Reacher-v2             Reacher-v4\n","Reacher-v5             Swimmer-v2             Swimmer-v3\n","Swimmer-v4             Swimmer-v5             Walker2d-v2\n","Walker2d-v3            Walker2d-v4            Walker2d-v5\n","===== None =====\n","GymV21Environment-v0   GymV26Environment-v0\n"]}]},{"cell_type":"code","source":["class GaussianActorCritic(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dims=(64, 64)):\n","        super().__init__()\n","        # shared layers\n","        self.share = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","\n","        # actor layers. actor_mean estimates the mean of the Gaussian distribution, and actor_log_std estimates the log_std\n","        self.actor_mean = nn.Linear(hidden_dims[1], output_dim)\n","\n","        # log_std is safe from having negative values, therefore more stable than estimating std. Since action is bounded from -1 to 1, initialize it as e^-2\n","        self.actor_log_std = nn.Parameter(torch.ones(output_dim) * -2)\n","\n","        # critic layer estimates value function\n","        self.critic = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.share(x)\n","\n","        # actor's mean layer estimates mean.\n","        mean = self.actor_mean(x)\n","\n","        # actor's log_std layer estimates log_std. Then, convert it to std\n","        log_std = self.actor_log_std\n","        std = torch.exp(log_std)\n","\n","        # critic layer estimates value\n","        value = self.critic(x)\n","\n","        return mean, std, value\n"],"metadata":{"id":"If6ymze-CrUd","executionInfo":{"status":"error","timestamp":1746504636654,"user_tz":-540,"elapsed":57,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"ae0ce0ae-e00d-4d06-a557-78d423ffbc99"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a984240d370f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGaussianActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# shared layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.share = nn.Sequential(\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = GaussianActorCritic(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        mean, std, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Normal(mean, std)\n","        action = action_dist.sample()\n","\n","        # bipedal walker's action space is bounded to [-1, 1]. Apply tanh function to keep the action in range\n","        action = torch.tanh(action)\n","        return action\n","\n","    # computing the gamma decaying rewards in Monte carlo\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","\n","        # Bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","\n","    # mostly equal to A2C_cartpole\n","    def train(self):\n","        episode_rewards = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            values = []\n","            rewards = []\n","            log_probs = []\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","\n","                # differs from A2C_cartpole when sampling action\n","                mean, std, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Normal(mean, std)\n","                action = action_dist.sample()\n","\n","                # not exactly the log probability, but log(probability density) since it is a continuous space\n","                log_prob = action_dist.log_prob(action)\n","\n","                # need to move the tensor to the cpu to convert it to numpy\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.detach().cpu().numpy().squeeze(0))\n","                done = terminated or truncated\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(reward)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or done:\n","                    _, _, next_value = self.policy_net(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n","                    next_value = next_value.squeeze()\n","\n","                    # BUG ALERT\n","                    # MUST MULTIPLY (1 - done) to next_value to mask the bootstrapped next_value when the game is over. CRITICAL BUG THAT TOOK HOURS TO FIND\n","                    returns = self.compute_n_step_returns(rewards, next_value * (1 - done))\n","                    values = torch.stack(values)\n","                    log_probs = torch.stack(log_probs)\n","                    advantages = returns - values\n","                    # matching the dimensions fo log_probs\n","                    advantages = advantages.unsqueeze(1)\n","\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","                    loss = actor_loss + 0.4 * critic_loss\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","\n","        self.env.close()\n","        return np.array(episode_rewards)\n"],"metadata":{"id":"8RhcUczlmRO_","executionInfo":{"status":"aborted","timestamp":1746504636789,"user_tz":-540,"elapsed":19,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env = gym.make('LunarLanderContinuous-v3')\n","gamma = 0.99\n","num_episodes = 2000\n","max_steps = 1000\n","num_steps = 8\n","lr = 7e-3\n","\n","a2c_model =  A2CAgent(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = num_steps)\n","\n","rewards = a2c_model.train()"],"metadata":{"id":"R1-YSXWCrFe8","executionInfo":{"status":"aborted","timestamp":1746504636791,"user_tz":-540,"elapsed":6,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"id":"TCfaIz4P9LuC","executionInfo":{"status":"aborted","timestamp":1746504636793,"user_tz":-540,"elapsed":7,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","env = gym.make('LunarLanderContinuous-v3')\n","test_episodes = 10\n","test_rewards = []\n","\n","for _ in range(test_episodes):\n","    state, _ = env.reset()\n","    done = False\n","    ep_reward = 0\n","\n","    while not done:\n","        with torch.no_grad():\n","            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(a2c_model.device)\n","            mean, std, _ = a2c_model.policy_net(state_tensor)\n","            action = mean\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy().squeeze(0))\n","        done = terminated or truncated\n","        ep_reward += reward\n","        state = next_state\n","\n","    test_rewards.append(ep_reward)\n","\n","\n","print(f\"Average reward over {test_episodes} test episodes: {np.mean(test_rewards):.2f}\")\n","\n","plt.plot(test_rewards)\n","plt.title(\"A2C Test Episode Rewards\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Reward\")\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"OEaRNu7_IzPf","executionInfo":{"status":"aborted","timestamp":1746504636794,"user_tz":-540,"elapsed":7,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","\n","\n","env = gym.make('LunarLanderContinuous-v3')\n","test_episodes = 10\n","random_rewards = []\n","\n","for ep in range(test_episodes):\n","    state, _ = env.reset()\n","    done = False\n","    ep_reward = 0\n","\n","    while not done:\n","        action = env.action_space.sample()  # random continuous action\n","        state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        ep_reward += reward\n","\n","    random_rewards.append(ep_reward)\n","    print(f\"Episode {ep+1} reward: {ep_reward:.2f}\")\n","\n","env.close()\n","\n","avg_reward = np.mean(random_rewards)\n","print(f\"\\n✅ Average reward over {test_episodes} random episodes: {avg_reward:.2f}\")\n"],"metadata":{"id":"9OgOgj5DKVzY","executionInfo":{"status":"aborted","timestamp":1746504636796,"user_tz":-540,"elapsed":9,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6QaeQNNRKtfF","executionInfo":{"status":"aborted","timestamp":1746504636797,"user_tz":-540,"elapsed":9,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from tqdm import tqdm\n","# import torch.profiler\n","\n","# n_envs = 1\n","# envs = [gym.make('LunarLanderContinuous-v3') for _ in range(n_envs)]\n","\n","# n_states = envs[0].observation_space.shape[0]\n","# n_actions = envs[0].action_space.shape[0]\n","\n","# actor_network = GaussianActor(n_states, n_actions)\n","# critic_network = Critic(n_states)\n","\n","# state_normalizer = Normalizer(shape=(envs[0].observation_space.shape[0],))\n","# optimizer = optim.Adam(list(actor_network.parameters()) + list(critic_network.parameters()), lr=initial_lr)\n","# mse_loss = nn.MSELoss()\n","\n","# max_step = 1600\n","# states, actions, log_probs, rewards, values, dones, entropies = [], [], [], [], [], [], []\n","\n","\n","# for ep in tqdm(range(episodes)):\n","#     states, rewards, dones, values, actions = [], [], [], [], []\n","#     ep_reward = 0\n","#     latest_debug_info = {}\n","\n","#     for env in envs:\n","#         state, _ = env.reset()\n","#         done = False\n","#         steps = 0\n","\n","#         while not done:\n","#             state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","#             mean, std = actor_network(state_tensor)\n","#             dist = torch.distributions.Normal(mean, std)\n","#             action = dist.sample()\n","#             log_prob = dist.log_prob(action).sum(dim=-1)\n","#             entropy = dist.entropy().sum(dim=-1)\n","#             value = critic_network(state_tensor)\n","#             steps += 1\n","\n","#             next_state, reward, terminated, truncated, _ = env.step(action.detach().numpy().squeeze(0))\n","#             done = terminated or truncated or steps > max_step\n","\n","#             states.append(state_tensor)\n","#             actions.append(action)\n","#             log_probs.append(log_prob)\n","#             values.append(value)\n","#             rewards.append(reward)\n","#             dones.append(done)\n","#             entropies.append(entropy)\n","#             ep_reward += reward\n","\n","#             if len(rewards) >= num_steps or done:\n","\n","#                 next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n","#                 if dones[-1]:\n","#                     bootstrap_value = 0\n","#                 else:\n","#                     bootstrap_value = critic_network(next_state_tensor).item()\n","\n","#                 returns = []\n","#                 R = bootstrap_value\n","#                 for i in reversed(range(len(rewards))):\n","#                     R = rewards[-1 - i] + gamma * R\n","#                     returns.insert(0, R)\n","#                 returns = torch.tensor(returns, dtype=torch.float32)\n","\n","#                 values_tensor = torch.stack(values[-num_steps:]).squeeze(-1)\n","#                 log_probs_tensor = torch.stack(log_probs[-num_steps:]).squeeze(-1)\n","#                 entropies_tensor = torch.stack(entropies[-num_steps:]).squeeze(-1)\n","#                 returns_tensor = returns.clone().detach()\n","#                 advantages = returns - values_tensor\n","#                 clipped_advantages = torch.clamp(advantages, min=-10.0, max=10.0)\n","\n","#                 policy_loss = -(log_probs_tensor * clipped_advantages.detach()).mean()\n","#                 value_loss = mse_loss(values_tensor, returns_tensor)\n","#                 entropy_loss = -entropies_tensor.mean()\n","\n","#                 # total_loss = policy_loss + 0.5 * value_loss + 0.001 * entropy_loss\n","#                 total_loss = policy_loss + 0.4 * value_loss    #following rl-baselines3-zoo\n","\n","#                 optimizer.zero_grad()\n","#                 total_loss.backward()\n","#                 grad_norm = torch.nn.utils.clip_grad_norm_(\n","#                     list(actor_network.parameters()) + list(critic_network.parameters()),\n","#                     max_norm=0.5\n","#                 )\n","#                 optimizer.step()\n","\n","\n","#                 latest_debug_info = {\n","#                     \"values\": values_tensor,\n","#                     \"returns\": returns,\n","#                     \"log_probs\": log_probs_tensor,\n","#                     \"entropies\": entropies_tensor,\n","#                     \"policy_loss\": policy_loss,\n","#                     \"value_loss\": value_loss,\n","#                     \"grad_norm\": grad_norm\n","#                 }\n","\n","\n","#                 states, actions, log_probs, rewards, values, dones, entropies = [], [], [], [], [], [], []\n","\n","#             state = next_state\n","\n","\n","#     if ep % 10 == 0 and latest_debug_info:\n","#         a2c_debug_log(ep, steps, ep_reward, **latest_debug_info)"],"metadata":{"id":"deb3jHJxP-ta","executionInfo":{"status":"aborted","timestamp":1746504636798,"user_tz":-540,"elapsed":10,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def a2c_debug_log(ep, steps, ep_reward, values, returns, log_probs, entropies, policy_loss, value_loss, grad_norm):\n","    advantages = returns - values\n","\n","    print(f\"\\n[Episode {ep}] Debug Summary\")\n","    print(f\"# of Steps:         {steps:.2f}\")\n","    print(f\"Total Reward:       {ep_reward:.2f}\")\n","    print(f\"Mean V(s):          {values.mean().item():.4f}\")\n","    print(f\"Advantage Mean:     {advantages.mean().item():.4f}\")\n","    print(f\"Advantage Std:      {advantages.std().item():.4f}\")\n","    print(f\"Entropy (avg):      {entropies.mean().item():.4f}\")\n","    print(f\"Policy Loss:        {policy_loss.item():.4f}\")\n","    print(f\"Value Loss:         {value_loss.item():.4f}\")\n","    print(f\"Gradient Norm:      {grad_norm:.4f}\")\n","    print(f\"Log Prob Mean:      {log_probs.mean().item():.4f}\")\n","    print(\"-\" * 50)\n"],"metadata":{"id":"meg69gHmODz_","executionInfo":{"status":"aborted","timestamp":1746504636800,"user_tz":-540,"elapsed":11,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","# class Normalizer:\n","#     def __init__(self, shape, epsilon=1e-8):\n","#         self.shape = shape\n","#         self.mean = np.zeros(shape)\n","#         self.var = np.ones(shape)\n","#         self.count = epsilon\n","\n","#     def update(self, x):\n","#         batch_mean = np.mean(x, axis=0)\n","#         batch_var = np.var(x, axis=0)\n","\n","#         batch_size = x.shape[0]\n","#         self.count += batch_size\n","#         self.mean += (batch_mean - self.mean) * batch_size / self.count\n","#         self.var += (batch_var - self.var) * batch_size / self.count\n","\n","#     def normalize(self, x):\n","#         return (x - self.mean) / np.sqrt(self.var + 1e-8)\n","\n","\n","def compute_gae(rewards, values, next_values, dones, gamma, gae_lambda):\n","    advantages = []\n","    advantage = 0\n","    for i in reversed(range(len(rewards))):\n","        delta = rewards[i] + gamma * next_values[i] * dones[i] - values[i]\n","        advantage = delta + gamma * gae_lambda * dones[i] * advantage\n","        advantages.insert(0, advantage)\n","    return advantages\n","\n"],"metadata":{"id":"2YnfV8N5RsA3","executionInfo":{"status":"aborted","timestamp":1746504636801,"user_tz":-540,"elapsed":7,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":null,"outputs":[]}]}