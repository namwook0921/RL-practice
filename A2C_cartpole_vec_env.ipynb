{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18011,"status":"ok","timestamp":1747056737127,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"nXMnywNrJT21","outputId":"20421eee-eeac-471f-bca9-664a3fa1c731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46895,"status":"ok","timestamp":1747232723242,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"9TE5jqXjRGj5","outputId":"937e6a97-c0c6-4b40-8736-b9a0c683d7fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.4.0)\n","Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","\u001b[33m  DEPRECATION: Building 'box2d-py' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'box2d-py'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n","\u001b[0m  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379440 sha256=34d5bca59e813aeeb70b3253a9428a2744005fcde5d041c44be120832ff5073d\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools pip --upgrade\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5629,"status":"ok","timestamp":1747232728897,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"JifS_sPARTGj"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCritic(nn.Module):\n","\n","    # initially started with (64, 64) hidden dimension, but emprically found out (32, 32) works better. (64, 64) might be too much power for simple game like cartpole\n","    def __init__(self, input_dim, output_dim, hidden_dims=(32, 32)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1747234514021,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"iLbCiMcsOt5a"},"outputs":[],"source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5, num_envs = 8):\n","        # using vectorized environments to boost training speed\n","        self.env = gym.make_vec(env_id, num_envs = num_envs, vectorization_mode=\"sync\")\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value  # shape: (num_envs,)\n","        returns = []\n","        for r in reversed(rewards):  # each r: (num_envs,)\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)  # shape: (n_steps, num_envs)\n","\n","    def train(self):\n","        episode_rewards = []\n","        step_sum = 0\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs = [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                # print(done)\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_mask = np.logical_or(done_mask, done)\n","                reward = np.where(done_mask, 0.0, reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or np.any(done):\n","                    with torch.no_grad():\n","                        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_value = self.policy_net(next_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                    returns = self.compute_n_step_returns(rewards, next_value)  # shape: (n_steps, num_envs)\n","                    returns = returns.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    values = torch.stack(values).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    log_probs = torch.stack(log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    advantages = returns - values\n","\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","                    # penalize using entropy to encourage exploration\n","                    entropy = action_dist.entropy().mean()\n","\n","                    loss = actor_loss + 0.4 * critic_loss- 0.01 * entropy\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","            step_sum += steps\n","\n","        print(step_sum)\n","        self.env.close()\n","        return np.array(episode_rewards)\n"]},{"cell_type":"code","source":["class A2CAgent_single:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","    def train(self):\n","        episode_rewards = []\n","        step_sum = 0\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            values = []\n","            rewards = []\n","            log_probs = []\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n","                done = terminated or truncated\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(reward)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or done:\n","                    _, next_value = self.policy_net(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n","                    next_value = next_value.squeeze()\n","                    # BUG ALERT\n","                    # MUST MULTIPLY (1 - done) to next_value to mask the bootstrapped next_value when the game is over. CRITICAL BUG THAT TOOK HOURS TO FIND\n","                    returns = self.compute_n_step_returns(rewards, next_value * (1 - done))\n","                    values = torch.stack(values)\n","                    log_probs = torch.stack(log_probs)\n","                    advantages = returns - values\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","\n","                    # penalize using entropy to encourage exploration\n","                    entropy = action_dist.entropy().mean()\n","                    loss = actor_loss + 0.4 * critic_loss- 0.01 * entropy\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","            step_sum += steps\n","\n","        self.env.close()\n","        print(step_sum)\n","        return np.array(episode_rewards)\n"],"metadata":{"id":"1yZl_zJG2ztG","executionInfo":{"status":"ok","timestamp":1747232801554,"user_tz":-540,"elapsed":36,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env = gym.make(\"CartPole-v1\")\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 1000 episodes explored\n","a2c_model_single_env =  A2CAgent_single(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_single_env.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAMO0Kpj2Es2","executionInfo":{"status":"ok","timestamp":1747234559980,"user_tz":-540,"elapsed":25855,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"591b627d-385d-4ab9-ebed-e1254a1e5257"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:25<00:00, 38.69it/s]"]},{"output_type":"stream","name":"stdout","text":["27862\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33639,"status":"ok","timestamp":1747234610785,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"szBR38HBF7_Q","outputId":"647f47c5-2ee0-4fe1-ab6f-c16821192669"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 125/125 [00:33<00:00,  3.72it/s]"]},{"output_type":"stream","name":"stdout","text":["19571\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["env_id = \"CartPole-v1\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 8000 episodes explored\n","a2c_model_multiple_env =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env.train()"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1747234611054,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"TPjtvt_7F7mW","outputId":"f561715a-3b2a-4be4-c63f-87a7b34990ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 76.0\n","Episode 2 Reward: 65.0\n","Episode 3 Reward: 72.0\n","Episode 4 Reward: 71.0\n","Episode 5 Reward: 69.0\n","Episode 6 Reward: 63.0\n","Episode 7 Reward: 77.0\n","Episode 8 Reward: 65.0\n","Episode 9 Reward: 63.0\n","Episode 10 Reward: 74.0\n","Average Reward over 10 episodes: 69.5\n"]}],"source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_single_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rkMAPuZTq8B","executionInfo":{"status":"ok","timestamp":1747234611720,"user_tz":-540,"elapsed":659,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"86a049b2-b0c4-4816-e822-36383e12f88f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 174.0\n","Episode 2 Reward: 174.0\n","Episode 3 Reward: 186.0\n","Episode 4 Reward: 203.0\n","Episode 5 Reward: 175.0\n","Episode 6 Reward: 202.0\n","Episode 7 Reward: 173.0\n","Episode 8 Reward: 189.0\n","Episode 9 Reward: 174.0\n","Episode 10 Reward: 223.0\n","Average Reward over 10 episodes: 187.3\n"]}],"source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 125\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 1000 episoded explored\n","a2c_model_multiple_env_125 =  A2CAgent(env_id, num_episodes=125, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env_125.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92W2A2XCM6yU","executionInfo":{"status":"ok","timestamp":1747234300712,"user_tz":-540,"elapsed":1164,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"162ebea5-e6c1-4434-b8b7-44f25ba0cd0b"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False  True False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False  True False False False False False]\n","[False False False False False  True False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False  True False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False  True  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False  True False False False False False]\n","[False False False False  True False False False]\n","[ True False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False  True False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False  True False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False  True False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False  True False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False  True False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[ True False False False False False False  True]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True  True False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[ True  True False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False  True]\n","[ True False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False  True False  True False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[ True False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False  True  True False False False False False]\n","[False False False False False False False False]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:01<00:00,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["[False False False False  True False False False]\n","[ True False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False  True False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False  True  True  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False  True False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False  True False]\n","[False False False False False False False False]\n","[ True False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False  True False False False False]\n","[False False False False False False False False]\n","[False False  True False False  True False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[False  True False False False False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","[ True False False False False  True False False]\n","[False False  True False False False False False]\n","[False False False False False False False False]\n","[False False False  True False False False False]\n","[False False False False False False False False]\n","[False False False False False False False  True]\n","[False False False False False False False False]\n","[False False False False False False  True False]\n","[False  True False False False False False False]\n","[False False False False  True False False False]\n","[False False False False False False False False]\n","[False False False False False False False False]\n","500\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env_125.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLoIDtGZM8Vb","executionInfo":{"status":"ok","timestamp":1747233970889,"user_tz":-540,"elapsed":1864,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"dc57a142-6a00-43c7-e197-76de84ba768c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 500.0\n","Episode 2 Reward: 500.0\n","Episode 3 Reward: 500.0\n","Episode 4 Reward: 500.0\n","Episode 5 Reward: 500.0\n","Episode 6 Reward: 500.0\n","Episode 7 Reward: 500.0\n","Episode 8 Reward: 500.0\n","Episode 9 Reward: 500.0\n","Episode 10 Reward: 500.0\n","Average Reward over 10 episodes: 500.0\n"]}]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 10\n","max_steps = 500\n","lr = 1e-3\n","\n","# total of 80 episodes explored\n","a2c_model_multiple_env_10 =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env_10.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHgJhh8H6YjC","executionInfo":{"status":"ok","timestamp":1747233990693,"user_tz":-540,"elapsed":10432,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"d232b207-91b3-4579-a23d-fbcb4ab7b51a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:10<00:00,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["5000\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env_10.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRVv-c8o6eXJ","executionInfo":{"status":"ok","timestamp":1747233991168,"user_tz":-540,"elapsed":469,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"389a5e92-ec99-47fc-fa19-a6da194635a5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 134.0\n","Episode 2 Reward: 132.0\n","Episode 3 Reward: 127.0\n","Episode 4 Reward: 122.0\n","Episode 5 Reward: 121.0\n","Episode 6 Reward: 107.0\n","Episode 7 Reward: 111.0\n","Episode 8 Reward: 101.0\n","Episode 9 Reward: 115.0\n","Episode 10 Reward: 123.0\n","Average Reward over 10 episodes: 119.3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoXsZSP1F20e"},"outputs":[],"source":["### Comparing single environment vs. synchronized environments vs. asynchronized environments"]},{"cell_type":"code","source":[],"metadata":{"id":"X6K5cuaQ89nP"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[],"authorship_tag":"ABX9TyOJSoWuQsTa2fIEIauu+pjz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}