{"cells":[{"cell_type":"code","execution_count":107,"metadata":{"id":"UG4iiEKdsB2l","colab":{"base_uri":"https://localhost:8080/","height":114},"executionInfo":{"status":"ok","timestamp":1748962134656,"user_tz":-540,"elapsed":15,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"e38a5de0-e468-4baa-ce42-c5ec125af628"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nImplementing PPO\\nFirst mistake I made -> ran the action using the policy, then calculated the ratio using old_policy. The correct way is to run the action using old_policy, then calculate the ratio using the new policy.\\nSecond mistake -> didn't run K epochs while calculating the surrogate objective.\\nThen, it started behaving like a trained model, though did not achieve goal.\\nAnother fix. Added minibatching according to the original paper.\\nMinibatching made the training significantly slower due to more frequent optimizer steps.\\nExperimented both GAE and decaying returns. Both did not show good results. Will try debugging in Cartpole.\\nTried using single environment and no minibatching like the reference model.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":107}],"source":["\"\"\"\n","Implementing PPO\n","First mistake I made -> ran the action using the policy, then calculated the ratio using old_policy. The correct way is to run the action using old_policy, then calculate the ratio using the new policy.\n","Second mistake -> didn't run K epochs while calculating the surrogate objective.\n","Then, it started behaving like a trained model, though did not achieve goal.\n","Another fix. Added minibatching according to the original paper.\n","Minibatching made the training significantly slower due to more frequent optimizer steps.\n","Experimented both GAE and decaying returns. Both did not show good results. Will try debugging in Cartpole.\n","Tried using single environment and no minibatching like the reference model -> didn't work.\n","Switched to time step based update instead of episode based updates -> didn't work.\n","Went back to decaying returns -> didn't work.\n","Found dimension bugs when calculating returns -> didn't fix.\n","Debugged by resetting decaying reward in terminated states 0> fixed!\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21539,"status":"ok","timestamp":1748169063399,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aNbCFO1m96R1","outputId":"a8925ba7-428c-49c1-c267-c2cc47506f7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10461,"status":"ok","timestamp":1748956171061,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"0Va0BgsV98Lt","outputId":"b7bdf438-112f-407a-ee1d-2e71c56d4ebc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.14.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1565,"status":"ok","timestamp":1748956172628,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KJwVWxwiW0Q3"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCriticPPO(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim, hidden_dims=(64, 64)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[1]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[1], output_dim)\n","        )\n","        self.critic_layer = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[1]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[1], 1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n"]},{"cell_type":"code","execution_count":110,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1748962398073,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"GYE9dNX2-NMy"},"outputs":[],"source":["from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","import copy\n","import time\n","\n","class PPOAgent:\n","    def __init__(self, env_id, total_timesteps=1e5, max_steps=500, epsilon=float('inf'), gamma=0.99, lambda_GAE=0.95,\n","                 lr=1e-3, num_steps=0, num_envs=8, num_epochs=4, minibatch_size=4, vectorization_mode=\"sync\", seed=123):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        # using vectorized environments to boost training speed\n","        if num_envs == 1:\n","            self.env = gym.make(env_id)\n","            self.policy_net = ActorCriticPPO(self.env.observation_space.shape[0], self.env.action_space.n).to(self.device)\n","        else:\n","            self.env = gym.make_vec(env_id, num_envs=num_envs, vectorization_mode=vectorization_mode)\n","            self.policy_net = ActorCriticPPO(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.num_envs = num_envs\n","        self.total_timesteps = int(total_timesteps)\n","        self.num_epochs = num_epochs\n","        self.max_steps = max_steps\n","        self.minibatch_size = minibatch_size\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","        self.lambda_GAE = lambda_GAE\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.old_policy_net = copy.deepcopy(self.policy_net)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        # added scheduler after observing divergence after getting close to solving\n","        self.scheduler = StepLR(self.optimizer, step_size=500 * num_epochs, gamma=0.9)\n","        self.loss = nn.MSELoss()\n","        self.seed = seed\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.old_policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_returns(self, rewards, values, next_value, is_dones):\n","        \"\"\"\n","        Args:\n","            rewards: torch.Tensor of shape [T, N] where\n","                    T = rollout steps, N = num_envs\n","        Returns:\n","            returns: torch.Tensor of shape [T, N], normalized\n","        \"\"\"\n","        # rewards' shape was [N, 1] -> caused bug\n","        rewards = torch.stack(rewards).squeeze()\n","        # also, haven't masked terminal states which led R to build up forever, not reset upon new episodes.\n","        is_dones = torch.stack([torch.tensor(d, device=rewards.device, dtype=torch.float32) for d in is_dones]).squeeze()\n","\n","        if self.num_envs == 1:\n","            T = rewards.shape[0]\n","            N = 1\n","        else:\n","            T, N = rewards.shape\n","        returns = torch.zeros_like(rewards)\n","        R = torch.zeros(N, device=rewards.device)\n","\n","        for t in reversed(range(T)):\n","            R = rewards[t] + self.gamma * R * (1 - is_dones[t])\n","            returns[t] = R\n","\n","        # normalize returns across all timesteps and environments\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","        advantages = returns - values\n","\n","        return returns, advantages\n","\n","    # computing GAE\n","    # def compute_returns(self, rewards, values, next_value):\n","    #     rewards = torch.stack(rewards)\n","    #     values = torch.cat([values, next_value.unsqueeze(0)], dim=0)\n","\n","    #     T, N = rewards.shape\n","    #     advantages = torch.zeros_like(rewards)\n","    #     gae = torch.zeros(N, device=rewards.device)\n","    #     for t in reversed(range(T)):\n","    #         # temporal difference error\n","    #         td = rewards[t] + self.gamma * values[t + 1] - values[t]\n","    #         # higher labmda -> more sampling, lower lambda -> more bootstrapping\n","    #         gae = td + self.gamma * self.lambda_GAE * gae\n","    #         advantages[t] = gae\n","\n","    #     # compute returns by adding value to advantage\n","    #     returns = advantages + values[:-1]\n","    #     # returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","\n","    #     # normalize advantage across timesteps and environments\n","    #     advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","    #     return returns, advantages\n","\n","\n","\n","    def train(self):\n","        episode_rewards = []\n","        steps = 0\n","        recent_rewards = []\n","        old_states, old_actions, old_log_probs, old_rewards, old_values, is_dones = [], [], [], [], [], []\n","\n","        while steps < self.total_timesteps:\n","            start_episode_time = time.time()  # ⏱ total episode start\n","            env_time = 0                      # ⏱ initialize environment interaction time\n","            train_time = 0                    # ⏱ initialize training time\n","\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            done_steps = np.zeros(self.num_envs)\n","\n","            for _ in range(self.num_steps):\n","                if steps >= self.total_timesteps:\n","                    break\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                old_states.append(state_tensor)\n","\n","                # ⏱ start measuring env time\n","                start_env_time = time.time()\n","                with torch.no_grad():\n","                    old_action_probs, old_value = self.old_policy_net(state_tensor)\n","                    old_action_dist = torch.distributions.Categorical(old_action_probs)\n","                    old_action = old_action_dist.sample()\n","                    old_log_prob = old_action_dist.log_prob(old_action)\n","\n","                next_state, old_reward, terminated, truncated, _ = self.env.step(old_action.cpu().numpy())\n","                env_time += time.time() - start_env_time  # ⏱ accumulate env time\n","\n","                done = np.logical_or(terminated, truncated)\n","                if self.num_envs == 1:\n","                    if done:\n","                        next_state, _ = self.env.reset()\n","                else:\n","                    if np.any(done):\n","                        reset_indices = np.where(done)[0]\n","                        reset_obs, _ = self.env.reset(seed=None, options=None, indices=reset_indices)\n","                        next_state[reset_indices] = reset_obs\n","                done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","                done_mask = np.logical_or(done_mask, done)\n","                old_reward = np.where(done_mask, 0.0, old_reward)\n","\n","                old_values.append(old_value.squeeze())\n","                old_rewards.append(torch.tensor(old_reward, dtype=torch.float32).to(self.device))\n","                old_log_probs.append(old_log_prob)\n","                old_actions.append(old_action)\n","                is_dones.append(done_mask)\n","\n","                episode_reward += old_reward\n","                state = next_state\n","\n","                if steps % (self.num_steps * 4) == 0:\n","                    with torch.no_grad():\n","                        next_old_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_old_value = self.policy_net(next_old_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_old_value = next_old_value.squeeze() * (1 - done_tensor)\n","\n","                    old_states = torch.stack(old_states).to(self.device)\n","                    old_values = torch.stack(old_values).to(self.device)\n","                    old_log_probs = torch.stack(old_log_probs).to(self.device)\n","                    old_actions = torch.stack(old_actions).to(self.device)\n","                    old_returns, old_advantages = self.compute_returns(old_rewards, old_values, next_old_value, is_dones)\n","\n","                    if self.num_envs == 1:\n","                        old_states = old_states.squeeze()         # shape: [T, obs_dim]\n","                        old_actions = old_actions.squeeze()       # shape: [T]\n","                        old_log_probs = old_log_probs.squeeze()   # shape: [T]\n","                        old_returns = old_returns.squeeze()       # shape: [T]\n","                        old_advantages = old_advantages.squeeze() # shape: [T]\n","                    else:\n","                        T, N = old_actions.shape[:2]\n","                        old_states = old_states.reshape(T * N, -1)\n","                        old_actions = old_actions.reshape(T * N)\n","                        old_log_probs = old_log_probs.reshape(T * N)\n","                        old_returns = old_returns.reshape(T * N)\n","                        old_advantages = old_advantages.reshape(T * N)\n","\n","                    batch_size = old_states.shape[0]\n","                    minibatch_size = self.minibatch_size\n","\n","                    # ⏱ measure train time\n","                    start_train_time = time.time()\n","\n","                    for _ in range(self.num_epochs):\n","                        action_probs, values = self.policy_net(old_states)\n","                        action_dist = torch.distributions.Categorical(action_probs)\n","                        log_probs = action_dist.log_prob(old_actions)\n","\n","                                # indices = torch.randperm(batch_size)\n","                                # # mini batching\n","                                # for i in range(0, batch_size, minibatch_size):\n","                                #     # selecting inputs for this mini batch\n","                                #     mb_idx = indices[i : i + minibatch_size]\n","                                #     mb_states = old_states[mb_idx]\n","                                #     mb_actions = old_actions[mb_idx]\n","                                #     mb_log_probs_old = old_log_probs[mb_idx]\n","                                #     mb_returns = old_returns[mb_idx]\n","                                #     mb_advantages = old_advantages[mb_idx]\n","\n","                                #     # use current policy to find log_probs of the trajectory ran by old_policy\n","                                #     action_probs, values = self.policy_net(mb_states)\n","                                #     action_dist = torch.distributions.Categorical(action_probs)\n","                                #     mb_log_probs = action_dist.log_prob(mb_actions)\n","\n","                                #     prob_ratio = torch.exp(mb_log_probs - mb_log_probs_old.detach())\n","                                #     values = values.squeeze(-1)\n","                                #     surrogate = torch.min(prob_ratio * mb_advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * mb_advantages)\n","                                #     actor_loss = -surrogate.mean()\n","                                #     critic_loss = self.loss(mb_returns, values)\n","\n","                                #     loss = actor_loss + 0.5 * critic_loss\n","                                #     self.optimizer.zero_grad()\n","                                #     loss.backward()\n","                                #     self.optimizer.step()\n","\n","                        prob_ratio = torch.exp(log_probs - old_log_probs.detach())\n","                        values = values.squeeze(-1)\n","                        surrogate = torch.min(\n","                            prob_ratio * old_advantages,\n","                            prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * old_advantages\n","                        )\n","                        actor_loss = -surrogate.mean()\n","\n","                        critic_loss = self.loss(old_returns, values)\n","\n","                        entropy = action_dist.entropy().mean()\n","\n","                        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n","\n","                        self.optimizer.zero_grad()\n","                        loss.mean().backward()\n","                        self.optimizer.step()\n","\n","                    train_time += time.time() - start_train_time  # ⏱ add train time\n","                    old_states, old_actions, old_log_probs, old_rewards, old_values, is_dones = [], [], [], [], [], []\n","                    # self.scheduler.step()\n","                    self.old_policy_net = copy.deepcopy(self.policy_net)\n","\n","            episode_rewards.append(episode_reward)\n","            recent_rewards.append(np.mean(episode_reward))\n","            if len(recent_rewards) > 10:\n","                recent_rewards.pop(0)\n","\n","            # Log every batch of self.num_steps * num_envs timesteps\n","            if steps % (self.num_envs * self.num_steps * 10) == 0:\n","                print(f\"\\n[Timesteps: {steps}]\")\n","                print(f\"Reward (mean over envs): {np.mean(episode_reward):.2f} | Avg Reward (last 10): {np.mean(recent_rewards):.2f}\")\n","                print(f\"Actor Loss: {actor_loss.item():.4f} | Critic Loss: {critic_loss.item():.4f} | Entropy Loss: {entropy.item():.4f}\")\n","                print(f\"Env Time: {env_time:.2f}s | Train Time: {train_time:.2f}s | Total: {time.time() - start_episode_time:.2f}s\")\n","                print(f\"Action std: {action_probs.std().item():.4f}\")\n","\n","        self.env.close()\n","        return np.array(episode_rewards)\n"]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xS4b8URHWybG","executionInfo":{"status":"ok","timestamp":1748962384548,"user_tz":-540,"elapsed":137536,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"89fe76e3-619a-4a1f-8d60-f1de176e213f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[Timesteps: 10000]\n","Reward (mean over envs): 42.00 | Avg Reward (last 10): 24.20\n","Actor Loss: 0.0084 | Critic Loss: 0.9889 | Entropy Loss: 0.6536\n","Env Time: 1.06s | Train Time: 0.00s | Total: 1.19s\n","Action std: 0.1386\n","\n","[Timesteps: 20000]\n","Reward (mean over envs): 24.00 | Avg Reward (last 10): 61.90\n","Actor Loss: 0.0874 | Critic Loss: 0.9738 | Entropy Loss: 0.6115\n","Env Time: 1.04s | Train Time: 0.29s | Total: 1.92s\n","Action std: 0.1961\n","\n","[Timesteps: 30000]\n","Reward (mean over envs): 63.00 | Avg Reward (last 10): 80.50\n","Actor Loss: 0.0001 | Critic Loss: 0.9772 | Entropy Loss: 0.5898\n","Env Time: 1.07s | Train Time: 0.00s | Total: 1.20s\n","Action std: 0.2187\n","\n","[Timesteps: 40000]\n","Reward (mean over envs): 232.00 | Avg Reward (last 10): 107.70\n","Actor Loss: 0.0186 | Critic Loss: 0.9392 | Entropy Loss: 0.5811\n","Env Time: 1.04s | Train Time: 0.29s | Total: 1.92s\n","Action std: 0.2258\n","\n","[Timesteps: 50000]\n","Reward (mean over envs): 57.00 | Avg Reward (last 10): 107.00\n","Actor Loss: 0.0600 | Critic Loss: 0.9668 | Entropy Loss: 0.6032\n","Env Time: 1.06s | Train Time: 0.00s | Total: 1.19s\n","Action std: 0.2046\n","\n","[Timesteps: 60000]\n","Reward (mean over envs): 118.00 | Avg Reward (last 10): 89.70\n","Actor Loss: 0.0066 | Critic Loss: 0.9798 | Entropy Loss: 0.5755\n","Env Time: 1.03s | Train Time: 0.28s | Total: 1.89s\n","Action std: 0.2325\n","\n","[Timesteps: 70000]\n","Reward (mean over envs): 127.00 | Avg Reward (last 10): 111.00\n","Actor Loss: 0.0478 | Critic Loss: 0.9765 | Entropy Loss: 0.5624\n","Env Time: 1.05s | Train Time: 0.00s | Total: 1.18s\n","Action std: 0.2439\n","\n","[Timesteps: 80000]\n","Reward (mean over envs): 134.00 | Avg Reward (last 10): 101.60\n","Actor Loss: 0.0130 | Critic Loss: 0.9535 | Entropy Loss: 0.5633\n","Env Time: 1.03s | Train Time: 0.28s | Total: 1.92s\n","Action std: 0.2430\n","\n","[Timesteps: 90000]\n","Reward (mean over envs): 117.00 | Avg Reward (last 10): 134.30\n","Actor Loss: 0.0226 | Critic Loss: 0.9546 | Entropy Loss: 0.5432\n","Env Time: 1.06s | Train Time: 0.00s | Total: 1.19s\n","Action std: 0.2601\n","\n","[Timesteps: 100000]\n","Reward (mean over envs): 121.00 | Avg Reward (last 10): 143.70\n","Actor Loss: 0.0207 | Critic Loss: 0.9533 | Entropy Loss: 0.5229\n","Env Time: 1.02s | Train Time: 0.28s | Total: 1.89s\n","Action std: 0.2761\n"]}],"source":["import gymnasium as gym\n","\n","env_id = 'CartPole-v1'\n","max_steps = 1000\n","lr = 3e-4\n","\n","\n","ppo_model =  PPOAgent(env_id, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=1, num_steps=1000, num_epochs = 80, minibatch_size=16)\n","\n","rewards = ppo_model.train()\n","\n"]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12872,"status":"ok","timestamp":1748962415786,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KwcnQRLD9cTX","outputId":"e0538e67-4442-4e61-a649-6bda079578df"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 179.0\n","Episode 2 Reward: 473.0\n","Episode 3 Reward: 169.0\n","Episode 4 Reward: 251.0\n","Episode 5 Reward: 291.0\n","Episode 6 Reward: 200.0\n","Episode 7 Reward: 500.0\n","Episode 8 Reward: 166.0\n","Episode 9 Reward: 227.0\n","Episode 10 Reward: 240.0\n","Average Reward over 10 episodes: 269.6\n"]}],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = ppo_model.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAXsopwasbMo"},"outputs":[],"source":["import gymnasium as gym\n","from gymnasium import spaces\n","import numpy as np\n","\n","class LinearValueEnv(gym.Env):\n","    def __init__(self, gamma=0.99, episode_length=100):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.episode_length = episode_length\n","        self.current_step = 0\n","\n","        # Observation: continuous scalar in [-1, 1]\n","        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n","\n","        # Action: continuous scalar (not used)\n","        self.action_space = spaces.Discrete(1)\n","\n","        self.state = None\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.state = np.random.uniform(-1.0, 1.0, size=(1,)).astype(np.float32)\n","        self.current_step = 0\n","        return self.state.copy(), {}\n","\n","    def step(self, action):\n","        # Reward is simply the state value\n","        reward = float(self.state[0])\n","        self.current_step += 1\n","\n","        terminated = self.current_step >= self.episode_length\n","        truncated = False\n","        return self.state.copy(), reward, terminated, truncated, {}\n","\n","    def render(self):\n","        print(f\"State: {self.state}\")\n","\n","    def close(self):\n","        pass\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UO-G1kxysz46"},"outputs":[],"source":["from gymnasium.envs.registration import register\n","\n","register(\n","    id=\"LinearValue-v0\",\n","    entry_point=\"__main__:LinearValueEnv\",  # if you're running in a script\n","    max_episode_steps=100\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQ0d1b4tKwwl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-BU9qP2s0si"},"outputs":[],"source":["env_id = \"LinearValue-v0\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model_value =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=2, num_steps=0)\n","\n","rewards, steps = ppo_model_value.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pV9BZMLONrhD"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNe6yBYErzRjgW16V27YIfC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}