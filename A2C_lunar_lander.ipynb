{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"id":"LA6ipJjA9UFJ","executionInfo":{"status":"ok","timestamp":1747579022057,"user_tz":-540,"elapsed":35,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["# LunarLander debug note\n","# Initially, used the same code I used for cartpole.\n","# Would not train, so printed out the probs. When the model is first made, it is fairly distributed like this [[0.2660, 0.2257, 0.2731, 0.2352]]. However, after training for 1000 steps,\n","# it converges to a certain action like this - tensor([[9.9943e-01, 4.8898e-04, 7.9276e-05, 4.1656e-08]]\n","# Applied entropy for more exploration. Didn't work\n","# Found much larger absolute value of advantages, returns, compared to cartpole. Especially, when terminated, the return is -100 which is the dominant cause for the returns.\n","# Normalizing advantage solved the problem above. Now it doesn't fixate in a certain action.\n","# Then, action_probability turned into a somewhat uniform distribution. Therefore, printed out the losses.\n","# Actor loss:  tensor(-1.9073e-08, device='cuda:0', grad_fn=<NegBackward0>) Critic loss:  tensor(15.9172, device='cuda:0', grad_fn=<MseLossBackward0>)\n","# Entropy:  tensor(1.3784, device='cuda:0', grad_fn=<MeanBackward0>)\n","# Found out that Actor loss is extremely small. Therefore, actor layer was barely getting updated.\n","# Reason: log prob is pretty much uniform when model is initialized and the advantage is normalized with mean 0. We calculate actor_loss = (advantages * log_probs).mean()\n","# This converges to the mean of the normal distribution (since log_probs is uniform), and therefore to 0.\n","# Try with normalizing the n-step rewards instead of the advantage.\n","# Reference https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/blob/master/train.py\n","# The only difference between my model and the reference model was the reduction method of actor and critic loss. Reference model used sum, my model used mean.\n","# mean vs. sum was not the problem. The dimensions of log_probs, returns, state_values were all different in my code and was broadcasting very wierdly. That's why step-wise calculation of the reference code\n","# worked, but my code didn't. Should pay more attention to dimensions, and also debugging warnings because I was ignoring the dimension disparity.\n","# Trains well using mean. Maybe even better.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":18029,"status":"error","timestamp":1746622996822,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aNbCFO1m96R1","outputId":"b4c2ba5b-4c48-4b4d-d2cb-ef95ab68535f"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    355\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Va0BgsV98Lt","executionInfo":{"status":"ok","timestamp":1747573347116,"user_tz":-540,"elapsed":8784,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"4067f828-9999-43b7-d4a1-ba0fef115867"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.7.1)\n","Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","\u001b[33m  DEPRECATION: Building 'box2d-py' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'box2d-py'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n","\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n","Failed to build box2d-py\n","\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools pip --upgrade\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","source":["!pip uninstall -y box2d-py\n","!pip install box2d pygame swig\n","!pip install \"gymnasium[box2d]\" --no-deps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aVKs6T5ifpUd","executionInfo":{"status":"ok","timestamp":1747573456733,"user_tz":-540,"elapsed":3833,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"219417d0-e99b-4b8a-a121-ac770cc91ffb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping box2d-py as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting box2d\n","  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aZCrgHbE94dZ","executionInfo":{"status":"ok","timestamp":1747573279634,"user_tz":-540,"elapsed":6803,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["# New implementation following https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/blob/master/train.py\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","\n","class ActorCritic(nn.Module):\n","\n","    # Increased hidden dim to (128, 128) compared to cartpole as the input_dim is more complex\n","    def __init__(self, input_dim, output_dim, hidden_dims=(128, 128)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            # nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            # nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[0], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[0], 1)\n","\n","        self.logprobs = []\n","        self.state_values = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        state_value = self.critic_layer(x)\n","\n","        action_distribution = Categorical(action_probs)\n","        action = action_distribution.sample()\n","\n","        self.logprobs.append(action_distribution.log_prob(action))\n","        self.state_values.append(state_value.squeeze())\n","\n","        return action.item()\n","\n","    def compute_return(self, gamma):\n","        returns = []\n","        R = 0\n","        for r in reversed(self.rewards):\n","            R = r + gamma * R\n","            returns.insert(0, R)\n","\n","        returns = torch.tensor(returns)\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","        return returns\n","\n","    def calculate_loss(self, gamma):\n","        rewards = self.compute_return(gamma).detach()\n","\n","        # CRITICAL BUG - log_probs' shape was [90, 1], state_values' [90, 1, 1], rewards' [90] -- should read the warning messages more carefully from now on.\n","        # print(\"log_probs.shape\", self.logprobs.shape)\n","        # print(\"state_values.shape\", self.state_values.shape)\n","        # print(\"returns.shape\", returns.shape)\n","\n","        loss = 0\n","        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n","            value = value.to('cpu')\n","            advantage = reward - value.detach()\n","            action_loss = -logprob * advantage\n","            value_loss = F.smooth_l1_loss(value, reward)\n","            loss += (action_loss + value_loss)\n","        return loss\n","\n","\n","    def clearMemory(self):\n","        del self.logprobs[:]\n","        del self.state_values[:]\n","        del self.rewards[:]\n","\n"]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        random_seed = 543\n","        torch.manual_seed(random_seed)\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","                action = self.policy_net(state_tensor)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action)\n","                done = terminated or truncated\n","                self.policy_net.rewards.append(reward)\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                # if (steps % self.num_steps == 0) or done:\n","                if done:\n","                    self.optimizer.zero_grad()\n","                    loss = self.policy_net.calculate_loss(self.gamma)\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    self.policy_net.clearMemory()\n","\n","            if episode % 100 == 0:\n","                print(episode, \"reward: \", episode_reward, \"steps: \", steps)\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"],"metadata":{"id":"fkfHOIJ5DxWh","executionInfo":{"status":"ok","timestamp":1747573279639,"user_tz":-540,"elapsed":2,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DxncwMM9Zn8"},"outputs":[],"source":["import gymnasium as gym\n","\n","env = gym.make('LunarLander-v3')\n","env.reset(seed=543)\n","num_episodes = 2000\n","max_steps = 1000\n","lr = 0.02\n","\n","\n","a2c_model_ll =  A2CAgent(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 8)\n","\n","state, _ = env.reset()\n","state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(a2c_model_ll.device)\n","rewards, steps = a2c_model_ll.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxOavbFc2WGX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(len(rewards)), rewards)\n","plt.show()"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCritic(nn.Module):\n","\n","    # reference model used only 1 layer. Will experiment both\n","    def __init__(self, input_dim, output_dim, hidden_dims=(128, 128)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            # nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            # nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n"],"metadata":{"id":"KJwVWxwiW0Q3","executionInfo":{"status":"ok","timestamp":1747573466970,"user_tz":-540,"elapsed":18,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GYE9dNX2-NMy","executionInfo":{"status":"ok","timestamp":1747575460306,"user_tz":-540,"elapsed":26,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"outputs":[],"source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5, num_envs = 8, vectorization_mode = \"sync\"):\n","        # using vectorized environments to boost training speed\n","        self.env = gym.make_vec(env_id, num_envs = num_envs, vectorization_mode=vectorization_mode)\n","        self.env.reset(seed=543)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_returns(self, rewards):\n","        \"\"\"\n","        Args:\n","            rewards: torch.Tensor of shape [T, N] where\n","                    T = rollout steps, N = num_envs\n","        Returns:\n","            returns: torch.Tensor of shape [T, N], normalized\n","        \"\"\"\n","        rewards = torch.stack(rewards)\n","\n","        T, N = rewards.shape\n","        returns = torch.zeros_like(rewards)\n","        R = torch.zeros(N, device=rewards.device)\n","        for t in reversed(range(T)):\n","            R = rewards[t] + self.gamma * R\n","            returns[t] = R\n","\n","        # Normalize returns across all timesteps and environments\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","\n","        return returns\n","\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        returns = torch.stack(returns)\n","        if returns.numel() > 1:\n","            return (returns - returns.mean()) / (returns.std() + 1e-8)\n","        else:\n","            return returns * 0\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs = [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            done_steps = np.zeros(self.num_envs)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","                done_mask = np.logical_or(done_mask, done)\n","                # record when each environment is done\n","                reward = np.where(done_mask, 0.0, reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                # if (steps % self.num_steps == 0) or np.any(done):\n","                if np.any(done):\n","                    with torch.no_grad():\n","                        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_value = self.policy_net(next_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                    returns = self.compute_returns(rewards)  # shape: (n_steps, num_envs)\n","                    returns = returns.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    values = torch.stack(values).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    log_probs = torch.stack(log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    advantages = returns - values\n","\n","                    # calculate sum instead of mean\n","                    actor_loss = - (log_probs * advantages.detach()).sum()\n","                    critic_loss = self.loss(returns, values)\n","\n","                    loss = actor_loss + critic_loss\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","            if episode % 20 == 0:\n","               print('Episode {}\\tlengths: {}\\treward: {}]\\tfull length: {}'.format(episode, done_steps, episode_reward, steps))\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env_id = 'LunarLander-v3'\n","num_episodes = 1000\n","max_steps = 1000\n","lr = 0.02\n","\n","\n","a2c_model_ll =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 8)\n","\n","rewards, steps = a2c_model_ll.train()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xS4b8URHWybG","executionInfo":{"status":"ok","timestamp":1747575231189,"user_tz":-540,"elapsed":1728354,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"4b8bf32b-0e59-4493-f9ce-fe1a20d4adc4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/1000 [00:01<27:29,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 0\tlengths: [115. 112. 192.  92. 118.  60. 123.  84.]\treward: [-245.34536021 -191.66144945 -500.03720977  -11.33437027 -178.22374885\n","    9.63657237 -559.72998484   96.26161105]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 21/1000 [00:23<07:55,  2.06it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 20\tlengths: [101. 167.  94. 102.  70.  97.  95.  69.]\treward: [-155.8002251  -243.81072087 -426.3456884    24.2093625  -445.11672289\n"," -281.91675918  -47.03076698 -378.58775796]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 41/1000 [00:42<29:40,  1.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 40\tlengths: [ 300.  157.  315. 1000.  197.  204.  108. 1000.]\treward: [ 36.25827272  54.9444442  -95.51194537  99.16392461  92.59645224\n"," -94.05307007  41.41175421  25.86264929]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 61/1000 [01:16<39:03,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 60\tlengths: [ 683.  106.  127.  116.  412. 1000.  428.  148.]\treward: [131.71595876  84.68805584 105.18670653 106.84486447 118.98577649\n"," 151.13403931 121.03806236 166.19470372]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 81/1000 [02:04<37:54,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 80\tlengths: [ 137.   94. 1000.   82.   86. 1000. 1000. 1000.]\treward: [ 121.24043136   98.28497141 -123.28026087   46.47805361   92.10324289\n"," -183.81792048  156.28795757  148.53421755]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 101/1000 [02:36<12:27,  1.20it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 100\tlengths: [154. 211. 109. 130. 124. 159. 191. 136.]\treward: [  87.26877879   76.60109185    1.07319248  -84.91215836   56.39140083\n","   65.89283573 -110.52570303  -96.25679812]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 121/1000 [02:58<17:14,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 120\tlengths: [262. 135. 333. 392. 188. 306. 349. 119.]\treward: [102.6902332  116.68060071 137.06094067  47.44396511 -76.53314038\n"," 135.41830985 132.38750216 -61.77191887]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 141/1000 [03:27<25:42,  1.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 140\tlengths: [ 89.  82. 133. 148. 120. 100. 127.  91.]\treward: [ 85.1159127   82.63594322  79.58670875 110.04740411 -42.28148933\n","  65.11869822 135.94782051  81.35190039]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 161/1000 [03:57<13:16,  1.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 160\tlengths: [279. 266. 146. 265. 185. 288. 274. 244.]\treward: [-678.58758938 -568.01361594 -190.00911658 -486.5083296  -653.58940941\n"," -723.4504952  -629.01211003 -674.01577993]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 181/1000 [04:19<15:12,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 180\tlengths: [199. 323. 267. 241. 148. 246. 225. 164.]\treward: [ -62.72877465   -3.50574871  -89.90072411  -75.10888283 -153.97001547\n"," -100.95841971   72.44573928  -32.04543514]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 201/1000 [04:56<33:41,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 200\tlengths: [ 386.  155.  151.  316. 1000.  486. 1000.  408.]\treward: [ 127.92530205  -74.14099929  -78.17412575  204.43273283 -142.81996895\n","  170.62510681 -145.79560213  114.35030478]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 221/1000 [05:51<35:57,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 220\tlengths: [ 143. 1000.  190. 1000.  108. 1000.  150.  783.]\treward: [  75.93219279 -105.79053423  101.68530347 -117.26778223   77.13229268\n","  -82.99989202   91.23330814    1.12782198]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 241/1000 [06:46<36:08,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 240\tlengths: [ 383. 1000. 1000. 1000.  242.  601. 1000. 1000.]\treward: [  74.63851506  -89.3659809  -105.53907031 -102.94729505  -55.07631686\n","  119.19244829    3.40923666 -124.26429833]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 261/1000 [07:42<33:56,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 260\tlengths: [ 371.  102.  231. 1000. 1000. 1000.  286. 1000.]\treward: [-133.89641451   99.67162432   78.97629744 -112.28776086 -123.30926123\n"," -136.94298051  160.27291714 -138.39553816]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 281/1000 [08:38<33:38,  2.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 280\tlengths: [ 164.  156. 1000. 1000.  154. 1000.  166. 1000.]\treward: [ 139.76030029  131.06837481  -90.48632439 -120.44240772  -11.98157949\n","  -66.41738716  109.03068275 -124.38967718]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 301/1000 [09:32<29:31,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 300\tlengths: [ 320.  224. 1000.  168. 1000. 1000. 1000. 1000.]\treward: [  72.33825265   92.75781625 -100.53717755  119.3523404  -119.69584177\n","  -54.63099816 -101.01475561  -34.0739966 ]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 321/1000 [10:26<26:19,  2.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 320\tlengths: [511. 282. 414. 156. 394. 548. 546. 145.]\treward: [178.59451195 109.10239143  31.06058293  94.8979631   10.57932359\n"," -14.29096039  55.8626344  -38.12067883]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 341/1000 [11:15<19:46,  1.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 340\tlengths: [132. 171. 199. 310. 191. 393. 302. 159.]\treward: [-58.5285509   95.03906084  88.16874432  74.88118691 -65.85876407\n","  64.29318345  41.58052574 127.55689323]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 361/1000 [11:47<16:40,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 360\tlengths: [297.  92. 205. 434. 197. 296. 153. 136.]\treward: [ 80.86221921 137.15593677  91.40627644  81.59909336 -51.02244472\n","  78.06286163  93.93002093 -55.64185208]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 381/1000 [12:37<29:05,  2.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 380\tlengths: [205. 825. 196. 268.  92. 650. 164. 904.]\treward: [ 81.34834166  23.0333727  -65.91814556  72.10469494 128.4677137\n","  70.38575124 116.36489928  51.65949671]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 401/1000 [13:26<24:53,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 400\tlengths: [ 292.  694.  163. 1000. 1000.  754. 1000.  612.]\treward: [  32.77006438   23.14179414 -106.84157152 -158.06209929 -174.40479767\n"," -249.20073494  100.12105731  -37.91379588]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 42%|████▏     | 421/1000 [14:14<21:27,  2.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 420\tlengths: [112. 164. 330. 150. 346.  81. 113. 362.]\treward: [105.80808001 169.41386285 167.40635582 111.84117864 151.39108756\n","  80.73344567 -42.60983933 162.38802472]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 441/1000 [15:01<19:03,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 440\tlengths: [ 74. 207. 342. 207. 513. 158. 234. 205.]\treward: [-59.27552048  95.4300757   63.30034576 118.06193808  88.79323914\n"," -68.19937428  52.05749778  51.58770277]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 461/1000 [15:31<11:00,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 460\tlengths: [339. 250. 311. 269. 212. 261. 228. 240.]\treward: [ 174.460396     43.58129604    6.06236435  -77.55874279 -109.98656502\n"," -126.78305143 -117.78754448 -163.08289765]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 481/1000 [16:07<21:29,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 480\tlengths: [ 184.  244.  832.  438.   71.  157. 1000.  144.]\treward: [  51.13555438   37.15444509 -109.1954511    59.38020324 -108.25502635\n","   82.95503617 -211.88334161   73.34869258]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 501/1000 [16:35<09:08,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 500\tlengths: [172. 214.  70. 121. 314.  87. 199. 146.]\treward: [119.78704893 129.39036292 -59.19749181 -52.50968776  97.56239334\n"," 103.46110488  65.6335222   84.09609062]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 521/1000 [16:56<08:15,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 520\tlengths: [ 94.  76. 251. 114. 163.  59. 274. 166.]\treward: [-118.26326498 -127.04786604   -1.3836776   -74.37574693   70.84393706\n"," -178.54553924  140.99598524   63.65890687]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 541/1000 [17:16<08:11,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 540\tlengths: [174. 468.  74.  91. 133.  84.  78. 100.]\treward: [  58.72060986   89.51618692 -249.25307947 -233.20476807   44.65433467\n"," -128.67555112 -247.30756508  121.66489266]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 561/1000 [17:36<06:34,  1.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 560\tlengths: [190. 131. 133.  91. 147.  84.  95. 146.]\treward: [ -65.06832145 -105.85827197   71.70869902  -48.75376526   97.37811616\n"," -282.84069006 -125.180682    114.80074004]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 581/1000 [18:17<15:28,  2.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 580\tlengths: [1000.  343.  336.  825.  105.   83.   95.  152.]\treward: [-119.26616631   98.29182254   66.71973997   38.6068371    63.25992627\n","  -49.77663382  100.72464711  138.8558806 ]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 601/1000 [19:10<18:37,  2.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 600\tlengths: [ 238.  150.  883.  383. 1000. 1000. 1000. 1000.]\treward: [-125.7155418  -159.28458166 -103.31086137   55.78530788  -85.40632807\n"," -148.62462273 -191.14069943 -137.78676338]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 621/1000 [20:00<17:23,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 620\tlengths: [ 237.  225.  158.  100. 1000.  231.  206.  350.]\treward: [  67.37747472   90.45867616  -43.84796599   32.26899408 -122.55685083\n","   82.11232101  -83.58658458 -236.19173889]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 641/1000 [20:49<12:19,  2.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 640\tlengths: [438. 136.  93. 728. 134. 161. 215. 232.]\treward: [ 125.05805414  -91.83819208  -49.9161833   -30.41033699  -79.8770048\n","   80.49001576 -128.62000897  116.50498041]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 661/1000 [21:21<09:07,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 660\tlengths: [191.  95.  98. 229. 127. 324. 178. 195.]\treward: [ 130.55773653   74.43907236   54.83478175  -98.45494295 -117.59201963\n","   17.39386498   58.69168772  -89.21085109]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 681/1000 [21:46<07:14,  1.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 680\tlengths: [ 80. 107. 206. 137. 353. 311. 254. 295.]\treward: [ 44.8232395  -67.00974626  71.83005556 119.15018302 150.72253742\n","  40.17496413  61.58742817 154.14449279]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 701/1000 [22:15<05:35,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 700\tlengths: [478. 415. 158. 140.  90.  99. 133. 294.]\treward: [108.42309629 117.61005584 140.55860475 136.5528924   67.34218513\n","  68.3949471  -49.03626577  80.85962704]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 721/1000 [22:47<06:31,  1.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 720\tlengths: [340. 147.  79. 193.  66.  66.  79.  53.]\treward: [  71.52600322   58.03084623 -209.55964911  141.63479287 -219.79755649\n"," -151.89928434  -45.56016623 -189.07465092]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 741/1000 [23:20<10:46,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 740\tlengths: [290. 277. 296.  80.  85. 283. 372. 810.]\treward: [ -67.29206288   98.22519476  119.35863638   48.07049934   55.75295514\n","  125.61177833  -27.20088073 -102.26978281]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 761/1000 [24:02<06:42,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 760\tlengths: [134. 366. 724. 127. 238. 111. 164. 325.]\treward: [116.77773477  88.03754466  13.99733532 -83.16667117  57.60059822\n"," -35.87212923 115.74310745 174.02564522]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 781/1000 [24:29<03:58,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 780\tlengths: [396. 352. 159. 252. 264. 160. 225. 232.]\treward: [-134.38696067    3.60052413  -84.51360507   38.91071199   19.39984031\n","   20.85179663   77.16206825  -99.67072692]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 801/1000 [24:51<03:23,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 800\tlengths: [249. 232. 456. 237. 224. 253. 132. 156.]\treward: [ 69.12882639 205.77474962 108.01877663 -91.29077284 -83.2714081\n"," -88.28082838 -63.62172644 -90.14823556]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 821/1000 [25:16<03:28,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 820\tlengths: [222. 218. 392. 165. 339. 242. 369. 163.]\treward: [ 74.55110002 112.7158559  140.96138643 168.69438227 -79.87556682\n","  93.81456088 151.05397347  78.17752628]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 841/1000 [25:40<03:16,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 840\tlengths: [163. 314. 416. 256. 306. 211. 247. 208.]\treward: [ 45.75726139  14.74570441  -7.88837625 110.4376182    0.80634217\n"," -53.14588687  93.23230597 -74.18582462]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 861/1000 [26:03<02:42,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 860\tlengths: [302. 478. 253. 465. 257. 375. 498. 264.]\treward: [-108.71186244  152.16549746   90.95988728 -217.59617464   69.35195708\n","   46.76918596  -57.03299706 -124.94465846]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 881/1000 [26:31<02:39,  1.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 880\tlengths: [407. 300. 277. 208. 476. 349. 288. 489.]\treward: [ 107.74613327   80.26340373 -112.73295543  -53.59094314   79.87784966\n","  170.74108868   90.65675995  150.19819008]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 901/1000 [26:58<02:07,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 900\tlengths: [279. 244. 282. 462. 190. 142. 204. 265.]\treward: [  21.3207728    72.92064394 -114.72126275   16.15195808  145.59188791\n","  -50.25099448  -47.62084701  -70.13688599]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 921/1000 [27:23<01:22,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 920\tlengths: [386.  80. 136. 325. 118. 150. 134.  78.]\treward: [  8.72059442  75.05343801 -70.34026224  94.88434423 134.27390313\n"," -79.09664676 110.4687373   98.81491069]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▍| 941/1000 [27:44<01:02,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 940\tlengths: [198. 224. 122. 390. 292. 237. 253. 385.]\treward: [-74.84835203  88.30448988 -72.80138677 135.6611101  176.10431465\n","  91.25914809  90.67378591 127.15762369]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 961/1000 [28:06<00:44,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 960\tlengths: [189. 188. 245. 240. 178. 337. 126. 201.]\treward: [136.20201953  73.94018374  96.70209153  88.408784   -52.37408652\n"," 172.53432012 -43.3308698  -89.94665456]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 981/1000 [28:26<00:19,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 980\tlengths: [145.  80. 214. 241. 188. 378. 382.  85.]\treward: [-43.64471542  35.81622811  86.844281    99.04345521 -62.70261642\n"," 155.28235398  51.19856435 101.08168946]]\tfull length\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [28:48<00:00,  1.73s/it]\n"]}]},{"cell_type":"code","source":["env_id = 'LunarLander-v3'\n","num_episodes = 1000\n","max_steps = 1000\n","lr = 0.02\n","\n","a2c_model_ll_async =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = 8, vectorization_mode=\"async\")\n","\n","rewards_async, steps_async = a2c_model_ll_async.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0rXhsqr0m7sh","executionInfo":{"status":"ok","timestamp":1747578626474,"user_tz":-540,"elapsed":3133704,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"8ad2f88e-1441-4d1e-8002-f7724fe13dc2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/1000 [00:00<08:47,  1.89it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 0\tlengths: [109. 105.  91.  86. 108.  71. 120.  88.]\treward: [-384.05631434 -244.53897027 -475.32608407 -237.94813173 -252.59032813\n"," -274.03278797 -330.4445024  -269.70100465]]\tfull length: 120\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 21/1000 [01:02<55:04,  3.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 20\tlengths: [ 196. 1000.  796.  218. 1000.  159. 1000. 1000.]\treward: [  82.64851266   36.39450977   -7.41036394  -48.31003876 -148.97778193\n","  -32.81208858 -198.13951374    6.12669533]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 41/1000 [01:22<13:59,  1.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 40\tlengths: [ 86. 230. 109. 159.  87.  93.  92. 143.]\treward: [ -462.59093989 -1577.94356144  -607.13759573 -1055.76156587\n","  -367.76281493  -493.07035469  -435.92103231  -858.91124158]]\tfull length: 230\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 61/1000 [01:40<13:16,  1.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 60\tlengths: [ 87. 178.  86. 138.  98. 137.  78. 239.]\treward: [ -576.03430351 -1138.8067684   -535.20599488  -854.11062355\n","  -492.03414504  -989.66993346  -314.19442624 -1851.93755169]]\tfull length: 239\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 81/1000 [01:58<14:35,  1.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 80\tlengths: [ 75. 207. 126.  80. 102.  84.  90. 174.]\treward: [ -414.16012077 -1557.14632454  -834.5660788   -300.10120843\n","  -574.44005449  -500.04567563  -519.98510693  -909.04609315]]\tfull length: 207\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 101/1000 [02:13<10:06,  1.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 100\tlengths: [177.  90.  90. 127. 100.  97.  95.  91.]\treward: [-1125.02442561  -500.26380887  -505.99300807  -690.90158127\n","  -484.1613093   -577.31650747  -530.5431576   -457.41590699]]\tfull length: 177\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 121/1000 [02:30<11:30,  1.27it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 120\tlengths: [126.  79. 110.  78. 144. 126.  74. 165.]\treward: [-635.34711291 -358.60807026 -337.11017892 -393.39840076 -923.98736053\n"," -503.23676003 -382.96365575 -931.78235867]]\tfull length: 165\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 141/1000 [02:49<12:14,  1.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 140\tlengths: [181. 241. 113. 105. 143. 138. 110.  75.]\treward: [-1004.74683975 -2002.88165614  -625.36933763  -604.30711679\n","  -767.86910851  -811.07509087  -674.91567238  -459.55873753]]\tfull length: 241\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 161/1000 [03:06<10:01,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 160\tlengths: [118. 101. 101. 106.  85. 146.  98.  82.]\treward: [-573.8544654  -625.44755596 -538.28693969 -540.80240612 -400.56939056\n"," -638.58159372 -365.28652018 -421.45363421]]\tfull length: 146\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 181/1000 [03:25<12:12,  1.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 180\tlengths: [ 88. 194. 175. 118.  76.  75.  83. 218.]\treward: [ -537.91701816 -1289.54441715 -1335.90524056  -835.37844455\n","  -291.01067031  -388.99840621  -358.12932771 -1506.09880187]]\tfull length: 218\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 201/1000 [03:40<11:59,  1.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 200\tlengths: [112.  78.  90. 163.  79.  93. 101.  83.]\treward: [-344.10018878 -516.75436063 -451.47057422 -812.47074513 -365.58444936\n"," -474.98576271 -509.10252341 -456.41762508]]\tfull length: 163\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 221/1000 [03:57<10:44,  1.21it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 220\tlengths: [100. 130.  83.  85. 168.  78.  78. 104.]\treward: [-438.52111468 -726.46274799 -414.34656165 -362.20719961 -677.58437644\n"," -475.01365097 -272.50379489 -646.89626347]]\tfull length: 168\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 241/1000 [04:16<12:51,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 240\tlengths: [179.  92. 110. 329.  76. 182.  76.  83.]\treward: [-1357.48252832  -578.33687813  -550.062748   -2864.78439036\n","  -455.0147292  -1360.55624176  -345.55442762  -463.88710304]]\tfull length: 329\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 261/1000 [04:37<14:48,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 260\tlengths: [ 86. 104. 179. 195. 113. 102.  76. 136.]\treward: [-114.05910248 -206.37115741 -232.75354921 -395.45025727 -469.96378992\n"," -530.61670258 -311.16094141 -459.83693494]]\tfull length: 195\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 281/1000 [05:46<46:13,  3.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 280\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-103.80873131 -125.93983482 -131.77512129 -108.96766129  -71.94045862\n"," -147.0252354   -82.21261674  -75.41661402]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 301/1000 [07:07<50:06,  4.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 300\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-112.72238691 -114.08257973 -125.7871353  -118.66178169 -108.21215252\n"," -148.91787382 -130.92372483 -141.95922252]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 321/1000 [08:26<44:19,  3.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 320\tlengths: [1000.  404.  163. 1000. 1000. 1000. 1000. 1000.]\treward: [-105.91387056   -9.69604308    5.24867955 -131.25136476  -80.84445754\n"," -132.11230226 -117.31658717 -105.56074613]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 341/1000 [09:44<42:30,  3.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 340\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [ -98.80234954  -81.13904957 -101.5375782   -98.80752704  -95.41806789\n"," -131.1010752  -101.66802325 -106.36929191]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 361/1000 [11:03<41:58,  3.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 360\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-125.37832482 -124.32265362 -107.75263136  -96.57335201  -51.60568299\n"," -134.24102012 -150.19918119 -104.27719949]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 381/1000 [12:19<39:12,  3.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 380\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [ -97.8834318  -110.0607733   -64.04976144  -97.31670689 -127.71550722\n"," -120.15630361 -102.01230769 -128.11051256]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 401/1000 [13:35<37:48,  3.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 400\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [ -77.33219303 -100.761501   -121.24803773 -106.59239453  -99.50934654\n","  -73.45034814 -102.34248038  -90.80377348]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 42%|████▏     | 421/1000 [14:54<38:05,  3.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 420\tlengths: [1000. 1000. 1000. 1000. 1000.  246. 1000.  172.]\treward: [-125.6751428  -136.52164014 -121.60852811 -114.61834222  -97.38230747\n","  -10.86311355 -145.75412635   -2.34751171]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 441/1000 [16:10<36:09,  3.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 440\tlengths: [1000. 1000. 1000. 1000.  709. 1000. 1000. 1000.]\treward: [-124.38287823 -131.25591348 -109.3124047   -85.10600116  -70.89103439\n"," -111.50592293 -146.91368623 -124.04390348]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 461/1000 [17:27<34:20,  3.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 460\tlengths: [1000.  832. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [ -85.03088912 -106.05297368 -121.96273146 -162.7281477  -116.61578999\n","  -87.07216886 -106.35855505  -92.22483346]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 481/1000 [18:44<32:41,  3.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 480\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-127.50867886 -119.49754911 -142.94080331 -106.05784875 -119.56905753\n"," -130.72519292 -140.35778265 -117.3400798 ]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 501/1000 [20:02<32:28,  3.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 500\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-148.33848733 -106.13944292 -109.69188957 -126.4379701  -117.0449929\n"," -150.94980392 -142.39900136 -110.12102534]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 521/1000 [21:20<31:13,  3.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 520\tlengths: [1000.  455.  784.  267. 1000.  397. 1000. 1000.]\treward: [-167.80537161  -76.54411736 -138.91259654  -44.74956744  -83.85821258\n","  -32.52678368 -153.31779996 -103.92165315]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 541/1000 [22:37<29:16,  3.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 540\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [ -98.4315443  -146.8360373  -138.73919074 -149.92435096 -133.57675605\n","  -99.53869552 -158.77597429 -140.40570733]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 561/1000 [23:55<27:44,  3.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 560\tlengths: [ 263. 1000.  698. 1000. 1000. 1000. 1000. 1000.]\treward: [  -0.82392627 -118.89219745  -72.67126177 -122.77243524 -108.19115002\n"," -139.83415468 -147.12865315 -155.947741  ]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 581/1000 [25:14<28:08,  4.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 580\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-132.52571491 -109.50593155 -123.95195609 -103.74374809 -129.33971572\n"," -118.30662941 -156.08269344  -98.86080416]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 601/1000 [26:32<25:50,  3.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 600\tlengths: [1000.  897. 1000. 1000. 1000.  462. 1000. 1000.]\treward: [-127.51490491 -151.38590548 -180.0868826  -143.71513533 -148.83517231\n","  -44.64217862 -130.69619177 -131.8432758 ]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 621/1000 [27:51<24:28,  3.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 620\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-115.81190946 -149.64022602 -141.37488358 -117.11607531 -188.56024124\n"," -161.33211079 -159.96060476 -167.20362474]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 641/1000 [29:09<23:40,  3.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 640\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000.  619.]\treward: [-107.42119823  -99.95123181 -162.60327712 -139.72996456 -107.23691177\n"," -101.14508185 -107.57401465  -90.39787444]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 661/1000 [30:27<22:06,  3.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 660\tlengths: [1000. 1000.  151.  240.  824. 1000.  190.  914.]\treward: [ -94.22994952  -73.03564355   22.83749759  -36.87790023 -119.15365934\n"," -134.98197188  -30.29187938 -129.9644914 ]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 681/1000 [31:45<20:09,  3.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 680\tlengths: [1000. 1000. 1000. 1000. 1000.  373.  557.  144.]\treward: [-124.9076301  -167.42043942 -125.90526925 -109.11310838 -108.56820878\n","  -53.82918903  -66.48287194  -13.96959938]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 701/1000 [33:03<19:46,  3.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 700\tlengths: [1000. 1000.  923. 1000.  217.  300.  387. 1000.]\treward: [-134.31325276 -129.86751352 -133.21185951  -83.52078322   -3.19075195\n","  -50.81849813  -32.9090075  -113.28168364]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 721/1000 [34:20<18:01,  3.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 720\tlengths: [1000. 1000. 1000.  208.  260. 1000. 1000.  322.]\treward: [-127.92133274 -107.77843997 -101.12177427   -1.55886637  -30.67612279\n"," -134.43234559 -134.66049343  -49.39066112]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 741/1000 [35:35<16:18,  3.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 740\tlengths: [1000.  861. 1000. 1000.  588.  899.  232. 1000.]\treward: [-112.83056745 -121.22834973 -135.83444766  -83.30015513  -90.43588805\n"," -147.94335319  -16.25280227  -93.70333007]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 761/1000 [36:50<14:51,  3.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 760\tlengths: [1000. 1000.  914. 1000. 1000. 1000.  746. 1000.]\treward: [-108.02040786 -117.08126796 -157.13048066  -88.40712798 -144.6296387\n"," -155.2548229  -122.04684502 -121.49398714]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 781/1000 [38:06<13:43,  3.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 780\tlengths: [1000. 1000. 1000.  176. 1000. 1000. 1000.  152.]\treward: [-131.87365898 -163.36005007 -172.79696206   -9.9238602  -156.5885749\n"," -125.57787431 -133.05830114  -13.44881689]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 801/1000 [39:23<12:52,  3.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 800\tlengths: [1000.  431. 1000. 1000. 1000. 1000.  405. 1000.]\treward: [-117.25783704  -43.10737909 -139.96676214 -150.78353279 -125.34889947\n","  -99.9548083   -33.0919144  -107.07932847]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 821/1000 [40:41<11:37,  3.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 820\tlengths: [1000. 1000. 1000.  240. 1000. 1000. 1000.  985.]\treward: [ -73.50978099  -85.09655674 -125.84396878  -29.81979249 -108.54179216\n"," -132.27902463 -132.89961339 -130.8115245 ]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 841/1000 [41:59<10:10,  3.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 840\tlengths: [ 311. 1000.  137.  121.  239. 1000.  953. 1000.]\treward: [ -15.60466556  -46.11310831   -4.71022413  -22.39613671  -18.46034174\n"," -163.84335683 -171.23959823 -131.36839132]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 861/1000 [43:15<08:37,  3.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 860\tlengths: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\treward: [-138.68242825 -104.80301659 -105.61814214  -68.11996698  -79.27334072\n"," -121.85651721  -95.34440449 -134.90174182]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 881/1000 [44:32<07:50,  3.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 880\tlengths: [ 333. 1000. 1000. 1000.  983.  765.  468. 1000.]\treward: [ -44.04476501 -158.58224446 -142.85835747 -185.98124971 -175.0697975\n"," -148.76592478  -95.55764368 -194.73055143]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 901/1000 [45:50<06:27,  3.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 900\tlengths: [ 417.  302. 1000. 1000.  836. 1000.  766. 1000.]\treward: [ -52.83557328  -25.79769614 -157.16363939 -143.26356869  -99.86224954\n"," -154.22643713  -92.88858716 -138.47737442]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 921/1000 [47:08<05:00,  3.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 920\tlengths: [ 854.  215. 1000.  259.  603. 1000. 1000. 1000.]\treward: [ -96.10029362   -6.36942193  -94.73783624   -5.98232015  -81.00102342\n"," -103.70856738 -124.44304167 -139.39481338]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▍| 941/1000 [48:26<03:50,  3.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 940\tlengths: [ 597. 1000. 1000.  705.  161.  217.  228.  464.]\treward: [ -96.8805591  -194.48420117 -148.10633049 -122.27800902  -51.17079851\n","  -30.85328686   -4.36406524  -84.33573635]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 961/1000 [49:40<02:28,  3.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 960\tlengths: [ 155.  363. 1000.  468.  148.  355.  116.  451.]\treward: [ -21.17706031  -37.88632335 -125.1789049   -69.86483816  -27.39609436\n","  -88.35731558  -48.01772539  -36.389359  ]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 981/1000 [50:58<01:17,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 980\tlengths: [ 322.  219.  565. 1000.  831. 1000.  728. 1000.]\treward: [ -55.8004386   -20.76729692  -68.17674496 -125.28326551 -116.31863609\n","  -99.77122249 -108.74760863  -98.86020141]]\tfull length: 1000\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [52:13<00:00,  3.13s/it]\n"]}]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24962,"status":"ok","timestamp":1747579106936,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KwcnQRLD9cTX","outputId":"c1026793-654a-4fc2-8680-5312c5d61221"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: -52.894699397403144\n","Episode 2 Reward: -27.627745976441886\n","Episode 3 Reward: -236.95609091731924\n","Episode 4 Reward: 135.61399384725155\n","Episode 5 Reward: -66.2182336378679\n","Episode 6 Reward: 19.699363217334394\n","Episode 7 Reward: 26.81668761972692\n","Episode 8 Reward: -15.518179226312213\n","Episode 9 Reward: 13.410176329457428\n","Episode 10 Reward: -11.25524196136928\n","Average Reward over 10 episodes: -21.49299701029434\n"]}],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_ll.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, name_prefix=\"async_demo\", episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_ll_async.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"id":"YsQWqwPEr6Vz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747579167271,"user_tz":-540,"elapsed":60318,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"86cfea31-2473-4969-a2a2-79a395ca35b2"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: -113.4247666969921\n","Episode 2 Reward: -136.1491438008048\n","Episode 3 Reward: -113.35021589916234\n","Episode 4 Reward: -108.9208804584625\n","Episode 5 Reward: -88.38824172533324\n","Episode 6 Reward: -127.98817557744319\n","Episode 7 Reward: -129.28597991379633\n","Episode 8 Reward: -117.62409761534742\n","Episode 9 Reward: -104.91873832786548\n","Episode 10 Reward: -92.15144943763181\n","Average Reward over 10 episodes: -113.2201689452839\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fr8M0ZjX0vhY"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMVRfHvVto9/qwL5zKH9KZs"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}