{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LA6ipJjA9UFJ"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21539,"status":"ok","timestamp":1748169063399,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aNbCFO1m96R1","outputId":"a8925ba7-428c-49c1-c267-c2cc47506f7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79400,"status":"ok","timestamp":1748178469096,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"0Va0BgsV98Lt","outputId":"91b8454d-b8f2-49e6-ccb6-5d93a994543a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n","Collecting swig\n","  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.3.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379367 sha256=02a8c98464434f527806005df839f6072c62b5b45505f25cc8776a42132aec23\n","  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10485,"status":"ok","timestamp":1748178479576,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"aVKs6T5ifpUd","outputId":"73ac0bc3-39ed-461f-8d56-a05092225722"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: box2d-py 2.3.5\n","Uninstalling box2d-py-2.3.5:\n","  Successfully uninstalled box2d-py-2.3.5\n","Collecting box2d\n","  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n"]}],"source":["!pip uninstall -y box2d-py\n","!pip install box2d pygame swig\n","!pip install \"gymnasium[box2d]\" --no-deps"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1748184220671,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"KJwVWxwiW0Q3"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCriticPPO(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim, hidden_dims=(64, 64)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1748184307019,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"GYE9dNX2-NMy"},"outputs":[],"source":["from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","import copy\n","\n","class PPOAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, epsilon=float('inf'), gamma=0.99, lambda_GAE=1, lr=1e-3, num_steps=0, num_envs=8, vectorization_mode = \"sync\", seed=123):\n","        # using vectorized environments to boost training speed\n","        self.env = gym.make_vec(env_id, num_envs=num_envs, vectorization_mode=vectorization_mode)\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","        self.lambda_GAE = lambda_GAE\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCriticPPO(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.old_policy_net = copy.deepcopy(self.policy_net)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        # added scheduler after observing divergence after getting close to solving\n","        self.scheduler = StepLR(self.optimizer, step_size=100, gamma=0.9)\n","        self.loss = nn.MSELoss()\n","        self.seed = seed\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.old_policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing GAE\n","    def compute_returns(self, rewards, values, next_value):\n","        rewards = torch.stack(rewards)\n","        values = torch.cat([values, next_value.unsqueeze(0)], dim=0)\n","\n","        T, N = rewards.shape\n","        advantages = torch.zeros_like(rewards)\n","        gae = torch.zeros(N, device=rewards.device)\n","        for t in reversed(range(T)):\n","            # temporal difference error\n","            td = rewards[t] + self.gamma * values[t + 1] - values[t]\n","            # higher labmda -> more sampling, lower lambda -> more bootstrapping\n","            gae = td + self.gamma * self.lambda_GAE * gae\n","            advantages[t] = gae\n","\n","        # compute returns by adding value to advantage\n","        returns = advantages + values[:-1]\n","        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","\n","        # normalize advantage across timesteps and environments\n","        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","        return returns, advantages\n","\n","    # computing the gamma decaying rewards\n","    # def compute_returns(self, rewards, values, next_value):\n","    #     \"\"\"\n","    #     Args:\n","    #         rewards: torch.Tensor of shape [T, N] where\n","    #                 T = rollout steps, N = num_envs\n","    #     Returns:\n","    #         returns: torch.Tensor of shape [T, N], normalized\n","    #     \"\"\"\n","    #     rewards = torch.stack(rewards)\n","\n","    #     T, N = rewards.shape\n","    #     returns = torch.zeros_like(rewards)\n","    #     R = torch.zeros(N, device=rewards.device)\n","    #     for t in reversed(range(T)):\n","    #         R = rewards[t] + self.gamma * R\n","    #         returns[t] = R\n","\n","    #     # Normalize returns across all timesteps and environments\n","    #     returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n","    #     advantages = returns - values\n","\n","    #     return returns, advantages\n","\n","\n","    def train(self):\n","        episode_rewards = []\n","        episode_steps = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset(seed=self.seed)\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs, old_log_probs, old_values = [], [], [], [], []\n","            done_mask = np.zeros(self.num_envs, dtype=bool)\n","            done_steps = np.zeros(self.num_envs)\n","            steps = 0\n","\n","            while not np.all(done_mask) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                with torch.no_grad():\n","                    old_action_probs, old_value = self.old_policy_net(state_tensor)\n","                    old_action_dist = torch.distributions.Categorical(old_action_probs)\n","                    old_log_prob = old_action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","                done_steps = np.where(np.logical_and(done, ~done_mask), steps, done_steps)\n","                done_mask = np.logical_or(done_mask, done)\n","                # record when each environment is done\n","                reward = np.where(done_mask, 0.0, reward)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                log_probs.append(log_prob)\n","                old_log_probs.append(old_log_prob)\n","                old_values.append(old_value.squeeze())\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","\n","                # finish full trajectory, then update\n","                if self.num_steps == 0:\n","                    if np.any(done):\n","                        with torch.no_grad():\n","                            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                            _, next_value = self.policy_net(next_state_tensor)\n","                            done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                            next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                        values = torch.stack(values)  # shape: (n_steps, num_envs)\n","                        returns, advantages = self.compute_returns(rewards, values, next_value)  # shape: (n_steps, num_envs)\n","                        returns, values, advantages = returns.transpose(0, 1), values.transpose(0, 1), advantages.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                        log_probs, old_log_probs = torch.stack(log_probs).transpose(0, 1), torch.stack(old_log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                        prob_ratio = torch.exp(log_probs - old_log_probs)\n","\n","                        surrogate = torch.min(prob_ratio * advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * advantages)\n","                        actor_loss = -surrogate.mean()\n","                        critic_loss = self.loss(returns, values)\n","\n","                        loss = actor_loss + critic_loss\n","                        self.optimizer.zero_grad()\n","                        loss.backward()\n","                        self.optimizer.step()\n","                        # self.scheduler.step()\n","\n","                        values = []\n","                        rewards = []\n","                        log_probs = []\n","                        old_log_probs = []\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                else:\n","                    if (steps % self.num_steps == 0) or np.any(done):\n","                        with torch.no_grad():\n","                            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                            _, next_value = self.policy_net(next_state_tensor)\n","                            done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                            next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                        values = torch.stack(values)  # shape: (n_steps, num_envs)\n","                        old_values = torch.stack(old_values).transpose(0, 1)\n","                        returns, advantages = self.compute_returns(rewards, values, next_value)  # shape: (n_steps, num_envs)\n","                        returns, values, advantages = returns.transpose(0, 1), values.transpose(0, 1), advantages.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                        log_probs, old_log_probs = torch.stack(log_probs).transpose(0, 1), torch.stack(old_log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                        prob_ratio = torch.exp(log_probs - old_log_probs.detach())\n","\n","                        clipped = (prob_ratio > 1 + self.epsilon) | (prob_ratio < 1 - self.epsilon)\n","                        clip_fraction = clipped.float().mean().item()\n","                        # print(f\"Clipped fraction: {clip_fraction:.2%}\")\n","\n","                        surrogate = torch.min(prob_ratio * advantages, prob_ratio.clamp(1 - self.epsilon, 1 + self.epsilon) * advantages)\n","                        actor_loss = -surrogate.mean()\n","\n","                        # clipped_values = torch.clamp(values, old_values.detach() * (1 - self.epsilon), old_values.detach() * (1 + self.epsilon))\n","                        # critic_loss = min(self.loss(returns, values), self.loss(returns, clipped_values))\n","                        critic_loss = self.loss(returns, values)\n","\n","                        loss = actor_loss + 0.5 * critic_loss\n","                        self.optimizer.zero_grad()\n","                        loss.backward()\n","                        self.optimizer.step()\n","                        # self.scheduler.step()\n","                        # if episode % 10 == 0:\n","                        #     print(\"VALUES\", values, \"\\n\", \"RETURNS\", returns)\n","\n","                        values, rewards, log_probs, old_log_probs, old_values = [], [], [], [], []\n","\n","\n","            self.old_policy_net = copy.deepcopy(self.policy_net)\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","            if episode % 20 == 0:\n","               print('Episode {}\\tlengths: {}\\treward: {}]\\tfull length: {}'.format(episode, done_steps, episode_reward, steps))\n","            if episode % 10 == 0:\n","                print(f\"\\n[Episode {episode}]\")\n","                print(f\"Reward (mean): {np.mean(episode_reward):.2f}\")\n","                print(f\"Actor Loss: {actor_loss.item():.4f} | Critic Loss: {critic_loss.item():.4f}\")\n","                print(f\"Prob Ratio - mean: {prob_ratio.mean().item():.4f}, max: {prob_ratio.max().item():.4f}, min: {prob_ratio.min().item():.4f}\")\n","\n","            episode_rewards.append(episode_reward)\n","            episode_steps.append(steps)\n","\n","        self.env.close()\n","        return np.array(episode_rewards), np.array(episode_steps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xS4b8URHWybG","outputId":"61590be1-71e6-47db-c57e-552a48f7117b"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/500 [00:00<04:14,  1.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 0\tlengths: [ 64.  69. 109.  97.  89. 100.  90.  72.]\treward: [-167.85368813 -112.30575797  -19.21825992 -111.71412477 -243.36392964\n","  -20.24118961  -58.12143199  -61.3414641 ]]\tfull length: 109\n","\n","[Episode 0]\n","Reward (mean): -99.27\n","Actor Loss: -0.0002 | Critic Loss: 0.9986\n","Prob Ratio - mean: 1.0005, max: 1.0094, min: 0.9910\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 11/500 [00:06<05:23,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 10]\n","Reward (mean): -31.01\n","Actor Loss: -0.0012 | Critic Loss: 1.0522\n","Prob Ratio - mean: 0.9998, max: 1.0099, min: 0.9934\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 21/500 [00:11<04:00,  1.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 20\tlengths: [ 75.  71. 115.  92.  92. 101. 105.  71.]\treward: [-109.91888568   12.54096396  -54.51765037  -47.23984847 -252.27680638\n"," -152.98533693  -80.40698152   16.08809575]]\tfull length: 115\n","\n","[Episode 20]\n","Reward (mean): -83.59\n","Actor Loss: 0.0020 | Critic Loss: 1.0547\n","Prob Ratio - mean: 1.0009, max: 1.0055, min: 0.9932\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 31/500 [00:19<05:18,  1.47it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 30]\n","Reward (mean): -58.89\n","Actor Loss: 0.0003 | Critic Loss: 1.1895\n","Prob Ratio - mean: 1.0003, max: 1.0107, min: 0.9826\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 41/500 [00:25<05:14,  1.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 40\tlengths: [ 81.  83. 129. 112. 103. 199.  89.  76.]\treward: [  16.27629173  -10.49972822   30.88871917 -104.60947435  -53.08857004\n","   89.4517117  -245.49828245   14.37917108]]\tfull length: 199\n","\n","[Episode 40]\n","Reward (mean): -32.84\n","Actor Loss: -0.0061 | Critic Loss: 0.9782\n","Prob Ratio - mean: 1.0056, max: 1.0297, min: 0.9711\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 51/500 [00:31<05:12,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 50]\n","Reward (mean): -89.24\n","Actor Loss: -0.0008 | Critic Loss: 1.0184\n","Prob Ratio - mean: 0.9992, max: 1.0168, min: 0.9798\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 61/500 [00:38<05:14,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 60\tlengths: [ 80.  90. 180. 113. 111. 143.  90.  91.]\treward: [ -57.93899207  -83.64925274 -208.93097588 -100.12167251  -61.70605881\n","   10.25081273 -199.21157365   87.21567256]]\tfull length: 180\n","\n","[Episode 60]\n","Reward (mean): -76.76\n","Actor Loss: -0.0028 | Critic Loss: 1.3052\n","Prob Ratio - mean: 1.0017, max: 1.0204, min: 0.9743\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 71/500 [00:46<05:57,  1.20it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 70]\n","Reward (mean): -59.71\n","Actor Loss: -0.0005 | Critic Loss: 0.8676\n","Prob Ratio - mean: 0.9992, max: 1.0209, min: 0.9703\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 81/500 [00:55<06:40,  1.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 80\tlengths: [108. 116. 179. 146. 123. 143. 174.  94.]\treward: [   9.11136123 -197.51816442 -276.41080272 -138.28411376  -75.71150763\n"," -182.67862781   48.22398545   50.81839938]]\tfull length: 179\n","\n","[Episode 80]\n","Reward (mean): -95.31\n","Actor Loss: 0.0019 | Critic Loss: 0.9845\n","Prob Ratio - mean: 0.9962, max: 1.0310, min: 0.9628\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 91/500 [01:08<10:28,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 90]\n","Reward (mean): -57.75\n","Actor Loss: 0.0033 | Critic Loss: 0.9562\n","Prob Ratio - mean: 0.9981, max: 1.0400, min: 0.9729\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 101/500 [01:17<06:44,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 100\tlengths: [136. 116. 251. 146. 183. 189. 139. 192.]\treward: [  62.20572694   54.77111974  -57.42712938   18.07899594 -172.33669923\n"," -182.96143868  -82.34637169   39.85784717]]\tfull length: 251\n","\n","[Episode 100]\n","Reward (mean): -40.02\n","Actor Loss: 0.0095 | Critic Loss: 1.0399\n","Prob Ratio - mean: 0.9986, max: 1.0474, min: 0.9650\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 111/500 [01:28<07:21,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 110]\n","Reward (mean): -74.00\n","Actor Loss: 0.0016 | Critic Loss: 0.8418\n","Prob Ratio - mean: 1.0008, max: 1.0192, min: 0.9753\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 121/500 [01:39<06:15,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 120\tlengths: [110. 161. 227. 124. 128. 223. 143. 138.]\treward: [  67.19934334  -12.76516794   23.70566946  -86.00856604   38.46255286\n"," -140.65763118   80.8899943    50.7060729 ]]\tfull length: 227\n","\n","[Episode 120]\n","Reward (mean): 2.69\n","Actor Loss: 0.0006 | Critic Loss: 0.8874\n","Prob Ratio - mean: 1.0000, max: 1.0346, min: 0.9618\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 131/500 [01:55<09:47,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 130]\n","Reward (mean): -37.10\n","Actor Loss: 0.0010 | Critic Loss: 0.8136\n","Prob Ratio - mean: 1.0032, max: 1.0385, min: 0.9645\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 141/500 [02:11<11:07,  1.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 140\tlengths: [225.   0. 135. 144. 188. 285. 214. 151.]\treward: [  99.80237338   74.75750321   18.47925088   -4.24190299  -98.6377475\n","   59.98680456 -217.99700208   54.08802943]]\tfull length: 500\n","\n","[Episode 140]\n","Reward (mean): -1.72\n","Actor Loss: -0.0144 | Critic Loss: 0.9905\n","Prob Ratio - mean: 1.0103, max: 1.1321, min: 0.9231\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 151/500 [02:26<09:51,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 150]\n","Reward (mean): -33.07\n","Actor Loss: 0.0000 | Critic Loss: 0.9571\n","Prob Ratio - mean: 0.9983, max: 1.0475, min: 0.9620\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 161/500 [02:44<11:19,  2.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 160\tlengths: [  0. 111. 157. 104. 149. 204. 175.   0.]\treward: [120.68614485  98.92749685 -84.72711435  56.66343257  19.39970616\n"," -18.44608254 170.88567528  96.76168424]]\tfull length: 500\n","\n","[Episode 160]\n","Reward (mean): 57.52\n","Actor Loss: -0.0055 | Critic Loss: 1.0599\n","Prob Ratio - mean: 0.9964, max: 1.1097, min: 0.9017\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 171/500 [03:00<09:41,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 170]\n","Reward (mean): 45.20\n","Actor Loss: -0.0096 | Critic Loss: 0.7833\n","Prob Ratio - mean: 1.0089, max: 1.1395, min: 0.9250\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 181/500 [03:16<07:10,  1.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 180\tlengths: [246. 142. 192. 141. 250. 262. 155. 232.]\treward: [  26.6531723   144.5542992    38.13452866  106.44026156   32.93601274\n","  -13.74532273  -90.47717048 -116.87950372]]\tfull length: 262\n","\n","[Episode 180]\n","Reward (mean): 15.95\n","Actor Loss: -0.0014 | Critic Loss: 0.6927\n","Prob Ratio - mean: 0.9968, max: 1.0497, min: 0.9564\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 191/500 [03:31<07:43,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 190]\n","Reward (mean): -16.78\n","Actor Loss: 0.0022 | Critic Loss: 0.5561\n","Prob Ratio - mean: 1.0011, max: 1.0265, min: 0.9457\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 201/500 [03:49<08:01,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 200\tlengths: [182. 184. 229. 188. 150. 248. 110. 189.]\treward: [  52.58419708   37.06604105  -87.23396144   99.29518335   15.20711999\n","  -80.66591298   82.97095141 -107.08406302]]\tfull length: 248\n","\n","[Episode 200]\n","Reward (mean): 1.52\n","Actor Loss: 0.0021 | Critic Loss: 0.6181\n","Prob Ratio - mean: 1.0074, max: 1.0893, min: 0.8762\n"]},{"output_type":"stream","name":"stderr","text":[" 42%|████▏     | 211/500 [04:07<07:53,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 210]\n","Reward (mean): -7.45\n","Actor Loss: 0.0089 | Critic Loss: 1.8169\n","Prob Ratio - mean: 1.0111, max: 1.0407, min: 0.9684\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 221/500 [04:25<07:41,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 220\tlengths: [175. 152. 304. 169. 261. 360. 151. 199.]\treward: [  89.86735959  124.69108282 -107.88521081  109.48733763   68.84122199\n","   81.288607    -76.74526567   72.12195637]]\tfull length: 360\n","\n","[Episode 220]\n","Reward (mean): 45.21\n","Actor Loss: 0.0072 | Critic Loss: 0.8084\n","Prob Ratio - mean: 1.0041, max: 1.0471, min: 0.9033\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 231/500 [04:44<08:23,  1.87s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 230]\n","Reward (mean): 67.39\n","Actor Loss: -0.0029 | Critic Loss: 0.9765\n","Prob Ratio - mean: 1.0066, max: 1.1026, min: 0.9265\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 241/500 [05:04<07:56,  1.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 240\tlengths: [196. 175. 240. 237. 184. 227. 167. 255.]\treward: [ 71.25428892 110.16454257 -96.5560944  -91.46226634  63.01679317\n"," -86.05406221 -72.80882115 100.30649125]]\tfull length: 255\n","\n","[Episode 240]\n","Reward (mean): -0.27\n","Actor Loss: 0.0003 | Critic Loss: 0.8073\n","Prob Ratio - mean: 0.9998, max: 1.0324, min: 0.9329\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 251/500 [05:25<09:02,  2.18s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 250]\n","Reward (mean): -8.39\n","Actor Loss: -0.0080 | Critic Loss: 0.9386\n","Prob Ratio - mean: 1.0006, max: 1.0626, min: 0.9147\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 261/500 [05:45<08:17,  2.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 260\tlengths: [315.   0. 220. 214. 148. 312. 140. 270.]\treward: [  63.49178314  124.32548649 -108.90540368  -70.37579034    7.72297507\n","   44.51631574  -44.27585951  -96.0521518 ]]\tfull length: 500\n","\n","[Episode 260]\n","Reward (mean): -9.94\n","Actor Loss: -0.0117 | Critic Loss: 0.8883\n","Prob Ratio - mean: 0.9955, max: 1.0253, min: 0.8236\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 271/500 [06:06<07:22,  1.93s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 270]\n","Reward (mean): 14.70\n","Actor Loss: 0.0214 | Critic Loss: 0.4785\n","Prob Ratio - mean: 0.9786, max: 1.0504, min: 0.9075\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 281/500 [06:24<05:58,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 280\tlengths: [166. 152. 314. 227. 170. 174. 143. 310.]\treward: [146.16892652  64.65912465  57.99984384 -73.32625228 112.82790794\n","  83.27257737 -80.78585007  69.92201465]]\tfull length: 314\n","\n","[Episode 280]\n","Reward (mean): 47.59\n","Actor Loss: -0.0015 | Critic Loss: 0.7089\n","Prob Ratio - mean: 1.0042, max: 1.0905, min: 0.9010\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 291/500 [06:44<07:15,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 290]\n","Reward (mean): 7.82\n","Actor Loss: -0.0008 | Critic Loss: 0.8252\n","Prob Ratio - mean: 0.9954, max: 1.0187, min: 0.9483\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 301/500 [07:05<06:37,  2.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 300\tlengths: [313. 211. 307. 157. 169. 199. 170. 348.]\treward: [  24.39636583  146.18582264 -102.19473603   -2.99462985   40.73217125\n"," -123.84567255  102.22303642 -222.5030605 ]]\tfull length: 348\n","\n","[Episode 300]\n","Reward (mean): -17.25\n","Actor Loss: -0.0020 | Critic Loss: 0.6685\n","Prob Ratio - mean: 1.0062, max: 1.0911, min: 0.9632\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 311/500 [07:27<07:06,  2.25s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 310]\n","Reward (mean): 50.38\n","Actor Loss: -0.0048 | Critic Loss: 0.8605\n","Prob Ratio - mean: 1.0057, max: 1.1192, min: 0.8941\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 321/500 [07:47<06:13,  2.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 320\tlengths: [173. 172.   0. 174. 471. 485. 207.   0.]\treward: [ 56.19164582 -31.65279165 139.21050101  77.71911511  78.87030846\n","   9.35084853 163.02595153  95.46558251]]\tfull length: 500\n","\n","[Episode 320]\n","Reward (mean): 73.52\n","Actor Loss: 0.0114 | Critic Loss: 0.9695\n","Prob Ratio - mean: 1.0086, max: 1.1954, min: 0.9189\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 331/500 [08:08<05:50,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 330]\n","Reward (mean): 20.24\n","Actor Loss: 0.0052 | Critic Loss: 0.6234\n","Prob Ratio - mean: 1.0115, max: 1.1552, min: 0.9317\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 341/500 [08:29<05:32,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 340\tlengths: [  0.   0. 210. 217. 238. 280.   0. 274.]\treward: [  37.4110903   199.82178822 -102.18299642  -49.56216753   73.27709085\n","  108.2801022   171.37425046   81.63323202]]\tfull length: 500\n","\n","[Episode 340]\n","Reward (mean): 65.01\n","Actor Loss: 0.0027 | Critic Loss: 0.5317\n","Prob Ratio - mean: 0.9999, max: 1.0842, min: 0.8840\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 351/500 [08:49<04:44,  1.91s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 350]\n","Reward (mean): 20.49\n","Actor Loss: -0.0021 | Critic Loss: 0.4305\n","Prob Ratio - mean: 0.9885, max: 1.1413, min: 0.9185\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 361/500 [09:09<04:33,  1.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 360\tlengths: [  0.   0. 223. 228. 180. 387. 200. 393.]\treward: [  10.24108112  125.88715188   83.39629896  119.17107966   73.0239187\n"," -111.99487718   87.69866983 -110.29764968]]\tfull length: 500\n","\n","[Episode 360]\n","Reward (mean): 34.64\n","Actor Loss: 0.0029 | Critic Loss: 0.5141\n","Prob Ratio - mean: 0.9998, max: 1.0365, min: 0.9329\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 371/500 [09:31<04:41,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 370]\n","Reward (mean): -10.52\n","Actor Loss: 0.0049 | Critic Loss: 0.2057\n","Prob Ratio - mean: 0.9942, max: 1.1837, min: 0.8529\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 381/500 [09:52<04:05,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 380\tlengths: [235. 188. 320. 237. 282. 392. 241. 422.]\treward: [  84.66890578  119.14048831 -102.01977557   96.18744214   98.93191445\n","  -98.44069451  114.27922276   26.42679148]]\tfull length: 422\n","\n","[Episode 380]\n","Reward (mean): 42.40\n","Actor Loss: 0.0012 | Critic Loss: 0.4379\n","Prob Ratio - mean: 1.0064, max: 1.1230, min: 0.8415\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 391/500 [10:12<03:38,  2.01s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 390]\n","Reward (mean): 76.07\n","Actor Loss: -0.0017 | Critic Loss: 0.5746\n","Prob Ratio - mean: 1.0315, max: 1.0734, min: 0.9699\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 401/500 [10:33<03:33,  2.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 400\tlengths: [222. 182.   0. 169.   0. 357. 166. 288.]\treward: [108.84792805 121.03055415 140.31965655  83.63628978 144.09797796\n","  58.88665244 124.51854866  75.29476461]]\tfull length: 500\n","\n","[Episode 400]\n","Reward (mean): 107.08\n","Actor Loss: -0.0124 | Critic Loss: 0.5375\n","Prob Ratio - mean: 1.0020, max: 1.1415, min: 0.8707\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 411/500 [10:55<03:19,  2.24s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 410]\n","Reward (mean): 57.28\n","Actor Loss: -0.0049 | Critic Loss: 0.2723\n","Prob Ratio - mean: 0.9977, max: 1.0778, min: 0.8329\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 421/500 [11:16<02:43,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 420\tlengths: [  0. 306.   0. 325.   0.   0. 177. 339.]\treward: [-43.79484087 114.38783137 122.11795208 -72.13869304 151.08476653\n"," 127.29092339 130.33852983  70.64483997]]\tfull length: 500\n","\n","[Episode 420]\n","Reward (mean): 74.99\n","Actor Loss: 0.0031 | Critic Loss: 0.5757\n","Prob Ratio - mean: 1.0092, max: 1.0892, min: 0.9080\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 431/500 [11:38<02:38,  2.30s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 430]\n","Reward (mean): 47.63\n","Actor Loss: -0.0010 | Critic Loss: 0.8727\n","Prob Ratio - mean: 1.0078, max: 1.1066, min: 0.8579\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 441/500 [12:00<02:07,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 440\tlengths: [300.   0. 288. 365. 436. 355.   0.   0.]\treward: [-83.67264648 159.56514194  70.87474862  82.67831988 128.37851506\n","  58.5540066  164.95822921  87.58866523]]\tfull length: 500\n","\n","[Episode 440]\n","Reward (mean): 83.62\n","Actor Loss: -0.0008 | Critic Loss: 0.7163\n","Prob Ratio - mean: 1.0138, max: 1.2885, min: 0.8486\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 451/500 [12:23<01:49,  2.23s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 450]\n","Reward (mean): 61.22\n","Actor Loss: -0.0055 | Critic Loss: 0.7272\n","Prob Ratio - mean: 0.9987, max: 1.0955, min: 0.8883\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 461/500 [12:44<01:28,  2.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 460\tlengths: [309.   0. 269. 234. 206. 297. 233. 285.]\treward: [ 59.38963    160.99306874 -78.47757314  94.24199116 -57.57558018\n","  57.64165731 113.30390929 -80.896906  ]]\tfull length: 500\n","\n","[Episode 460]\n","Reward (mean): 33.58\n","Actor Loss: 0.0000 | Critic Loss: 0.5503\n","Prob Ratio - mean: 1.0028, max: 1.0455, min: 0.9546\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▍| 471/500 [13:06<01:01,  2.12s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 470]\n","Reward (mean): 29.12\n","Actor Loss: 0.0120 | Critic Loss: 1.0777\n","Prob Ratio - mean: 1.0043, max: 1.1285, min: 0.7898\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 481/500 [13:28<00:40,  2.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Episode 480\tlengths: [381.   0. 278. 233. 293.   0. 248. 382.]\treward: [  86.83824918  134.00459355   86.16381746   86.95362427   73.09975959\n","  126.24677061  151.64064089 -108.56633362]]\tfull length: 500\n","\n","[Episode 480]\n","Reward (mean): 79.55\n","Actor Loss: 0.0030 | Critic Loss: 0.7645\n","Prob Ratio - mean: 1.0016, max: 1.0333, min: 0.9782\n"]},{"output_type":"stream","name":"stderr","text":[" 97%|█████████▋| 483/500 [13:33<00:38,  2.29s/it]"]}],"source":["import gymnasium as gym\n","\n","env_id = 'LunarLander-v3'\n","num_episodes = 500\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=8, num_steps=8)\n","\n","rewards, steps = ppo_model.train()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwcnQRLD9cTX"},"outputs":[],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from gymnasium.wrappers import RecordVideo\n","import os\n","\n","# Create folder to save the video\n","video_folder = \"./video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Wrap the environment with RecordVideo\n","env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = ppo_model.policy_net(state)\n","            # action_dist = torch.distributions.Categorical(action_probs)\n","            # action = action_dist.sample().item()\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","from gymnasium import spaces\n","import numpy as np\n","\n","class LinearValueEnv(gym.Env):\n","    def __init__(self, gamma=0.99, episode_length=100):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.episode_length = episode_length\n","        self.current_step = 0\n","\n","        # Observation: continuous scalar in [-1, 1]\n","        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n","\n","        # Action: continuous scalar (not used)\n","        self.action_space = spaces.Discrete(1)\n","\n","        self.state = None\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.state = np.random.uniform(-1.0, 1.0, size=(1,)).astype(np.float32)\n","        self.current_step = 0\n","        return self.state.copy(), {}\n","\n","    def step(self, action):\n","        # Reward is simply the state value\n","        reward = float(self.state[0])\n","        self.current_step += 1\n","\n","        terminated = self.current_step >= self.episode_length\n","        truncated = False\n","        return self.state.copy(), reward, terminated, truncated, {}\n","\n","    def render(self):\n","        print(f\"State: {self.state}\")\n","\n","    def close(self):\n","        pass\n","\n","\n"],"metadata":{"id":"gAXsopwasbMo","executionInfo":{"status":"ok","timestamp":1748182849721,"user_tz":-540,"elapsed":4,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["from gymnasium.envs.registration import register\n","\n","register(\n","    id=\"LinearValue-v0\",\n","    entry_point=\"__main__:LinearValueEnv\",  # if you're running in a script\n","    max_episode_steps=100\n",")\n"],"metadata":{"id":"UO-G1kxysz46","executionInfo":{"status":"ok","timestamp":1748182849969,"user_tz":-540,"elapsed":30,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQ0d1b4tKwwl"},"outputs":[],"source":[]},{"cell_type":"code","source":["env_id = \"LinearValue-v0\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-4\n","\n","\n","ppo_model_value =  PPOAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, epsilon=0.2, num_envs=2, num_steps=0)\n","\n","rewards, steps = ppo_model_value.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"z-BU9qP2s0si","executionInfo":{"status":"error","timestamp":1748184192621,"user_tz":-540,"elapsed":58338,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"0be61751-d655-4b26-d5bb-41f4679651e8"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/1000 [00:00<12:09,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 0\tlengths: [100. 100.]\treward: [-72.96960783  84.58938628]]\tfull length: 100\n","\n","[Episode 0]\n","Reward (mean): 5.81\n","Actor Loss: 0.3642 | Critic Loss: 0.9067\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  1%|          | 11/1000 [00:03<04:25,  3.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 10]\n","Reward (mean): -41.61\n","Actor Loss: 0.2812 | Critic Loss: 1.0306\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 21/1000 [00:06<04:13,  3.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 20\tlengths: [100. 100.]\treward: [-32.27667063  45.82619548]]\tfull length: 100\n","\n","[Episode 20]\n","Reward (mean): 6.77\n","Actor Loss: 0.3617 | Critic Loss: 1.0112\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 31/1000 [00:08<04:14,  3.81it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 30]\n","Reward (mean): 49.95\n","Actor Loss: 0.4339 | Critic Loss: 1.0572\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 41/1000 [00:11<04:05,  3.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 40\tlengths: [100. 100.]\treward: [-12.85368513  34.16484547]]\tfull length: 100\n","\n","[Episode 40]\n","Reward (mean): 10.66\n","Actor Loss: 0.3643 | Critic Loss: 1.0583\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 51/1000 [00:14<04:51,  3.26it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 50]\n","Reward (mean): -12.35\n","Actor Loss: 0.3224 | Critic Loss: 0.9231\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 61/1000 [00:17<04:10,  3.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 60\tlengths: [100. 100.]\treward: [ 41.3785345  -51.92305827]]\tfull length: 100\n","\n","[Episode 60]\n","Reward (mean): -5.27\n","Actor Loss: 0.3325 | Critic Loss: 0.9675\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 71/1000 [00:19<03:58,  3.90it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 70]\n","Reward (mean): -60.01\n","Actor Loss: 0.2350 | Critic Loss: 1.0500\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 81/1000 [00:22<03:56,  3.89it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 80\tlengths: [100. 100.]\treward: [18.60985883 19.40882935]]\tfull length: 100\n","\n","[Episode 80]\n","Reward (mean): 19.01\n","Actor Loss: 0.3707 | Critic Loss: 1.1324\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 91/1000 [00:25<04:43,  3.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 90]\n","Reward (mean): 54.82\n","Actor Loss: 0.4316 | Critic Loss: 1.1794\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 101/1000 [00:28<03:59,  3.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 100\tlengths: [100. 100.]\treward: [36.77222595 51.44622749]]\tfull length: 100\n","\n","[Episode 100]\n","Reward (mean): 44.11\n","Actor Loss: 0.4109 | Critic Loss: 1.1559\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 111/1000 [00:30<03:50,  3.86it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 110]\n","Reward (mean): 5.18\n","Actor Loss: 0.3404 | Critic Loss: 1.0804\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 121/1000 [00:33<03:42,  3.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 120\tlengths: [100. 100.]\treward: [-27.61505038  77.55969197]]\tfull length: 100\n","\n","[Episode 120]\n","Reward (mean): 24.97\n","Actor Loss: 0.3734 | Critic Loss: 0.9805\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 131/1000 [00:35<03:45,  3.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 130]\n","Reward (mean): -27.30\n","Actor Loss: 0.2786 | Critic Loss: 0.9290\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 141/1000 [00:39<04:24,  3.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 140\tlengths: [100. 100.]\treward: [-91.24863052  44.33011508]]\tfull length: 100\n","\n","[Episode 140]\n","Reward (mean): -23.46\n","Actor Loss: 0.2832 | Critic Loss: 0.8766\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 151/1000 [00:41<03:39,  3.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 150]\n","Reward (mean): -92.36\n","Actor Loss: 0.1577 | Critic Loss: 1.0184\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 161/1000 [00:44<03:34,  3.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 160\tlengths: [100. 100.]\treward: [ 3.80020684 92.52385944]]\tfull length: 100\n","\n","[Episode 160]\n","Reward (mean): 48.16\n","Actor Loss: 0.4072 | Critic Loss: 1.0393\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 171/1000 [00:46<03:33,  3.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 170]\n","Reward (mean): -14.98\n","Actor Loss: 0.2918 | Critic Loss: 0.9436\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 181/1000 [00:49<04:19,  3.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 180\tlengths: [100. 100.]\treward: [-11.27119991   1.15310721]]\tfull length: 100\n","\n","[Episode 180]\n","Reward (mean): -5.06\n","Actor Loss: 0.3075 | Critic Loss: 1.0708\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▉        | 191/1000 [00:52<03:34,  3.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 190]\n","Reward (mean): 41.69\n","Actor Loss: 0.3900 | Critic Loss: 1.0467\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 201/1000 [00:55<03:23,  3.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 200\tlengths: [100. 100.]\treward: [-81.71754402 -39.18708792]]\tfull length: 100\n","\n","[Episode 200]\n","Reward (mean): -60.45\n","Actor Loss: 0.2027 | Critic Loss: 0.9949\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██        | 211/1000 [00:57<03:23,  3.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[Episode 210]\n","Reward (mean): -59.89\n","Actor Loss: 0.2012 | Critic Loss: 1.0341\n","Prob Ratio - mean: 1.0000, max: 1.0000, min: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██▏       | 213/1000 [00:58<03:35,  3.65it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-100-781daad46b2d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mppo_model_value\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_model_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-99-5553c9874b05>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                         \u001b[0;31m# self.scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPqGJA3PILOwAScnbuTVnYr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}