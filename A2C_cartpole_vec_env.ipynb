{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18011,"status":"ok","timestamp":1747056737127,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"nXMnywNrJT21","outputId":"20421eee-eeac-471f-bca9-664a3fa1c731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5247,"status":"ok","timestamp":1747056803910,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"9TE5jqXjRGj5","outputId":"21de035e-e927-4250-fe69-e61e1ae8171f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.4.0)\n","Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Using cached box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install wheel setuptools pip --upgrade\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1747057749921,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"JifS_sPARTGj"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class ActorCritic(nn.Module):\n","\n","    # initially started with (64, 64) hidden dimension, but emprically found out (32, 32) works better. (64, 64) might be too much power for simple game like cartpole\n","    def __init__(self, input_dim, output_dim, hidden_dims=(32, 32)):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dims[0]),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims[0], hidden_dims[1]),\n","            nn.ReLU()\n","        )\n","        self.actor_layer = nn.Linear(hidden_dims[1], output_dim)\n","        self.critic_layer = nn.Linear(hidden_dims[1], 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        action_probs = F.softmax(self.actor_layer(x), dim=-1)\n","        value = self.critic_layer(x)\n","        return action_probs, value\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1747059382234,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"iLbCiMcsOt5a"},"outputs":[],"source":["from tqdm import tqdm\n","\n","class A2CAgent:\n","    def __init__(self, env_id, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5, num_envs = 8):\n","        # using vectorized environments to boost training speed\n","        self.env = gym.make_vec(env_id, num_envs = num_envs, vectorization_mode=\"sync\")\n","        self.num_envs = num_envs\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value  # shape: (num_envs,)\n","        returns = []\n","        for r in reversed(rewards):  # each r: (num_envs,)\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)  # shape: (n_steps, num_envs)\n","\n","    def train(self):\n","        episode_rewards = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            done = np.zeros(self.num_envs, dtype=bool)\n","            episode_reward = np.zeros(self.num_envs)\n","            values, rewards, log_probs = [], [], []\n","            steps = 0\n","\n","            while not np.all(done) and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy())\n","                done = np.logical_or(terminated, truncated)\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(torch.tensor(reward, dtype=torch.float32).to(self.device))  # shape: (num_envs,)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps for each environment, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or np.any(done):\n","                    with torch.no_grad():\n","                        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","                        _, next_value = self.policy_net(next_state_tensor)\n","                        done_tensor = torch.tensor(done, dtype=torch.float32).to(self.device)\n","                        next_value = next_value.squeeze() * (1 - done_tensor)\n","\n","                    returns = self.compute_n_step_returns(rewards, next_value)  # shape: (n_steps, num_envs)\n","                    returns = returns.transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    values = torch.stack(values).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    log_probs = torch.stack(log_probs).transpose(0, 1)  # shape: (num_envs, n_steps)\n","                    advantages = returns - values\n","\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","                    # penalize using entropy to encourage exploration\n","                    entropy = action_dist.entropy().mean()\n","\n","                    loss = actor_loss + 0.4 * critic_loss- 0.01 * entropy\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","\n","        self.env.close()\n","        return np.array(episode_rewards)\n"]},{"cell_type":"code","source":["class A2CAgent_single:\n","    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, num_steps = 5):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.max_steps = max_steps\n","        self.gamma = gamma\n","        self.lr = lr\n","        self.num_steps = num_steps\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.policy_net = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(self.device)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n","        self.loss = nn.MSELoss()\n","\n","    # choosing action from policy's probability distribution\n","    def choose_action(self, state):\n","        probs, _ = self.policy_net(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action\n","\n","    # computing the gamma decaying rewards\n","    def compute_return(self, rewards):\n","        returns = []\n","        R = 0\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return returns\n","\n","    # computing the n step rewards\n","    def compute_n_step_returns(self, rewards, next_value):\n","        # bootstraps the future reward using value estimate\n","        R = next_value\n","        returns = []\n","        for r in reversed(rewards):\n","            R = r + self.gamma * R\n","            returns.insert(0, R)\n","        return torch.stack(returns)\n","\n","    def train(self):\n","        episode_rewards = []\n","\n","        for episode in tqdm(range(self.num_episodes)):\n","            state, _ = self.env.reset()\n","            episode_reward = 0\n","            values = []\n","            rewards = []\n","            log_probs = []\n","            steps = 0\n","            done = False\n","\n","            while not done and steps < self.max_steps:\n","                steps += 1\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","                action_probs, value = self.policy_net(state_tensor)\n","                action_dist = torch.distributions.Categorical(action_probs)\n","                action = action_dist.sample()\n","                log_prob = action_dist.log_prob(action)\n","\n","                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n","                done = terminated or truncated\n","\n","                # saves the values, rewards, log_probs which are used to calculate the n_step returns, actor loss, and critic loss\n","                values.append(value.squeeze())\n","                rewards.append(reward)\n","                log_probs.append(log_prob)\n","\n","                episode_reward += reward\n","                state = next_state\n","\n","                # every n steps, calculate losses, update the actor & critic, then refresh the saved lists\n","                if (steps % self.num_steps == 0) or done:\n","                    _, next_value = self.policy_net(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device))\n","                    next_value = next_value.squeeze()\n","                    # BUG ALERT\n","                    # MUST MULTIPLY (1 - done) to next_value to mask the bootstrapped next_value when the game is over. CRITICAL BUG THAT TOOK HOURS TO FIND\n","                    returns = self.compute_n_step_returns(rewards, next_value * (1 - done))\n","                    values = torch.stack(values)\n","                    log_probs = torch.stack(log_probs)\n","                    advantages = returns - values\n","                    # calculate actor_loss by multiplying log probabilities to advantages. This will decrease the action probability of negative advantages, and vice-versa\n","                    actor_loss = - (log_probs * advantages.detach()).mean()\n","                    # updates the critic to find better estimate of values that matches the n-step reward\n","                    critic_loss = self.loss(returns, values)\n","\n","                    # penalize using entropy to encourage exploration\n","                    entropy = action_dist.entropy().mean()\n","                    loss = actor_loss + 0.4 * critic_loss- 0.01 * entropy\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                    values = []\n","                    rewards = []\n","                    log_probs = []\n","\n","            episode_rewards.append(episode_reward)\n","\n","        self.env.close()\n","        return np.array(episode_rewards)\n"],"metadata":{"id":"1yZl_zJG2ztG","executionInfo":{"status":"ok","timestamp":1747059464873,"user_tz":-540,"elapsed":2,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","\n","env = gym.make(\"CartPole-v1\")\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","a2c_model_single_env =  A2CAgent_single(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_single_env.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAMO0Kpj2Es2","executionInfo":{"status":"ok","timestamp":1747059504187,"user_tz":-540,"elapsed":30624,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"e1bc3f67-9d29-42f9-db0d-3cc8bc21164c"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:30<00:00, 32.67it/s]\n"]}]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":761231,"status":"ok","timestamp":1747060268956,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"szBR38HBF7_Q","outputId":"ea8e48e1-48e8-4fd6-be62-fe511ba128a5"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [12:41<00:00,  1.31it/s]\n"]}],"source":["env_id = \"CartPole-v1\"\n","num_episodes = 1000\n","max_steps = 500\n","lr = 1e-3\n","\n","#num_steps = max_steps make it Monte-Carlo\n","a2c_model_multiple_env =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env.train()"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":600,"status":"ok","timestamp":1747060288665,"user":{"displayName":"NW Lee","userId":"08636088353176978033"},"user_tz":-540},"id":"TPjtvt_7F7mW","outputId":"45f460b5-6716-42c0-df40-f9583c82885d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 94.0\n","Episode 2 Reward: 100.0\n","Episode 3 Reward: 105.0\n","Episode 4 Reward: 101.0\n","Episode 5 Reward: 106.0\n","Episode 6 Reward: 96.0\n","Episode 7 Reward: 92.0\n","Episode 8 Reward: 95.0\n","Episode 9 Reward: 103.0\n","Episode 10 Reward: 102.0\n","Average Reward over 10 episodes: 99.4\n"]}],"source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_single_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rkMAPuZTq8B","executionInfo":{"status":"ok","timestamp":1747060340956,"user_tz":-540,"elapsed":1819,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"f0898b0d-3267-4e60-abf0-c0dee4d92474"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 500.0\n","Episode 2 Reward: 500.0\n","Episode 3 Reward: 500.0\n","Episode 4 Reward: 500.0\n","Episode 5 Reward: 500.0\n","Episode 6 Reward: 500.0\n","Episode 7 Reward: 500.0\n","Episode 8 Reward: 500.0\n","Episode 9 Reward: 500.0\n","Episode 10 Reward: 500.0\n","Average Reward over 10 episodes: 500.0\n"]}],"source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 10\n","max_steps = 500\n","lr = 1e-3\n","\n","#num_steps = max_steps make it Monte-Carlo\n","a2c_model_multiple_env =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHgJhh8H6YjC","executionInfo":{"status":"ok","timestamp":1747060668141,"user_tz":-540,"elapsed":10134,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"c81462a2-9180-4de4-aeb2-78333d2b0aae"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRVv-c8o6eXJ","executionInfo":{"status":"ok","timestamp":1747060668575,"user_tz":-540,"elapsed":432,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"4003116e-1d46-4454-aa10-fddf4636faf1"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 120.0\n","Episode 2 Reward: 118.0\n","Episode 3 Reward: 119.0\n","Episode 4 Reward: 126.0\n","Episode 5 Reward: 113.0\n","Episode 6 Reward: 110.0\n","Episode 7 Reward: 123.0\n","Episode 8 Reward: 117.0\n","Episode 9 Reward: 120.0\n","Episode 10 Reward: 108.0\n","Average Reward over 10 episodes: 117.4\n"]}]},{"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_episodes = 80\n","max_steps = 500\n","lr = 1e-3\n","\n","#num_steps = max_steps make it Monte-Carlo\n","a2c_model_multiple_env =  A2CAgent(env_id, num_episodes=num_episodes, max_steps=max_steps, lr=lr, num_steps = max_steps)\n","\n","rewards = a2c_model_multiple_env.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eUUYLTc-Jue","executionInfo":{"status":"ok","timestamp":1747061487269,"user_tz":-540,"elapsed":64409,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"b552c7c1-99bb-4a15-c95c-533d07b69fb7"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 80/80 [01:04<00:00,  1.24it/s]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","\n","num_episodes = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","frames = []\n","episode_rewards = []\n","\n","for i in range(num_episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        with torch.no_grad():\n","            action_probs, _ = a2c_model_multiple_env.policy_net(state)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        episode_reward += reward\n","\n","        if i == 0:\n","            frame = env.render()\n","            frames.append(frame)\n","\n","        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","    episode_rewards.append(episode_reward)\n","    print(f\"Episode {i+1} Reward: {episode_reward}\")\n","\n","env.close()\n","\n","episode_rewards = np.array(episode_rewards)\n","print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fI4h9WBS-KYt","executionInfo":{"status":"ok","timestamp":1747061489046,"user_tz":-540,"elapsed":1773,"user":{"displayName":"NW Lee","userId":"08636088353176978033"}},"outputId":"7a7de081-17fd-4a43-c532-10327c51edf9"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 Reward: 500.0\n","Episode 2 Reward: 494.0\n","Episode 3 Reward: 500.0\n","Episode 4 Reward: 500.0\n","Episode 5 Reward: 500.0\n","Episode 6 Reward: 500.0\n","Episode 7 Reward: 500.0\n","Episode 8 Reward: 500.0\n","Episode 9 Reward: 500.0\n","Episode 10 Reward: 500.0\n","Average Reward over 10 episodes: 499.4\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoXsZSP1F20e"},"outputs":[],"source":["### Comparing single environment vs. synchronized environments vs. asynchronized environments"]},{"cell_type":"code","source":[],"metadata":{"id":"X6K5cuaQ89nP"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[],"authorship_tag":"ABX9TyNTV9kHyv85ST+/5hVCCzfp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}